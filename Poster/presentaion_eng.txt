Hello, colleagues, my name is Ilya Irkhin, I work in Yandex and study in magistracy of Moscow Institute of Physics and Technology.

Probabilistic topic modeling is applied to identify latent topics in large collections of texts. Topic model answers the questions what topic each document contains and what words each topic consists of. Topic models have been widely used for the last 15 years to solve various tasks of text analysis and information retrieval. Depending on each specific application the topic model must meet about ten different requirements simultaneously. But the popular Bayesian models do not have such flexibility. They require a new deduction of formulas and implementation in the code for each new application. 

To simplify the process of modeling, my scientific supervisor Konstantin Vorontsov in 2014 proposed a non-Bayesian multi-criteria approach, called additive regularization of topic models. It is based on the simultaneous maximization  of likelihood and a set of additional criteria regularizers. The advantage of ARTM  is that the algorithm is obtained and implemented once in the most general form of regularizer. In practice, it gives an opportunity to build complex composite models with the desired properties.

Basing on this approach an open source library BigARTM have been developed over the past two years. It includes a very fast parallel online Regularized Expectation-Maximization algorithm and its offline version. This library provides a possibility to add and remove regularizers, collecting model as from cubes without programming. 

However, there was an open question about convergence test for ARTM algorithm. These conditions were obtained in our work. In addition, we proposed two modifications of the EM-algorithm, which slightly improve the convergence and simplify the calculations. The convergence theorem is proved for these modifications.

The conditions look a bit bulky, but their main point is that regularized EM-algorithm converges if regularizer's effect on the model is not too big, and if there is a certain consistency in setting to zero of the main model parameters -- conditional probabilities of topics in documents and words in topics. 

The details are presented on the poster. Thank you for your attention.