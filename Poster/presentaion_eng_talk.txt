Hello, colleagues, my name is Ilya Irkhin, I work in Yandex and study in Moscow Institute of Physics and Technology.

Probabilistic topic modeling is applied to identify latent topics in large collections of texts. Topic model answers the questions which topics each document contains and which words each topic consists of. Topic models have been widely used for the last 15 years to solve various tasks of text analysis and information retrieval. Depending on each specific application the topic model must meet about ten different requirements simultaneously. But the popular Bayesian models do not provide such flexibility. They require a new deduction of formulas and implementation in the code for each new application. 

To simplify the process of modeling, my scientific supervisor Konstantin Vorontsov proposed a non-Bayesian multi-criteria approach, called additive regularization of topic models. It is based on the simultaneous maximization  of likelihood and a set of additional regularization criteria. The advantage of ARTM  is that its regularized Expectation Maximization algorithm is obtained and implemented once in the most general form.In practice, it gives an opportunity to build complex composite models with the desired properties.

However, there was an open question about convergence test for ARTM algorithm. These conditions were obtained in our work. In addition, we proposed two modifications of the EM-algorithm, which slightly improve the convergence and simplify the calculations. The convergence theorem was proved for these modifications.

The conditions might look a bit bulky, but their main point is that regularized EM-algorithm converges if regularizer's effect on the model is not too big, and if there is a certain consistency in setting to zero of the main model parameters - conditional probabilities of topics in documents and words in topics. 

The details will be presented in the poster. Thank you for your attention.