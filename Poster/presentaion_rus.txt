Здравствуйте, коллеги, меня зовут Илья Ирхин, я работаю в Яндексе и учусь в магистратуре Физтеха.
 
Вероятностное тематическое моделирование применятся для выявления  латентных тем в больших коллекциях текстов. Тематическая модель отвечает на вопрос, к каким темам относится каждый документ и какими словами представляется каждая тема.
Тематические модели широко используются последние 15 лет для решения разнообразных задач анализа текстов и информационного поиска.
В каждом конкретном приложении тематическая модель должна удовлетворять примерно десятку различных требований одновременно.
Однако популярные байесовские модели не обладают такой гибкостью.
Для каждого нового приложения байесовскую модель приходится заново выводить и имплементировать в коде.  
Чтобы упростить процесс моделирования, моим научным руководителем Константином Воронцовым в 2014 году был предложен не-байесовский многокритериальный подход, называемый аддитивной регуляризацией тематических моделей.
Он основан на одновременной максимизации правдоподобия модели и множества дополнительных критериев-регуляризаторов.
Преимущество АРТМ в том, что алгоритм удаётся вывести и имплементировать один раз в самом общем виде для произвольного регуляризатора или даже для произвольной комбинации регуляризаторов.
На практике это позволяет строить сложные композитные модели с заданными свойствами.
За последние два года была реализована библиотека BigARTM с открытым кодом, с очень быстрым параллельным онлайновым Regularized Expectation-Maximization  алгоритмом, и там действительно можно добавлять и удалять регуляризаторы, собирая модель как из кубиков и ничего не программируя.  
Однако до сих пор оставался открытым вопрос, при каких условиях этот алгоритм сходится, и не будут ли какие-то сочетания регуляризаторов ломать итерационный процесс. 
В моей работе такие условия получены.
Кроме того, предложены две модификации ЕМ-алгоритма, которые немного улучшают сходимость и упрощают вычисления.
Теорема сходимости доказана именно для этих модификаций.
Условия выглядят несколько громоздко, их основной смысл в следующем.
Регуляризованный ЕМ-алгоритм сходится, если регуляризатор воздействует на модель не слишком сильно, и если не проиходит преждевременного обнуления основных параметров модели -- условных вероятностей тем в документах и в слов в темах. 
Подробности представлены на постере.
Спасибо за внимание.