{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оптимальные питоновские реализация оптимизации ARTM\n",
    "\n",
    "# Оптимизация произвольной функции\n",
    "\n",
    "# Thetaless оптимизация\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.core.umath_tests import inner1d\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разные функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.log(x + 1e-20)\n",
    "    def calc_der(self, x):\n",
    "        return 1. / (x + 1e-20)\n",
    "    \n",
    "\n",
    "class IdFunction(object):\n",
    "    def calc(self, x):\n",
    "        return x + 1e-20\n",
    "    def calc_der(self, x):\n",
    "        return np.ones_like(x)\n",
    "    \n",
    "\n",
    "class SquareFunction(object):\n",
    "    def calc(self, x):\n",
    "        return (x + 1e-20) ** 2\n",
    "    def calc_der(self, x):\n",
    "        return 2. * (x + 1e-20) ** 2\n",
    "    \n",
    "\n",
    "class CubeLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.log(x + 1e-20) ** 3\n",
    "    def calc_der(self, x):\n",
    "        return 3. * np.log(x + 1e-20) ** 2 / (x + 1e-20)\n",
    "    \n",
    "\n",
    "class SquareLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.log(x + 1e-20) * np.abs(np.log(x + 1e-20))\n",
    "    def calc_der(self, x):\n",
    "        return 2. * np.abs(np.log(x + 1e-20)) / (x + 1e-20)\n",
    "\n",
    "    \n",
    "class FiveLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.log(x + 1e-20) ** 5\n",
    "    def calc_der(self, x):\n",
    "        return 5. * np.log(x + 1e-20) ** 4 / (x + 1e-20)\n",
    "    \n",
    "\n",
    "class CubeRootLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.cbrt(np.log(x + 1e-20))\n",
    "    def calc_der(self, x):\n",
    "        return 1. / 3 / (np.cbrt(np.log(x + 1e-20)) ** 2) / (x + 1e-20)\n",
    "    \n",
    "    \n",
    "class SquareRootLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.sqrt(- np.log(x + 1e-20))\n",
    "    def calc_der(self, x):\n",
    "        return 1. / 2. / np.sqrt(- np.log(x + 1e-20)) / (x + 1e-20)\n",
    "    \n",
    "\n",
    "class ExpFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.exp(x)\n",
    "    def calc_der(self, x):\n",
    "        return np.exp(x)\n",
    "\n",
    "    \n",
    "class EntropyFunction(object):\n",
    "    def calc(self, x):\n",
    "        return (np.log(x + 1e-20) + 50.) * (x + 1e-20)\n",
    "    def calc_der(self, x):\n",
    "        return np.log(x + 1e-20) + 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разные регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trivial_regularization(n_tw, n_dt):\n",
    "    return np.zeros_like(n_tw), np.zeros_like(n_dt)\n",
    "\n",
    "def create_reg_decorr(tau, theta_alpha=0.):\n",
    "    def fun(n_tw, n_dt):\n",
    "        phi_matrix = n_tw / np.sum(n_tw, axis=1)[:, np.newaxis]\n",
    "        theta_matrix = n_dt / np.sum(n_dt, axis=1)[:, np.newaxis]\n",
    "        aggr_phi = np.sum(phi_matrix, axis=1)\n",
    "        return - tau * np.transpose(phi_matrix * (aggr_phi[:, np.newaxis] - phi_matrix)), theta_alpha\n",
    "    return fun\n",
    "\n",
    "def create_reg_lda(phi_alpha, theta_alpha):\n",
    "    def fun (n_tw, n_dt):\n",
    "        return np.zeros_like(n_tw) + phi_alpha, np.zeros_like(n_dt) + theta_alpha\n",
    "    return fun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка Датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно скачать некоторые коллекции данных и установить библиотеки (nltk, gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tylorn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, calc_cooccurences=False):\n",
    "    # remove stopwords\n",
    "    occurences = Counter()\n",
    "    for i, doc in enumerate(dataset.data):\n",
    "        tokens = gensim.utils.lemmatize(doc)\n",
    "        for token in set(tokens):\n",
    "            occurences[token] += 1\n",
    "        if i % 500 == 0:\n",
    "            print 'Processed: ', i, 'documents from', len(dataset.data)\n",
    "    \n",
    "    row, col, data = [], [], []\n",
    "    token_2_num = {}\n",
    "    not_empty_docs_number = 0\n",
    "    doc_targets = []\n",
    "    doc_cooccurences = Counter()\n",
    "    doc_occurences = Counter()\n",
    "    for doc, target in zip(dataset.data, dataset.target):\n",
    "        tokens = gensim.utils.lemmatize(doc)\n",
    "        cnt = Counter()\n",
    "        for token in tokens:\n",
    "            word = token.split('/')[0]\n",
    "            if word not in english_stopwords and 3 <= occurences[token]:\n",
    "                if token not in token_2_num:\n",
    "                    token_2_num[token] = len(token_2_num)\n",
    "                cnt[token_2_num[token]] += 1\n",
    "        \n",
    "        if len(cnt) > 0:\n",
    "            for w, c in cnt.iteritems():\n",
    "                row.append(not_empty_docs_number)\n",
    "                col.append(w)\n",
    "                data.append(c)\n",
    "            not_empty_docs_number += 1\n",
    "            doc_targets.append(target)\n",
    "            \n",
    "            if calc_cooccurences:\n",
    "                doc_occurences.update(cnt.keys())\n",
    "                doc_cooccurences.update({(w1, w2) for w1 in cnt for w2 in cnt if w1 != w2})\n",
    "        \n",
    "    num_2_token = {\n",
    "        v: k\n",
    "        for k, v in token_2_num.iteritems()\n",
    "    }\n",
    "    print 'Nonzero values:', len(data)\n",
    "    if calc_cooccurences:\n",
    "        return scipy.sparse.csr_matrix((data, (row, col))), token_2_num, num_2_token, doc_targets, doc_occurences, doc_cooccurences\n",
    "    else:\n",
    "        return scipy.sparse.csr_matrix((data, (row, col))), token_2_num, num_2_token, doc_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    categories=['sci.electronics', 'sci.med', 'sci.space', 'sci.crypt', 'rec.sport.baseball', 'rec.sport.hockey'],\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0 documents from 5945\n",
      "Processed:  500 documents from 5945\n",
      "Processed:  1000 documents from 5945\n",
      "Processed:  1500 documents from 5945\n",
      "Processed:  2000 documents from 5945\n",
      "Processed:  2500 documents from 5945\n",
      "Processed:  3000 documents from 5945\n",
      "Processed:  3500 documents from 5945\n",
      "Processed:  4000 documents from 5945\n",
      "Processed:  4500 documents from 5945\n",
      "Processed:  5000 documents from 5945\n",
      "Processed:  5500 documents from 5945\n",
      "Nonzero values: 322664\n",
      "CPU times: user 10min 35s, sys: 4.76 s, total: 10min 39s\n",
      "Wall time: 10min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "origin_n_dw_matrix, token_2_num, num_2_token, doc_targets, doc_occurences, doc_cooccurences = prepare_dataset(dataset, calc_cooccurences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_dataset = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0 documents from 18846\n",
      "Processed:  500 documents from 18846\n",
      "Processed:  1000 documents from 18846\n",
      "Processed:  1500 documents from 18846\n",
      "Processed:  2000 documents from 18846\n",
      "Processed:  2500 documents from 18846\n",
      "Processed:  3000 documents from 18846\n",
      "Processed:  3500 documents from 18846\n",
      "Processed:  4000 documents from 18846\n",
      "Processed:  4500 documents from 18846\n",
      "Processed:  5000 documents from 18846\n",
      "Processed:  5500 documents from 18846\n",
      "Processed:  6000 documents from 18846\n",
      "Processed:  6500 documents from 18846\n",
      "Processed:  7000 documents from 18846\n",
      "Processed:  7500 documents from 18846\n",
      "Processed:  8000 documents from 18846\n",
      "Processed:  8500 documents from 18846\n",
      "Processed:  9000 documents from 18846\n",
      "Processed:  9500 documents from 18846\n",
      "Processed:  10000 documents from 18846\n",
      "Processed:  10500 documents from 18846\n",
      "Processed:  11000 documents from 18846\n",
      "Processed:  11500 documents from 18846\n",
      "Processed:  12000 documents from 18846\n",
      "Processed:  12500 documents from 18846\n",
      "Processed:  13000 documents from 18846\n",
      "Processed:  13500 documents from 18846\n",
      "Processed:  14000 documents from 18846\n",
      "Processed:  14500 documents from 18846\n",
      "Processed:  15000 documents from 18846\n",
      "Processed:  15500 documents from 18846\n",
      "Processed:  16000 documents from 18846\n",
      "Processed:  16500 documents from 18846\n",
      "Processed:  17000 documents from 18846\n",
      "Processed:  17500 documents from 18846\n",
      "Processed:  18000 documents from 18846\n",
      "Processed:  18500 documents from 18846\n",
      "Nonzero values: 1071359\n",
      "CPU times: user 1h 1min 23s, sys: 11.1 s, total: 1h 1min 34s\n",
      "Wall time: 1h 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "big_origin_n_dw_matrix, _, _, big_doc_targets, big_doc_occurences, big_doc_cooccurences = prepare_dataset(big_dataset, calc_cooccurences=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисление правдоподобных функций\n",
    "\n",
    "### имеется в виду вычисление функций вида $\\sum_{dw} n_{dw} f(\\sum_{t} \\phi_{wt} \\theta_{td})$\n",
    "\n",
    "##### Ключевой момент - использование функции inner1d. Она позволяет перемножить попарно строчки матриц, не сохраняя промежуточное состояние. А индексация в numpy не создаёт новый массив, делает view над ним. Таким образом, подсчёт $\\sum_{t} \\phi_{wt} \\theta_{td}$  делается максимально эффективным способом и по времени и по памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_calculate_likelihood_like_function(n_dw_matrix, loss_function=LogFunction()):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    docptr = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        docptr.extend([doc_num] * (indptr[doc_num + 1] - indptr[doc_num]))\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    \n",
    "    def fun(phi_matrix, theta_matrix):\n",
    "        s_data = loss_function.calc(inner1d(theta_matrix[docptr, :], np.transpose(phi_matrix)[wordptr, :]))\n",
    "        return np.sum(n_dw_matrix.data * s_data)\n",
    "\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM алгоритм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая схема:\n",
    "#### Неоходимо сначала вычислить $p_{tdw} = \\frac{\\phi_{wt} \\theta_{td}}{\\sum_s \\phi_{ws} \\theta_{sd}}$\n",
    "#### Считаем $n_{wt} = \\sum_d n_{dw} p_{tdw}$ и $n_{td} = \\sum_w n_{dw} p_{tdw}$\n",
    "#### Вычисляем $r_{wt}, r_{td}$ как функцию от $n_{wt}, n_{td}$\n",
    "#### Прибавляем, делаем положительную срезку и нормируем\n",
    "\n",
    "## Оптимизация вычисления:\n",
    "#### Обозначим за $s_{dw}$ следующее выражение $\\sum_t \\phi_{wt} \\theta_{td}$, фактически это наше предсказание для вероятности\n",
    "####  Тогда $p_{tdw} = \\frac{\\phi_{wt} \\theta_{td}}{s_{dw}}$\n",
    "#### Подставим это выражение например в $n_wt$\n",
    "#### И получим, что $n_{wt} = \\sum_d n_{dw} \\frac{\\phi_{wt} \\theta_{td}}{s_{dw}} = \\phi_{wt} \\sum_d \\theta_{td} \\cdot \\frac{n_{dw}}{s_{dw}}$, аналогично $n_{td} = \\theta_{td} \\sum_w \\phi_{wt} \\cdot \\frac{n_{dw}}{s_{dw}}$\n",
    "#### Таким образом, мы видим, что фактически нам нужно знать матрицу $\\frac{n_{dw}}{s_{dw}}$, а она очень разреженная, поэтому и $s_{dw}$ нужно не для всех пар вычислять, а только там, где $n_{dw} > 0$. \n",
    "#### То есть нам нужно эффективно закодить вычисление разженной матрицы $s_{dw}$ (матрица $n_{dw}$ уже есть в разреженном виде, так как подаётся на вход алгоритма), а затем просто поэлементно поделить\n",
    "#### Причём хочется, чтобы промежуточные значения $p_{tdw}$ не сохранялись (как мы увидели, они в конечном варианте не важны)\n",
    "#### Обозначим эту матрицу за $A$. Тогда $n_{wt} = \\phi_{wt} (\\Theta A)_{tw}$, а $n_{td} = \\theta_{td} (A \\Phi^T)_{dt}$.\n",
    "#### Перемножить разреженную матрицу на плотную можно быстро, если правильно её хранить (по строкам, или по столбцам)\n",
    "#### Если оптимизируется не правдоподобие, какая-то другая функция вида $\\sum_{dw} n_{dw} f(s_{dw})$ (правдоподобие будет, если $f(x) = \\ln x$ ) , то в этом случае нужно определить матрицу $A$ как $A_{dw} = n_{dw} f'(s_{dw})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def em_optimization(\n",
    "    n_dw_matrix, \n",
    "    phi_matrix,\n",
    "    theta_matrix,\n",
    "    regularization_list,\n",
    "    iters_count=100,\n",
    "    loss_function=LogFunction(),\n",
    "    iteration_callback=None\n",
    "):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    T = phi_matrix.shape[0]\n",
    "    phi_matrix = np.copy(phi_matrix)\n",
    "    theta_matrix = np.copy(theta_matrix)\n",
    "    docptr = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        docptr.extend([doc_num] * (indptr[doc_num + 1] - indptr[doc_num]))\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for it in xrange(iters_count):\n",
    "        phi_matrix_tr = np.transpose(phi_matrix)\n",
    "        # следующая строчка это 60% времени работы алгоритма\n",
    "        s_data = loss_function.calc_der(inner1d(theta_matrix[docptr, :], phi_matrix_tr[wordptr, :]))\n",
    "        # следующая часть это 25% времени работы алгоритма\n",
    "        A = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                n_dw_matrix.data * s_data, \n",
    "                n_dw_matrix.indices, \n",
    "                n_dw_matrix.indptr\n",
    "            ), \n",
    "            shape=n_dw_matrix.shape\n",
    "        )\n",
    "        A_tr = A.tocsc().transpose()\n",
    "        # Остальное это 15% времени\n",
    "        n_tw = np.transpose(A_tr.dot(theta_matrix)) * phi_matrix\n",
    "        n_dt = A.dot(phi_matrix_tr) * theta_matrix\n",
    "        \n",
    "        r_tw, r_dt = regularization_list[it](n_tw, n_dt)\n",
    "        n_tw += r_tw\n",
    "        n_dt += n_dt\n",
    "        n_tw[n_tw < 0] = 0\n",
    "        n_dt[n_dt < 0] = 0\n",
    "        \n",
    "        phi_matrix = n_tw / np.sum(n_tw, axis=1)[:, np.newaxis]\n",
    "        theta_matrix = n_dt / np.sum(n_dt, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        if iteration_callback is not None:\n",
    "            iteration_callback(it, phi_matrix, theta_matrix)\n",
    "    \n",
    "    print 'Iters time', time.time() - start_time\n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive thetaless EM\n",
    "\n",
    "\n",
    "### Основная идея: давайте вообще не хранить $\\Theta$, а вместо этого вычислять её на лету одной итерацией ЕМ алгоритма, которую можно легко выписать.\n",
    "\n",
    "##### Пусть тематический профиль документа инициализирован равномерно, то для этого документа $p_{tdw} = \\frac{\\phi_{wt}}{\\sum_s \\phi_s} \\equiv \\overline{\\phi}_{wt} \\equiv (\\overline{\\Phi})_{wt} \\equiv p(t~|~w)$ . Эту матрицу легко рассчитать.\n",
    "##### На первой итерации  будет подсчитано $n_{td} = \\sum_{d} n_{dw} p_{tdw} = \\sum_{d} n_{dw} (\\overline{\\Phi})_{wt} = (N\\overline{\\Phi})_{dt}$\n",
    "##### И, соответственно, $\\theta_{td} = \\frac{n_{td}}{\\sum_t n_{td}} =  \\frac{n_{td}}{n_d}$\n",
    "##### Введём матрицу $B_{dw} \\equiv \\frac{n_{dw}}{n_d}$, тогда $\\Theta = B \\overline{\\Phi}$ \n",
    "##### Идеологически, мы зафиксировали, что $\\Theta$ - детерминированная функция от $\\Phi$. И теперь оптимизируем не $L(\\Phi, \\Theta)$, а $\\overline{L}(\\Phi) = L(\\Phi, B \\overline{\\Phi})$\n",
    "##### Наивность решения состоит в том, что мы полностью игнорируем любые действия с $\\Theta$ на М шаге. То есть мы не считаем $n_{td}$ и не обновляем $\\theta_{td}$ (этой матрицы вообще нет). А с $n_{wt}$ мы поступаем также как на обычном М шаге. Регуляризаторы на $\\Phi$ обрабатываются точно также как и раньше ($r_{wt}$ прибавляется к $n_{wt}$), а регуляризаторы $\\Theta$ игнорируются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_thetaless_em_optimization(\n",
    "    n_dw_matrix, \n",
    "    phi_matrix,\n",
    "    regularization_list,\n",
    "    iters_count=100,\n",
    "    iteration_callback=None\n",
    "):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    T = phi_matrix.shape[0]\n",
    "    phi_matrix = np.copy(phi_matrix)\n",
    "    docptr = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        docptr.extend([doc_num] * (indptr[doc_num + 1] - indptr[doc_num]))\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for it in xrange(iters_count):\n",
    "        phi_rev_matrix = np.transpose(phi_matrix / np.sum(phi_matrix, axis=0))\n",
    "        theta_matrix = n_dw_matrix.dot(phi_rev_matrix)\n",
    "        theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "        phi_matrix_tr = np.transpose(phi_matrix)\n",
    "        \n",
    "        s_data = 1. / inner1d(theta_matrix[docptr, :], phi_matrix_tr[wordptr, :])\n",
    "        A = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                n_dw_matrix.data  * s_data , \n",
    "                n_dw_matrix.indices, \n",
    "                n_dw_matrix.indptr\n",
    "            ), \n",
    "            shape=n_dw_matrix.shape\n",
    "        ).tocsc()\n",
    "            \n",
    "        n_tw = (A.T.dot(theta_matrix)).T * phi_matrix\n",
    "        r_tw, _ = regularization_list[it](n_tw, theta_matrix)\n",
    "        n_tw += r_tw\n",
    "        n_tw[n_tw < 0] = 0\n",
    "        phi_matrix = n_tw / np.sum(n_tw, axis=1)[:, np.newaxis]\n",
    "\n",
    "        if iteration_callback is not None:\n",
    "            iteration_callback(it, phi_matrix, theta_matrix)\n",
    "    \n",
    "    print 'Iters time', time.time() - start_time    \n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARTM thetaless EM optimization\n",
    "\n",
    "##### Данное решение исправляет \"наивность\" предыдущего подхода, учитывая зависимость от $\\Theta$. Фактически, это можно написать в виде регуляризатора. Чтобы понять, как именно это сделать, вспомним как работает EM алгоритм.\n",
    "\n",
    "##### На каждой итерации сначала определяются $p_{tdw}$, фиксируются, а затем строится функционал нижней оценки: $Q(\\Phi, \\Theta) = \\sum_{dtw} n_{dw} p_{tdw} \\left( \\ln \\phi_{wt} + \\ln \\theta_{td}\\right) + R(\\Phi, \\Theta)$. Цель М-шага увеличить значение данного функционала по сравнению с $\\Phi$ и $\\Theta$ с предыдущей итерации.\n",
    "\n",
    "##### Несмотря на то, что теперь $\\Theta$ это функция от $\\Phi$, тот факт, что это всё ещё нижняя оценка, никуда не пропадает. Поэтому теперь наша цель подобрать $\\Phi$, чтобы увеличить значение по сравнению с $\\Phi$ с предыдущей итерации следующий функционал: \n",
    "\n",
    "$\\sum_{dtw} n_{dw} p_{tdw} \\left( \\ln \\phi_{wt} + \\ln (\\Theta(\\Phi))_{dt}\\right) + R(\\Phi, \\Theta(\\Phi))$.\n",
    "\n",
    "##### Возьмём производные как обычно\n",
    "\n",
    "##### $\\frac{\\partial{Q}}{\\partial{\\phi_{vr}}} = \\frac{1}{\\phi_{vr}} \\left( \\sum_{d} n_{dv} p_{rdv} + \\phi_{vr} \\frac{\\partial{R}}{\\partial{\\phi_{vr}}} + \\sum_{dtw} n_{dw} p_{tdw} \\frac{1}{\\theta_{td}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} +  \\sum_{dt} \\frac{\\partial{R}}{\\partial{\\theta_{td}}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} \\right)$\n",
    "\n",
    "##### Так как $p_{tdw} = \\frac{\\phi_{wt} \\theta_{td}}{\\sum_s \\phi_{ws} \\theta_{sd}}$, то третье слагаемое можно упростить\n",
    "\n",
    "##### $\\sum_{dtw} n_{dw} p_{tdw} \\frac{1}{\\theta_{td}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} = \\sum_{dtw} n_{dw}\\frac{\\phi_{wt}}{\\sum_s \\phi_{ws} \\theta_{sd}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} = \\sum_{dtw} A_{dw} \\phi_{wt} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}}$\n",
    "\n",
    "##### Итого\n",
    "\n",
    "##### $\\frac{\\partial{Q}}{\\partial{\\phi_{vr}}} = \\frac{1}{\\phi_{vr}} \\left( \\sum_{d} n_{dv} p_{rdv} + \\phi_{vr}\\left( \\frac{\\partial{R}}{\\partial{\\phi_{vr}}} + \\sum_{dtw} A_{dw} \\phi_{wt} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} +  \\sum_{dt} \\frac{\\partial{R}}{\\partial{\\theta_{td}}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} \\right) \\right)$\n",
    "\n",
    "##### Обозначим за $C = A \\Phi^T + \\frac{\\partial{R}}{\\partial{\\Theta}}$, тогда\n",
    "$\\frac{\\partial{Q}}{\\partial{\\phi_{vr}}} = \\frac{1}{\\phi_{vr}} \\left( \\sum_{d} n_{dv} p_{rdv} + \\phi_{vr}\\left( \\frac{\\partial{R}}{\\partial{\\phi_{vr}}} + \\sum_{dt} C_{dt} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} \\right) \\right)$\n",
    "\n",
    "##### Как видим, получившийся остаток фактически и есть требуемый регуяризатор на $\\Phi$. Осталось только найти $\\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}}$\n",
    "\n",
    "##### $\\theta_{td} = \\sum_{w} B_{dw} \\frac{\\phi_{wt}}{\\sum_s \\phi_{ws}}$. Обозначим $\\frac{1}{\\sum_s \\phi_{ws}}$ за $norm_w$, тогда\n",
    "\n",
    "$\\theta_{td} = \\sum_{w} B_{dw} \\phi_{wt} norm_w$\n",
    "\n",
    "$\\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} =  \\sum_{w} B_{dw}~norm_w \\delta_{vwrt} +  \\sum_{w} B_{dw} \\phi_{wt} \\frac{\\partial{norm_w}}{\\partial{\\phi_{vr}}} = \n",
    "\\sum_{w} B_{dw} norm_w \\delta_{vwrt} - \\sum_{w} B_{dw}~\\phi_{wt}~norm_w^2~\\delta_{vw} =\n",
    "B_{dv}~norm_v~\\delta_{rt} - B_{dv}~\\phi_{vt}~norm_w^2\n",
    "$\n",
    "\n",
    "##### Тут $\\delta$ это символ Кронекера\n",
    "\n",
    "##### Теперь\n",
    "\n",
    "$\\sum_{dt} C_{dt} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} = \\sum_{dt} C_{dt} \\left( B_{dv}~norm_v~\\delta_{rt} - B_{dv}~\\phi_{vt}~norm_v^2 \\right) =  norm_v~\\sum_d C_{dr} B_{dv} -  norm_v^2~\\sum_{dt} C_{dt} B_{dv} \\phi_{vt} = norm_v (C^T B)_{rv} - norm_v^2 (\\Phi^T C^T B)_{vv}$\n",
    "\n",
    "##### В numpy можно вычислить только диагональ при помощи einsum, поэтому эту регуляризационную добавку можно эффективно вычислить\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artm_thetaless_em_optimization(\n",
    "    n_dw_matrix, \n",
    "    phi_matrix,\n",
    "    regularization_list,\n",
    "    iters_count=100,\n",
    "    iteration_callback=None\n",
    "):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    T = phi_matrix.shape[0]\n",
    "    phi_matrix = np.copy(phi_matrix)\n",
    "    docptr = []\n",
    "    docsizes = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        size = indptr[doc_num + 1] - indptr[doc_num]\n",
    "        docptr.extend([doc_num] * size)\n",
    "        docsizes.extend([size] * size)\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    docsizes = np.array(docsizes)\n",
    "    \n",
    "    B = scipy.sparse.csr_matrix(\n",
    "        (\n",
    "            1. * n_dw_matrix.data  / docsizes, \n",
    "            n_dw_matrix.indices, \n",
    "            n_dw_matrix.indptr\n",
    "        ), \n",
    "        shape=n_dw_matrix.shape\n",
    "    ).tocsc()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for it in xrange(iters_count):\n",
    "        word_norm = np.sum(phi_matrix, axis=0)\n",
    "        phi_rev_matrix = np.transpose(phi_matrix / word_norm)\n",
    "        \n",
    "        theta_matrix = n_dw_matrix.dot(phi_rev_matrix)\n",
    "        theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "        phi_matrix_tr = np.transpose(phi_matrix)\n",
    "        \n",
    "        s_data = 1. / inner1d(theta_matrix[docptr, :], phi_matrix_tr[wordptr, :])\n",
    "        A = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                n_dw_matrix.data  * s_data , \n",
    "                n_dw_matrix.indices, \n",
    "                n_dw_matrix.indptr\n",
    "            ), \n",
    "            shape=n_dw_matrix.shape\n",
    "        ).tocsc()\n",
    "            \n",
    "        n_tw = A.T.dot(theta_matrix).T * phi_matrix\n",
    "        \n",
    "        r_tw, r_dt = regularization_list[it](n_tw, theta_matrix)\n",
    "        theta_indices = theta_matrix > 0\n",
    "        r_dt[theta_indices] /= theta_matrix[theta_indices]\n",
    "        \n",
    "        g_dt = A.dot(phi_matrix_tr) + r_dt\n",
    "        tmp = g_dt.T * B / word_norm\n",
    "        r_tw += (tmp - np.einsum('ij,ji->i', phi_rev_matrix, tmp)) * phi_matrix\n",
    "        \n",
    "        n_tw += r_tw\n",
    "        n_tw[n_tw < 0] = 0\n",
    "        phi_matrix = n_tw / np.sum(n_tw, axis=1)[:, np.newaxis]\n",
    "\n",
    "        if iteration_callback is not None:\n",
    "            iteration_callback(it, phi_matrix, theta_matrix)\n",
    "    \n",
    "    print 'Iters time', time.time() - start_time    \n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient Descent\n",
    "\n",
    "##### Мы можем найти градиент оптимизируемой функции и сделать шаг вдоль него. Сделано для сравнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_optimization(\n",
    "    n_dw_matrix, \n",
    "    phi_matrix,\n",
    "    theta_matrix,\n",
    "    regularization_gradient_list,\n",
    "    iters_count=100,\n",
    "    loss_function=LogFunction(),\n",
    "    iteration_callback=None,\n",
    "    learning_rate=1.\n",
    "):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    T = phi_matrix.shape[0]\n",
    "    phi_matrix = np.copy(phi_matrix)\n",
    "    theta_matrix = np.copy(theta_matrix)\n",
    "    docptr = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        docptr.extend([doc_num] * (indptr[doc_num + 1] - indptr[doc_num]))\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for it in xrange(iters_count):\n",
    "        phi_matrix_tr = np.transpose(phi_matrix)\n",
    "        # следующая строчка это 60% времени работы алгоритма\n",
    "        s_data = loss_function.calc_der(inner1d(theta_matrix[docptr, :], phi_matrix_tr[wordptr, :]))\n",
    "        # следующая часть это 25% времени работы алгоритма\n",
    "        A = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                n_dw_matrix.data * s_data, \n",
    "                n_dw_matrix.indices, \n",
    "                n_dw_matrix.indptr\n",
    "            ), \n",
    "            shape=n_dw_matrix.shape\n",
    "        ).tocsc()\n",
    "        # Остальное это 15% времени\n",
    "        g_tw = theta_matrix.T * A\n",
    "        g_dt = A.dot(phi_matrix_tr)\n",
    "        \n",
    "        r_tw, r_dt = regularization_gradient_list[it](phi_matrix, theta_matrix)\n",
    "        g_tw += r_tw\n",
    "        g_dt += r_dt\n",
    "        \n",
    "        g_tw -= np.sum(g_tw * phi_matrix, axis=1)[:, np.newaxis]\n",
    "        g_dt -= np.sum(g_dt * theta_matrix, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        phi_matrix += g_tw * learning_rate\n",
    "        theta_matrix += g_dt * learning_rate\n",
    "        \n",
    "        phi_matrix[phi_matrix < 0] = 0\n",
    "        theta_matrix[theta_matrix < 0] = 0\n",
    "        \n",
    "        phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "        theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        if iteration_callback is not None:\n",
    "            iteration_callback(it, phi_matrix, theta_matrix)\n",
    "    \n",
    "    print 'Iters time', time.time() - start_time  \n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Оценка качества классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_score(theta, targets):\n",
    "    C_2d_range = [1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
    "    gamma_2d_range = [1e-3, 1e-2, 1e-1, 1, 1e1]\n",
    "    best_C, best_gamma, best_val = None, None, 0.\n",
    "    best_cv_algo_score_on_test = 0.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(theta, targets, test_size=0.30, stratify=targets, random_state=42)\n",
    "    for C in C_2d_range:\n",
    "        for gamma in gamma_2d_range:\n",
    "            val = np.mean(cross_val_score(SVC(C=C, gamma=gamma), X_train, y_train, scoring='accuracy', cv=4))\n",
    "            algo = SVC(C=C, gamma=gamma).fit(X_train, y_train)\n",
    "            test_score = accuracy_score(y_test, algo.predict(X_test))\n",
    "            print 'SVM(C={}, gamma={}) cv-score: {}  test-score: {}'.format(\n",
    "                C,\n",
    "                gamma,\n",
    "                round(val, 3),\n",
    "                round(test_score, 3)\n",
    "            )\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_C = C\n",
    "                best_gamma = gamma\n",
    "                best_cv_algo_score_on_test = test_score\n",
    "    print '\\n\\n\\nBest cv params: C={}, gamma={}\\nCV score: {}\\nTest score:{}'.format(\n",
    "        best_C,\n",
    "        best_gamma,\n",
    "        round(best_val, 3),\n",
    "        round(best_cv_algo_score_on_test, 3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artm_calc_topic_correlation(phi):\n",
    "    T, W = phi.shape\n",
    "    return (np.sum(np.sum(phi, axis=0) ** 2) - np.sum(phi ** 2)) / (T * (T - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artm_calc_perplexity_factory(n_dw_matrix):\n",
    "    helper = create_calculate_likelihood_like_function(\n",
    "        loss_function=LogFunction(),\n",
    "        n_dw_matrix=n_dw_matrix\n",
    "    )\n",
    "    total_words_number = n_dw_matrix.sum()\n",
    "    return lambda phi, theta: np.exp(- helper(phi, theta) / total_words_number)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, documents_number, top_size):\n",
    "    def fun(phi):\n",
    "        T, W = phi.shape\n",
    "        pmi = 0.\n",
    "        for t in xrange(T):\n",
    "            top = heapq.nlargest(top_size, xrange(W), key=lambda w: phi[t, w])\n",
    "            for w1 in top:\n",
    "                for w2 in top:\n",
    "                    if w1 != w2:\n",
    "                        pmi += np.log(documents_number * (doc_cooccurences[(w1, w2)] + 0.1) * 1. / doc_occurences[w1] / doc_occurences[w2])\n",
    "        return pmi / (T * top_size * (top_size - 1))\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры запусков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLSA: EM optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3986.0704926\n",
      "1 3906.19009155\n",
      "2 3816.50258883\n",
      "3 3706.06393687\n",
      "4 3572.34675498\n",
      "5 3422.22011816\n",
      "6 3268.08384344\n",
      "7 3119.83183247\n",
      "8 2983.70125846\n",
      "9 2863.18846394\n",
      "10 2759.69951822\n",
      "11 2672.42541819\n",
      "12 2599.05123244\n",
      "13 2536.97312372\n",
      "14 2483.83221649\n",
      "15 2437.75378852\n",
      "16 2397.31633049\n",
      "17 2361.2891356\n",
      "18 2328.82670318\n",
      "19 2299.34993209\n",
      "20 2272.58561051\n",
      "21 2248.73220363\n",
      "22 2227.65996306\n",
      "23 2208.70765597\n",
      "24 2191.35635596\n",
      "25 2174.89348508\n",
      "26 2159.11967244\n",
      "27 2144.42764304\n",
      "28 2131.28250351\n",
      "29 2119.31240838\n",
      "30 2108.36589959\n",
      "31 2098.46248252\n",
      "32 2089.30733262\n",
      "33 2081.12016961\n",
      "34 2073.44101797\n",
      "35 2066.20339851\n",
      "36 2059.38450385\n",
      "37 2053.0078317\n",
      "38 2046.96650212\n",
      "39 2041.17463077\n",
      "40 2035.4610121\n",
      "41 2029.96956402\n",
      "42 2024.71547536\n",
      "43 2019.67900783\n",
      "44 2014.82612812\n",
      "45 2010.11252984\n",
      "46 2005.54425925\n",
      "47 2001.10582942\n",
      "48 1996.92925567\n",
      "49 1993.06135312\n",
      "50 1989.37842384\n",
      "51 1985.82831163\n",
      "52 1982.42779562\n",
      "53 1979.20828384\n",
      "54 1976.22776342\n",
      "55 1973.49514204\n",
      "56 1970.91173228\n",
      "57 1968.36860342\n",
      "58 1965.85370816\n",
      "59 1963.40595461\n",
      "60 1961.12623071\n",
      "61 1958.965441\n",
      "62 1956.93703538\n",
      "63 1955.07347686\n",
      "64 1953.29567494\n",
      "65 1951.60438027\n",
      "66 1950.02060316\n",
      "67 1948.48911878\n",
      "68 1946.96359044\n",
      "69 1945.45878197\n",
      "70 1944.01738116\n",
      "71 1942.59535662\n",
      "72 1941.20357724\n",
      "73 1939.88246022\n",
      "74 1938.63416694\n",
      "75 1937.42081995\n",
      "76 1936.21654796\n",
      "77 1935.00300288\n",
      "78 1933.79669128\n",
      "79 1932.69144487\n",
      "80 1931.70716259\n",
      "81 1930.77261384\n",
      "82 1929.83407443\n",
      "83 1928.87729396\n",
      "84 1927.90589349\n",
      "85 1926.92901065\n",
      "86 1925.96649274\n",
      "87 1925.04324945\n",
      "88 1924.17951915\n",
      "89 1923.37279385\n",
      "90 1922.60043027\n",
      "91 1921.86880874\n",
      "92 1921.20305105\n",
      "93 1920.57121616\n",
      "94 1919.95984756\n",
      "95 1919.36367202\n",
      "96 1918.76684653\n",
      "97 1918.13890455\n",
      "98 1917.52405437\n",
      "99 1916.96283418\n",
      "100 1916.45127549\n",
      "101 1915.94582385\n",
      "102 1915.41791624\n",
      "103 1914.87795757\n",
      "104 1914.36691098\n",
      "105 1913.87040349\n",
      "106 1913.3417976\n",
      "107 1912.7771315\n",
      "108 1912.2438752\n",
      "109 1911.7919402\n",
      "110 1911.39811781\n",
      "111 1911.01409667\n",
      "112 1910.62369642\n",
      "113 1910.23432361\n",
      "114 1909.85375106\n",
      "115 1909.47392237\n",
      "116 1909.08976778\n",
      "117 1908.69857866\n",
      "118 1908.30326462\n",
      "119 1907.93751198\n",
      "120 1907.55970561\n",
      "121 1907.16402885\n",
      "122 1906.78669467\n",
      "123 1906.43157718\n",
      "124 1906.10215469\n",
      "125 1905.77739551\n",
      "126 1905.4244275\n",
      "127 1905.01930252\n",
      "128 1904.63587106\n",
      "129 1904.30411033\n",
      "130 1903.99205957\n",
      "131 1903.68301703\n",
      "132 1903.38326979\n",
      "133 1903.09501518\n",
      "134 1902.81747478\n",
      "135 1902.53624886\n",
      "136 1902.26401867\n",
      "137 1902.01330198\n",
      "138 1901.75978845\n",
      "139 1901.50139585\n",
      "140 1901.24865034\n",
      "141 1901.00144628\n",
      "142 1900.75927394\n",
      "143 1900.50200169\n",
      "144 1900.23109194\n",
      "145 1899.97800091\n",
      "146 1899.74512701\n",
      "147 1899.52893752\n",
      "148 1899.33209265\n",
      "149 1899.12498819\n",
      "150 1898.90574629\n",
      "151 1898.67777345\n",
      "152 1898.43134601\n",
      "153 1898.18250722\n",
      "154 1897.96597963\n",
      "155 1897.77710366\n",
      "156 1897.60104738\n",
      "157 1897.43079496\n",
      "158 1897.25589949\n",
      "159 1897.07352998\n",
      "160 1896.89551033\n",
      "161 1896.72525175\n",
      "162 1896.5563074\n",
      "163 1896.37900209\n",
      "164 1896.18935635\n",
      "165 1896.00286627\n",
      "166 1895.82824747\n",
      "167 1895.6540718\n",
      "168 1895.46985361\n",
      "169 1895.28053979\n",
      "170 1895.09864015\n",
      "171 1894.93347114\n",
      "172 1894.78400876\n",
      "173 1894.64465059\n",
      "174 1894.50665621\n",
      "175 1894.36657517\n",
      "176 1894.2320339\n",
      "177 1894.10065722\n",
      "178 1893.97238464\n",
      "179 1893.83994688\n",
      "180 1893.69469621\n",
      "181 1893.52839492\n",
      "182 1893.33139332\n",
      "183 1893.11782204\n",
      "184 1892.91003881\n",
      "185 1892.7030789\n",
      "186 1892.51973988\n",
      "187 1892.35158939\n",
      "188 1892.17491746\n",
      "189 1891.98550762\n",
      "190 1891.79363223\n",
      "191 1891.60354794\n",
      "192 1891.41583659\n",
      "193 1891.23868735\n",
      "194 1891.0882744\n",
      "195 1890.95542954\n",
      "196 1890.82740356\n",
      "197 1890.69758296\n",
      "198 1890.57140675\n",
      "199 1890.44755589\n",
      "Iters time 24.6246218681\n",
      "Iters time 13.041162014\n"
     ]
    }
   ],
   "source": [
    "D, W = origin_n_dw_matrix.shape\n",
    "T = 10\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.random.uniform(size=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = trivial_regularization\n",
    "\n",
    "calc_log_likelihood = create_calculate_likelihood_like_function(\n",
    "    loss_function=LogFunction(),\n",
    "    n_dw_matrix=origin_n_dw_matrix\n",
    ")\n",
    "\n",
    "total_words_number = origin_n_dw_matrix.sum()\n",
    "def callback(it, phi, theta):\n",
    "    print it,  np.exp(- calc_log_likelihood(phi, theta) / total_words_number)\n",
    "\n",
    "phi, theta = em_optimization(\n",
    "    n_dw_matrix=origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    theta_matrix=theta_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200,\n",
    "    loss_function=LogFunction(),\n",
    "    iteration_callback=callback\n",
    ")\n",
    "\n",
    "phi, theta = em_optimization(\n",
    "    n_dw_matrix=origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    theta_matrix=theta_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200,\n",
    "    loss_function=LogFunction()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phi_plsa_em = phi\n",
    "theta_plsa_em = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 1890.44755589\n",
      "Topic correlation 0.000472938245581\n",
      "Avg top5 pmi 1.32100331417\n",
      "Avg top10 pmi 1.14489311995\n",
      "Avg top20 pmi 1.16503148316\n",
      "Sparsity 0.672531979978\n"
     ]
    }
   ],
   "source": [
    "print 'Perplexity', artm_calc_perplexity_factory(origin_n_dw_matrix)(phi, theta)\n",
    "print 'Topic correlation', artm_calc_topic_correlation(phi)\n",
    "print 'Avg top5 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 5)(phi)\n",
    "print 'Avg top10 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 10)(phi)\n",
    "print 'Avg top20 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 20)(phi)\n",
    "print 'Sparsity', 1. * np.sum(phi < 1e-20) / np.sum(phi >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(C=0.1, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.305  test-score: 0.486\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.676  test-score: 0.683\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.718  test-score: 0.722\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.728  test-score: 0.736\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.31  test-score: 0.493\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.673  test-score: 0.683\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.72  test-score: 0.723\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.742  test-score: 0.743\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.743  test-score: 0.745\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.672  test-score: 0.684\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.719  test-score: 0.723\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.74  test-score: 0.738\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.755  test-score: 0.743\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.725  test-score: 0.734\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.719  test-score: 0.722\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.739  test-score: 0.736\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.75  test-score: 0.742\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.748  test-score: 0.747\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.7  test-score: 0.722\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.737  test-score: 0.736\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.742  test-score: 0.745\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.753  test-score: 0.745\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.743  test-score: 0.743\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.688  test-score: 0.701\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.74  test-score: 0.741\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.749  test-score: 0.741\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.751  test-score: 0.748\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.716  test-score: 0.724\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.687  test-score: 0.682\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=10.0, gamma=1\n",
      "CV score: 0.755\n",
      "Test score:0.743\n"
     ]
    }
   ],
   "source": [
    "svm_score(theta, doc_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA: EM optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3987.37646827\n",
      "1 3897.4074752\n",
      "2 3798.04728921\n",
      "3 3680.90916256\n",
      "4 3546.86075721\n",
      "5 3404.40082914\n",
      "6 3264.35983653\n",
      "7 3133.42614365\n",
      "8 3015.05899483\n",
      "9 2911.23100587\n",
      "10 2822.68526189\n",
      "11 2748.84376085\n",
      "12 2687.63946797\n",
      "13 2636.38528098\n",
      "14 2592.68187264\n",
      "15 2555.07686584\n",
      "16 2523.26596425\n",
      "17 2497.10117303\n",
      "18 2475.8452058\n",
      "19 2458.50015838\n",
      "20 2444.08698345\n",
      "21 2431.83457879\n",
      "22 2421.18913973\n",
      "23 2411.9411239\n",
      "24 2403.92060151\n",
      "25 2396.88272169\n",
      "26 2390.67929499\n",
      "27 2385.21190858\n",
      "28 2380.39755883\n",
      "29 2376.12819046\n",
      "30 2372.32169245\n",
      "31 2368.94321943\n",
      "32 2365.92689874\n",
      "33 2363.16733183\n",
      "34 2360.63018304\n",
      "35 2358.32615751\n",
      "36 2356.24209776\n",
      "37 2354.33902549\n",
      "38 2352.5797764\n",
      "39 2350.94714679\n",
      "40 2349.42551463\n",
      "41 2348.00372283\n",
      "42 2346.66906665\n",
      "43 2345.42598409\n",
      "44 2344.2645335\n",
      "45 2343.1731887\n",
      "46 2342.13828756\n",
      "47 2341.14797914\n",
      "48 2340.19949656\n",
      "49 2339.29619816\n",
      "50 2338.44005861\n",
      "51 2337.62109798\n",
      "52 2336.83933634\n",
      "53 2336.09686008\n",
      "54 2335.38957787\n",
      "55 2334.70918016\n",
      "56 2334.04792875\n",
      "57 2333.42233786\n",
      "58 2332.83523389\n",
      "59 2332.27917001\n",
      "60 2331.75305196\n",
      "61 2331.25271696\n",
      "62 2330.77615874\n",
      "63 2330.33010407\n",
      "64 2329.9150551\n",
      "65 2329.52089968\n",
      "66 2329.14322494\n",
      "67 2328.78287141\n",
      "68 2328.44216167\n",
      "69 2328.12517492\n",
      "70 2327.82241853\n",
      "71 2327.5418305\n",
      "72 2327.28474822\n",
      "73 2327.04402804\n",
      "74 2326.81553894\n",
      "75 2326.60272005\n",
      "76 2326.40562719\n",
      "77 2326.22119101\n",
      "78 2326.04750722\n",
      "79 2325.87940292\n",
      "80 2325.71403989\n",
      "81 2325.54838716\n",
      "82 2325.38345598\n",
      "83 2325.21904186\n",
      "84 2325.06075757\n",
      "85 2324.90832362\n",
      "86 2324.75886924\n",
      "87 2324.61477147\n",
      "88 2324.48024014\n",
      "89 2324.3518429\n",
      "90 2324.22442069\n",
      "91 2324.10467562\n",
      "92 2323.99363126\n",
      "93 2323.8882548\n",
      "94 2323.78754344\n",
      "95 2323.6881951\n",
      "96 2323.58968282\n",
      "97 2323.49561163\n",
      "98 2323.40154587\n",
      "99 2323.3073218\n",
      "100 2323.21623718\n",
      "101 2323.131387\n",
      "102 2323.0527401\n",
      "103 2322.97873245\n",
      "104 2322.90705203\n",
      "105 2322.8371887\n",
      "106 2322.76968163\n",
      "107 2322.70323786\n",
      "108 2322.63449177\n",
      "109 2322.55920412\n",
      "110 2322.48739965\n",
      "111 2322.42325992\n",
      "112 2322.36022314\n",
      "113 2322.29867669\n",
      "114 2322.24079467\n",
      "115 2322.18437386\n",
      "116 2322.1273491\n",
      "117 2322.07313962\n",
      "118 2322.02334236\n",
      "119 2321.97659422\n",
      "120 2321.93133353\n",
      "121 2321.88651536\n",
      "122 2321.84136914\n",
      "123 2321.79557275\n",
      "124 2321.75014859\n",
      "125 2321.70511034\n",
      "126 2321.65801661\n",
      "127 2321.61019922\n",
      "128 2321.56611439\n",
      "129 2321.52545537\n",
      "130 2321.48717802\n",
      "131 2321.4508752\n",
      "132 2321.4157395\n",
      "133 2321.3815712\n",
      "134 2321.34876435\n",
      "135 2321.31850515\n",
      "136 2321.29044337\n",
      "137 2321.26345368\n",
      "138 2321.23523175\n",
      "139 2321.20800362\n",
      "140 2321.18570186\n",
      "141 2321.16511486\n",
      "142 2321.14301329\n",
      "143 2321.11840517\n",
      "144 2321.094345\n",
      "145 2321.07300953\n",
      "146 2321.05220663\n",
      "147 2321.03173697\n",
      "148 2321.01252794\n",
      "149 2320.99460846\n",
      "150 2320.97827487\n",
      "151 2320.96322912\n",
      "152 2320.94811306\n",
      "153 2320.93295492\n",
      "154 2320.91739542\n",
      "155 2320.9001958\n",
      "156 2320.87804102\n",
      "157 2320.84943973\n",
      "158 2320.82267776\n",
      "159 2320.79978337\n",
      "160 2320.78039855\n",
      "161 2320.76345968\n",
      "162 2320.74749676\n",
      "163 2320.73160032\n",
      "164 2320.71576767\n",
      "165 2320.69974005\n",
      "166 2320.68282179\n",
      "167 2320.66512145\n",
      "168 2320.64722048\n",
      "169 2320.6299356\n",
      "170 2320.61298627\n",
      "171 2320.59599604\n",
      "172 2320.57892294\n",
      "173 2320.56258844\n",
      "174 2320.54796558\n",
      "175 2320.53491176\n",
      "176 2320.5221391\n",
      "177 2320.50882887\n",
      "178 2320.49497395\n",
      "179 2320.48030399\n",
      "180 2320.464433\n",
      "181 2320.44664265\n",
      "182 2320.42617924\n",
      "183 2320.40450954\n",
      "184 2320.38385666\n",
      "185 2320.3647097\n",
      "186 2320.34707555\n",
      "187 2320.33002237\n",
      "188 2320.31278378\n",
      "189 2320.29536408\n",
      "190 2320.27959458\n",
      "191 2320.26553364\n",
      "192 2320.25259081\n",
      "193 2320.24069567\n",
      "194 2320.23016036\n",
      "195 2320.22002878\n",
      "196 2320.20936655\n",
      "197 2320.19732619\n",
      "198 2320.18152892\n",
      "199 2320.15559914\n",
      "Iters time 24.9970829487\n",
      "Iters time 12.7371869087\n"
     ]
    }
   ],
   "source": [
    "D, W = origin_n_dw_matrix.shape\n",
    "T = 10\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.random.uniform(size=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = create_reg_lda(-0.1, -0.1)\n",
    "\n",
    "calc_log_likelihood = create_calculate_likelihood_like_function(\n",
    "    loss_function=LogFunction(),\n",
    "    n_dw_matrix=origin_n_dw_matrix\n",
    ")\n",
    "\n",
    "total_words_number = origin_n_dw_matrix.sum()\n",
    "def callback(it, phi, theta):\n",
    "    print it,  np.exp(- calc_log_likelihood(phi, theta) / total_words_number)\n",
    "\n",
    "phi, theta = em_optimization(\n",
    "    n_dw_matrix=origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    theta_matrix=theta_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200,\n",
    "    loss_function=LogFunction(),\n",
    "    iteration_callback=callback\n",
    ")\n",
    "\n",
    "phi, theta = em_optimization(\n",
    "    n_dw_matrix=origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    theta_matrix=theta_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200,\n",
    "    loss_function=LogFunction()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 2320.15559914\n",
      "Topic correlation 0.000401210504811\n",
      "Avg top5 pmi 1.13836129491\n",
      "Avg top10 pmi 1.07842788999\n",
      "Avg top20 pmi 1.09476761913\n",
      "Sparsity 0.819639877642\n"
     ]
    }
   ],
   "source": [
    "print 'Perplexity', artm_calc_perplexity_factory(origin_n_dw_matrix)(phi, theta)\n",
    "print 'Topic correlation', artm_calc_topic_correlation(phi)\n",
    "print 'Avg top5 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 5)(phi)\n",
    "print 'Avg top10 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 10)(phi)\n",
    "print 'Avg top20 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 20)(phi)\n",
    "print 'Sparsity', 1. * np.sum(phi < 1e-20) / np.sum(phi >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(C=0.1, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.602  test-score: 0.597\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.641  test-score: 0.641\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.655  test-score: 0.664\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.602  test-score: 0.595\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.641  test-score: 0.639\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.668  test-score: 0.675\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.662  test-score: 0.677\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.602  test-score: 0.594\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.641  test-score: 0.638\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.662  test-score: 0.663\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.676  test-score: 0.68\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.637  test-score: 0.66\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.64  test-score: 0.638\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.66  test-score: 0.66\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.67  test-score: 0.675\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.668  test-score: 0.68\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.599  test-score: 0.61\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.659  test-score: 0.659\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.663  test-score: 0.664\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.672  test-score: 0.678\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.652  test-score: 0.663\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.584  test-score: 0.577\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.66  test-score: 0.661\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.669  test-score: 0.675\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.67  test-score: 0.674\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.625  test-score: 0.649\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.58  test-score: 0.564\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=10.0, gamma=1\n",
      "CV score: 0.676\n",
      "Test score:0.68\n"
     ]
    }
   ],
   "source": [
    "svm_score(theta, doc_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLSA: naive thetaless EM optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4028.35177178\n",
      "1 4021.70284106\n",
      "2 4010.85417715\n",
      "3 3992.47680023\n",
      "4 3962.57281777\n",
      "5 3918.66130822\n",
      "6 3858.60944736\n",
      "7 3779.02320026\n",
      "8 3678.00890305\n",
      "9 3557.60111366\n",
      "10 3424.09415717\n",
      "11 3286.93928091\n",
      "12 3155.88858099\n",
      "13 3037.63331739\n",
      "14 2934.65975247\n",
      "15 2846.60272155\n",
      "16 2771.81820433\n",
      "17 2708.09139679\n",
      "18 2653.23728614\n",
      "19 2605.52734057\n",
      "20 2563.82805547\n",
      "21 2527.47902652\n",
      "22 2496.01808851\n",
      "23 2468.9673896\n",
      "24 2445.77497288\n",
      "25 2425.86697009\n",
      "26 2408.70801891\n",
      "27 2393.82768905\n",
      "28 2380.85744338\n",
      "29 2369.4991576\n",
      "30 2359.4620215\n",
      "31 2350.46863365\n",
      "32 2342.29522488\n",
      "33 2334.77867514\n",
      "34 2327.84692884\n",
      "35 2321.57292316\n",
      "36 2316.02284294\n",
      "37 2311.15125784\n",
      "38 2306.84215015\n",
      "39 2302.97900869\n",
      "40 2299.47429011\n",
      "41 2296.27278533\n",
      "42 2293.34037007\n",
      "43 2290.65809191\n",
      "44 2288.1943657\n",
      "45 2285.92025384\n",
      "46 2283.81069361\n",
      "47 2281.83589404\n",
      "48 2279.96375713\n",
      "49 2278.17297431\n",
      "50 2276.45806591\n",
      "51 2274.82519468\n",
      "52 2273.27885688\n",
      "53 2271.79915634\n",
      "54 2270.35432729\n",
      "55 2268.92474786\n",
      "56 2267.50940967\n",
      "57 2266.12473908\n",
      "58 2264.8033004\n",
      "59 2263.57547476\n",
      "60 2262.43447918\n",
      "61 2261.33310658\n",
      "62 2260.22047473\n",
      "63 2259.0903221\n",
      "64 2257.96101759\n",
      "65 2256.83874271\n",
      "66 2255.74521675\n",
      "67 2254.70174238\n",
      "68 2253.76309614\n",
      "69 2252.91418772\n",
      "70 2252.1212363\n",
      "71 2251.37574874\n",
      "72 2250.6781632\n",
      "73 2250.01692952\n",
      "74 2249.36162022\n",
      "75 2248.71555251\n",
      "76 2248.1167606\n",
      "77 2247.56261022\n",
      "78 2247.03423359\n",
      "79 2246.48713755\n",
      "80 2245.91441066\n",
      "81 2245.37865135\n",
      "82 2244.89223354\n",
      "83 2244.44893714\n",
      "84 2244.02867519\n",
      "85 2243.62336988\n",
      "86 2243.22301232\n",
      "87 2242.82077091\n",
      "88 2242.42242205\n",
      "89 2242.03119325\n",
      "90 2241.64829111\n",
      "91 2241.28264415\n",
      "92 2240.93548664\n",
      "93 2240.59965196\n",
      "94 2240.26755079\n",
      "95 2239.94095367\n",
      "96 2239.6256652\n",
      "97 2239.32183559\n",
      "98 2239.02224247\n",
      "99 2238.716948\n",
      "100 2238.40217407\n",
      "101 2238.08728305\n",
      "102 2237.78787429\n",
      "103 2237.50960317\n",
      "104 2237.24278027\n",
      "105 2236.98090701\n",
      "106 2236.72229518\n",
      "107 2236.46767647\n",
      "108 2236.21828153\n",
      "109 2235.97380234\n",
      "110 2235.73356301\n",
      "111 2235.49652073\n",
      "112 2235.26135444\n",
      "113 2235.02741108\n",
      "114 2234.79450022\n",
      "115 2234.56175527\n",
      "116 2234.32639705\n",
      "117 2234.08939873\n",
      "118 2233.85470982\n",
      "119 2233.62136025\n",
      "120 2233.38891019\n",
      "121 2233.15656698\n",
      "122 2232.9231479\n",
      "123 2232.68779968\n",
      "124 2232.45028639\n",
      "125 2232.21036679\n",
      "126 2231.96779937\n",
      "127 2231.72285228\n",
      "128 2231.47684817\n",
      "129 2231.23153111\n",
      "130 2230.98785929\n",
      "131 2230.74725272\n",
      "132 2230.51215753\n",
      "133 2230.28436543\n",
      "134 2230.06459559\n",
      "135 2229.85176648\n",
      "136 2229.64485444\n",
      "137 2229.44465981\n",
      "138 2229.25007571\n",
      "139 2229.05944287\n",
      "140 2228.87202927\n",
      "141 2228.688057\n",
      "142 2228.50885496\n",
      "143 2228.33487437\n",
      "144 2228.16495706\n",
      "145 2227.99770403\n",
      "146 2227.83174583\n",
      "147 2227.66570449\n",
      "148 2227.49845246\n",
      "149 2227.32962502\n",
      "150 2227.15982181\n",
      "151 2226.99001\n",
      "152 2226.82135836\n",
      "153 2226.65548329\n",
      "154 2226.49396745\n",
      "155 2226.33724801\n",
      "156 2226.18424926\n",
      "157 2226.03510207\n",
      "158 2225.89076192\n",
      "159 2225.75019544\n",
      "160 2225.61279323\n",
      "161 2225.47804918\n",
      "162 2225.34485407\n",
      "163 2225.2118399\n",
      "164 2225.07833947\n",
      "165 2224.94496374\n",
      "166 2224.81319548\n",
      "167 2224.68403679\n",
      "168 2224.5574279\n",
      "169 2224.43297978\n",
      "170 2224.31026179\n",
      "171 2224.18828572\n",
      "172 2224.06576209\n",
      "173 2223.94233065\n",
      "174 2223.81905572\n",
      "175 2223.69767209\n",
      "176 2223.57992084\n",
      "177 2223.46638781\n",
      "178 2223.35585515\n",
      "179 2223.24644079\n",
      "180 2223.13637673\n",
      "181 2223.02405341\n",
      "182 2222.90783064\n",
      "183 2222.78592832\n",
      "184 2222.65682103\n",
      "185 2222.51992854\n",
      "186 2222.37520847\n",
      "187 2222.22237253\n",
      "188 2222.06179103\n",
      "189 2221.89587045\n",
      "190 2221.72915928\n",
      "191 2221.56693031\n",
      "192 2221.41352883\n",
      "193 2221.27127966\n",
      "194 2221.14009183\n",
      "195 2221.01792295\n",
      "196 2220.9016561\n",
      "197 2220.78826488\n",
      "198 2220.67630503\n",
      "199 2220.56651773\n",
      "Iters time 23.3136479855\n",
      "Iters time 12.8049178123\n"
     ]
    }
   ],
   "source": [
    "D, W = origin_n_dw_matrix.shape\n",
    "T = 10\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.random.uniform(size=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = trivial_regularization\n",
    "\n",
    "calc_log_likelihood = create_calculate_likelihood_like_function(\n",
    "    loss_function=LogFunction(),\n",
    "    n_dw_matrix=origin_n_dw_matrix\n",
    ")\n",
    "\n",
    "total_words_number = origin_n_dw_matrix.sum()\n",
    "def callback(it, phi, theta):\n",
    "    print it,  np.exp(- calc_log_likelihood(phi, theta) / total_words_number)\n",
    "\n",
    "phi, theta = naive_thetaless_em_optimization(\n",
    "    n_dw_matrix=origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200,\n",
    "    iteration_callback=callback\n",
    ")\n",
    "\n",
    "phi, theta = naive_thetaless_em_optimization(\n",
    "    n_dw_matrix=origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 2220.56651773\n",
      "Topic correlation 0.000290318954988\n",
      "Avg top5 pmi 1.02330066834\n",
      "Avg top10 pmi 1.16376209817\n",
      "Avg top20 pmi 1.21430217591\n",
      "Sparsity 0.720724416018\n"
     ]
    }
   ],
   "source": [
    "print 'Perplexity', artm_calc_perplexity_factory(origin_n_dw_matrix)(phi, theta)\n",
    "print 'Topic correlation', artm_calc_topic_correlation(phi)\n",
    "print 'Avg top5 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 5)(phi)\n",
    "print 'Avg top10 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 10)(phi)\n",
    "print 'Avg top20 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 20)(phi)\n",
    "print 'Sparsity', 1. * np.sum(phi < 1e-20) / np.sum(phi >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(C=0.1, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.784  test-score: 0.794\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.818  test-score: 0.817\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.824  test-score: 0.827\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.783  test-score: 0.796\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.821  test-score: 0.819\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.831  test-score: 0.832\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.837  test-score: 0.841\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.782  test-score: 0.795\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.821  test-score: 0.819\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.835  test-score: 0.83\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.841  test-score: 0.835\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.829  test-score: 0.832\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.821  test-score: 0.819\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.834  test-score: 0.826\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.838  test-score: 0.834\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.835  test-score: 0.834\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.808  test-score: 0.806\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.834  test-score: 0.826\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.838  test-score: 0.832\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.841  test-score: 0.838\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.829  test-score: 0.826\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.779  test-score: 0.776\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.838  test-score: 0.831\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.838  test-score: 0.834\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.834  test-score: 0.831\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.812  test-score: 0.813\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.772  test-score: 0.771\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=10.0, gamma=1\n",
      "CV score: 0.841\n",
      "Test score:0.835\n"
     ]
    }
   ],
   "source": [
    "svm_score(theta, doc_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Может быть матрица $\\Phi$ от PLSA тоже хороша, если $\\Theta$  по ней вычислить по нашим формулам (это одна итерация наивного алгоритма)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters time 0.0651209354401\n"
     ]
    }
   ],
   "source": [
    "phi, theta = naive_thetaless_em_optimization(\n",
    "    n_dw_matrix=origin_n_dw_matrix, \n",
    "    phi_matrix=phi_plsa_em,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 2465.80644291\n",
      "Topic correlation 0.000492166435216\n",
      "Avg top5 pmi 1.19327254829\n",
      "Avg top10 pmi 1.06846309606\n",
      "Avg top20 pmi 1.10763352193\n",
      "Sparsity 0.672712736374\n"
     ]
    }
   ],
   "source": [
    "print 'Perplexity', artm_calc_perplexity_factory(origin_n_dw_matrix)(phi, theta)\n",
    "print 'Topic correlation', artm_calc_topic_correlation(phi)\n",
    "print 'Avg top5 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 5)(phi)\n",
    "print 'Avg top10 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 10)(phi)\n",
    "print 'Avg top20 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 20)(phi)\n",
    "print 'Sparsity', 1. * np.sum(phi < 1e-20) / np.sum(phi >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(C=0.1, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.627  test-score: 0.667\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.76  test-score: 0.763\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.795  test-score: 0.791\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.633  test-score: 0.664\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.759  test-score: 0.763\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.798  test-score: 0.791\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.811  test-score: 0.805\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.633  test-score: 0.663\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.76  test-score: 0.762\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.796  test-score: 0.787\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.807  test-score: 0.809\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.809  test-score: 0.808\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.76  test-score: 0.762\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.796  test-score: 0.79\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.802  test-score: 0.801\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.809  test-score: 0.808\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.799  test-score: 0.799\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.796  test-score: 0.79\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.801  test-score: 0.795\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.805  test-score: 0.81\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.81  test-score: 0.801\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.772  test-score: 0.772\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.8  test-score: 0.792\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.801  test-score: 0.802\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.808  test-score: 0.809\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.799  test-score: 0.806\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.751  test-score: 0.75\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=1.0, gamma=10.0\n",
      "CV score: 0.811\n",
      "Test score:0.805\n"
     ]
    }
   ],
   "source": [
    "svm_score(theta, doc_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стало лучше, но не так хорошо как нашим методом + существенно менее разрежена + перплексия хуже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLSA: ARTM thetaless optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4024.0085702\n",
      "1 3991.50885265\n",
      "2 3911.89488681\n",
      "3 3756.1893153\n",
      "4 3493.3368556\n",
      "5 3171.316171\n",
      "6 2915.60988676\n",
      "7 2746.45369508\n",
      "8 2635.80508596\n",
      "9 2560.07395308\n",
      "10 2505.06796354\n",
      "11 2462.82826338\n",
      "12 2429.00455576\n",
      "13 2400.8921662\n",
      "14 2377.21732813\n",
      "15 2358.5833659\n",
      "16 2343.64807915\n",
      "17 2330.81632703\n",
      "18 2319.79565166\n",
      "19 2310.80408502\n",
      "20 2303.31341941\n",
      "21 2297.08052617\n",
      "22 2291.69432531\n",
      "23 2286.87792991\n",
      "24 2282.7277651\n",
      "25 2278.97319887\n",
      "26 2275.56004815\n",
      "27 2272.38892231\n",
      "28 2269.57526931\n",
      "29 2267.0551283\n",
      "30 2264.7551971\n",
      "31 2262.62370833\n",
      "32 2260.66444089\n",
      "33 2258.79769608\n",
      "34 2257.03570419\n",
      "35 2255.33731105\n",
      "36 2253.74794326\n",
      "37 2252.13412785\n",
      "38 2250.58292352\n",
      "39 2249.03407418\n",
      "40 2247.53274389\n",
      "41 2245.99435917\n",
      "42 2244.35455335\n",
      "43 2242.70489286\n",
      "44 2241.0986306\n",
      "45 2239.54583799\n",
      "46 2238.1159281\n",
      "47 2236.85119384\n",
      "48 2235.71748512\n",
      "49 2234.62317682\n",
      "50 2233.62029424\n",
      "51 2232.56358515\n",
      "52 2231.63564977\n",
      "53 2230.71625656\n",
      "54 2230.02567909\n",
      "55 2229.29650379\n",
      "56 2228.6848943\n",
      "57 2227.98746718\n",
      "58 2227.44599568\n",
      "59 2226.81180812\n",
      "60 2226.25755285\n",
      "61 2225.70820938\n",
      "62 2225.25473922\n",
      "63 2224.673093\n",
      "64 2224.21502091\n",
      "65 2223.68294015\n",
      "66 2223.28936305\n",
      "67 2222.79449267\n",
      "68 2222.3594943\n",
      "69 2221.95837624\n",
      "70 2221.50804593\n",
      "71 2221.13763419\n",
      "72 2220.69359261\n",
      "73 2220.29334202\n",
      "74 2219.87202013\n",
      "75 2219.55222553\n",
      "76 2219.18455026\n",
      "77 2218.92899244\n",
      "78 2218.62290014\n",
      "79 2218.38819606\n",
      "80 2218.11044055\n",
      "81 2217.86779979\n",
      "82 2217.58236586\n",
      "83 2217.39405905\n",
      "84 2217.09993794\n",
      "85 2216.87349137\n",
      "86 2216.63843114\n",
      "87 2216.4226403\n",
      "88 2216.2043291\n",
      "89 2216.03839399\n",
      "90 2215.81832997\n",
      "91 2215.64006199\n",
      "92 2215.39624414\n",
      "93 2215.21831473\n",
      "94 2215.00073472\n",
      "95 2214.79687725\n",
      "96 2214.61575461\n",
      "97 2214.41153313\n",
      "98 2214.25155815\n",
      "99 2214.08111159\n",
      "100 2213.95944785\n",
      "101 2213.73792503\n",
      "102 2213.60798509\n",
      "103 2213.40677304\n",
      "104 2213.29177035\n",
      "105 2213.06015259\n",
      "106 2212.88504698\n",
      "107 2212.65185037\n",
      "108 2212.42881766\n",
      "109 2212.11565367\n",
      "110 2211.95825891\n",
      "111 2211.75771022\n",
      "112 2211.65907999\n",
      "113 2211.47381282\n",
      "114 2211.37919794\n",
      "115 2211.20209704\n",
      "116 2211.1135971\n",
      "117 2210.95416634\n",
      "118 2210.88804835\n",
      "119 2210.74724145\n",
      "120 2210.67821896\n",
      "121 2210.55563142\n",
      "122 2210.51949559\n",
      "123 2210.40070851\n",
      "124 2210.35078645\n",
      "125 2210.25976876\n",
      "126 2210.20927997\n",
      "127 2210.05849377\n",
      "128 2209.93381556\n",
      "129 2209.7427646\n",
      "130 2209.6342226\n",
      "131 2209.48601867\n",
      "132 2209.41977534\n",
      "133 2209.31655646\n",
      "134 2209.25420641\n",
      "135 2209.10711495\n",
      "136 2209.01573284\n",
      "137 2208.88588383\n",
      "138 2208.8040312\n",
      "139 2208.67565461\n",
      "140 2208.60910701\n",
      "141 2208.50983306\n",
      "142 2208.47353751\n",
      "143 2208.38234527\n",
      "144 2208.34699054\n",
      "145 2208.27369568\n",
      "146 2208.25967112\n",
      "147 2208.17657227\n",
      "148 2208.13457199\n",
      "149 2208.03090701\n",
      "150 2207.96129152\n",
      "151 2207.8288742\n",
      "152 2207.7551842\n",
      "153 2207.63645679\n",
      "154 2207.57235073\n",
      "155 2207.4621593\n",
      "156 2207.41571383\n",
      "157 2207.33184692\n",
      "158 2207.30391625\n",
      "159 2207.22954525\n",
      "160 2207.2096745\n",
      "161 2207.13830887\n",
      "162 2207.11504186\n",
      "163 2207.02229978\n",
      "164 2206.98462677\n",
      "165 2206.9111831\n",
      "166 2206.88382122\n",
      "167 2206.7849338\n",
      "168 2206.75139917\n",
      "169 2206.69579844\n",
      "170 2206.69992708\n",
      "171 2206.65809443\n",
      "172 2206.66521459\n",
      "173 2206.61645956\n",
      "174 2206.62537488\n",
      "175 2206.58840285\n",
      "176 2206.60118082\n",
      "177 2206.55290821\n",
      "178 2206.54487918\n",
      "179 2206.48869486\n",
      "180 2206.48244708\n",
      "181 2206.42394596\n",
      "182 2206.4233144\n",
      "183 2206.36992682\n",
      "184 2206.37290218\n",
      "185 2206.32364929\n",
      "186 2206.33159472\n",
      "187 2206.27817152\n",
      "188 2206.28188604\n",
      "189 2206.22996798\n",
      "190 2206.23826001\n",
      "191 2206.20745923\n",
      "192 2206.22750179\n",
      "193 2206.17750077\n",
      "194 2206.1571455\n",
      "195 2206.07638778\n",
      "196 2206.06559872\n",
      "197 2206.01807095\n",
      "198 2206.02701237\n",
      "199 2205.97416445\n",
      "Iters time 24.9231948853\n",
      "Iters time 14.7280509472\n"
     ]
    }
   ],
   "source": [
    "D, W = origin_n_dw_matrix.shape\n",
    "T = 10\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.random.uniform(size=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = trivial_regularization\n",
    "\n",
    "calc_log_likelihood = create_calculate_likelihood_like_function(\n",
    "    loss_function=LogFunction(),\n",
    "    n_dw_matrix=origin_n_dw_matrix\n",
    ")\n",
    "\n",
    "total_words_number = origin_n_dw_matrix.sum()\n",
    "def callback(it, phi, theta):\n",
    "    print it,  np.exp(- calc_log_likelihood(phi, theta) / total_words_number)\n",
    "\n",
    "phi, theta = artm_thetaless_em_optimization(\n",
    "    n_dw_matrix=origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200,\n",
    "    iteration_callback=callback\n",
    ")\n",
    "\n",
    "phi, theta = artm_thetaless_em_optimization(\n",
    "    n_dw_matrix=origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 2205.97416445\n",
      "Topic correlation 8.79771042965e-05\n",
      "Avg top5 pmi 1.245457939\n",
      "Avg top10 pmi 1.45120011591\n",
      "Avg top20 pmi 1.52730456693\n",
      "Sparsity 0.823275862069\n"
     ]
    }
   ],
   "source": [
    "print 'Perplexity', artm_calc_perplexity_factory(origin_n_dw_matrix)(phi, theta)\n",
    "print 'Topic correlation', artm_calc_topic_correlation(phi)\n",
    "print 'Avg top5 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 5)(phi)\n",
    "print 'Avg top10 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 10)(phi)\n",
    "print 'Avg top20 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 20)(phi)\n",
    "print 'Sparsity', 1. * np.sum(phi < 1e-20) / np.sum(phi >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(C=0.1, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.734  test-score: 0.746\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.789  test-score: 0.788\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.798  test-score: 0.803\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.738  test-score: 0.748\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.789  test-score: 0.786\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.808  test-score: 0.806\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.81  test-score: 0.809\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.738  test-score: 0.748\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.789  test-score: 0.786\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.807  test-score: 0.806\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.812  test-score: 0.808\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.805  test-score: 0.802\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.789  test-score: 0.786\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.806  test-score: 0.806\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.812  test-score: 0.809\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.812  test-score: 0.806\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.778  test-score: 0.777\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.806  test-score: 0.806\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.81  test-score: 0.806\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.81  test-score: 0.809\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.804  test-score: 0.808\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.753  test-score: 0.743\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.809  test-score: 0.807\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.81  test-score: 0.805\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.808  test-score: 0.795\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.794  test-score: 0.791\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.748  test-score: 0.734\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=100.0, gamma=0.1\n",
      "CV score: 0.812\n",
      "Test score:0.809\n"
     ]
    }
   ],
   "source": [
    "svm_score(theta, doc_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество классификации похоже на PLSA + наш метод вычисления $\\Theta$, но разреженность существенно выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA: ARTM thetaless optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4030.28108815\n",
      "1 3990.9897084\n",
      "2 3899.38669268\n",
      "3 3727.08282455\n",
      "4 3451.68043309\n",
      "5 3141.1961042\n",
      "6 2913.58418182\n",
      "7 2768.58796076\n",
      "8 2675.21864029\n",
      "9 2611.53083777\n",
      "10 2565.34881792\n",
      "11 2529.67850682\n",
      "12 2501.16086618\n",
      "13 2477.59867134\n",
      "14 2458.31089648\n",
      "15 2443.18033821\n",
      "16 2431.27973344\n",
      "17 2421.87860377\n",
      "18 2413.8725605\n",
      "19 2407.52647627\n",
      "20 2402.61654245\n",
      "21 2398.48972593\n",
      "22 2394.94707993\n",
      "23 2391.83264523\n",
      "24 2389.27380383\n",
      "25 2387.21154269\n",
      "26 2385.47128783\n",
      "27 2383.95824591\n",
      "28 2382.65911143\n",
      "29 2381.43303985\n",
      "30 2380.36913475\n",
      "31 2379.36922131\n",
      "32 2378.46024322\n",
      "33 2377.5461144\n",
      "34 2376.7904632\n",
      "35 2376.10054338\n",
      "36 2375.42785366\n",
      "37 2374.76881349\n",
      "38 2374.17220415\n",
      "39 2373.61299429\n",
      "40 2373.11653376\n",
      "41 2372.65077785\n",
      "42 2372.25361927\n",
      "43 2371.8543423\n",
      "44 2371.4690954\n",
      "45 2371.05433414\n",
      "46 2370.67343122\n",
      "47 2370.3074962\n",
      "48 2369.90871946\n",
      "49 2369.50302726\n",
      "50 2369.0803948\n",
      "51 2368.64942507\n",
      "52 2368.21044045\n",
      "53 2367.76265262\n",
      "54 2367.31041267\n",
      "55 2366.8733206\n",
      "56 2366.4736494\n",
      "57 2366.12933067\n",
      "58 2365.86589031\n",
      "59 2365.6245814\n",
      "60 2365.41453605\n",
      "61 2365.22649404\n",
      "62 2365.041011\n",
      "63 2364.85516658\n",
      "64 2364.67339055\n",
      "65 2364.50005878\n",
      "66 2364.33263006\n",
      "67 2364.1919012\n",
      "68 2364.07288625\n",
      "69 2363.97776213\n",
      "70 2363.90517771\n",
      "71 2363.83853849\n",
      "72 2363.80133847\n",
      "73 2363.77723558\n",
      "74 2363.76305533\n",
      "75 2363.75572418\n",
      "76 2363.74901593\n",
      "77 2363.73925123\n",
      "78 2363.73473394\n",
      "79 2363.72815589\n",
      "80 2363.72483805\n",
      "81 2363.72137293\n",
      "82 2363.73086435\n",
      "83 2363.7319842\n",
      "84 2363.71890209\n",
      "85 2363.72152031\n",
      "86 2363.73179133\n",
      "87 2363.74025032\n",
      "88 2363.74302172\n",
      "89 2363.7549671\n",
      "90 2363.77086617\n",
      "91 2363.76905051\n",
      "92 2363.77733608\n",
      "93 2363.79691488\n",
      "94 2363.81744039\n",
      "95 2363.84305997\n",
      "96 2363.8707299\n",
      "97 2363.89887014\n",
      "98 2363.92940772\n",
      "99 2363.96026262\n",
      "100 2363.99046619\n",
      "101 2364.02074845\n",
      "102 2364.04937214\n",
      "103 2364.07979376\n",
      "104 2364.1117744\n",
      "105 2364.13840136\n",
      "106 2364.16056587\n",
      "107 2364.18840465\n",
      "108 2364.2008805\n",
      "109 2364.22398257\n",
      "110 2364.24779954\n",
      "111 2364.27479524\n",
      "112 2364.30093827\n",
      "113 2364.31366859\n",
      "114 2364.32946591\n",
      "115 2364.3402647\n",
      "116 2364.35170068\n",
      "117 2364.36237789\n",
      "118 2364.37525249\n",
      "119 2364.38783436\n",
      "120 2364.40067728\n",
      "121 2364.41498398\n",
      "122 2364.43280199\n",
      "123 2364.45373152\n",
      "124 2364.47773988\n",
      "125 2364.50499426\n",
      "126 2364.53353239\n",
      "127 2364.5622186\n",
      "128 2364.58661225\n",
      "129 2364.61298439\n",
      "130 2364.63791278\n",
      "131 2364.66216985\n",
      "132 2364.68685014\n",
      "133 2364.71092587\n",
      "134 2364.73792547\n",
      "135 2364.75285489\n",
      "136 2364.76744087\n",
      "137 2364.78693457\n",
      "138 2364.80620898\n",
      "139 2364.82718504\n",
      "140 2364.84577574\n",
      "141 2364.86607233\n",
      "142 2364.88673755\n",
      "143 2364.90760858\n",
      "144 2364.92592216\n",
      "145 2364.94445117\n",
      "146 2364.96466631\n",
      "147 2364.98556325\n",
      "148 2365.00677307\n",
      "149 2365.02925429\n",
      "150 2365.05120791\n",
      "151 2365.06597817\n",
      "152 2365.08305462\n",
      "153 2365.09903077\n",
      "154 2365.11398148\n",
      "155 2365.1295174\n",
      "156 2365.14544698\n",
      "157 2365.16136866\n",
      "158 2365.17688991\n",
      "159 2365.19283652\n",
      "160 2365.20924911\n",
      "161 2365.22470528\n",
      "162 2365.23863343\n",
      "163 2365.25356181\n",
      "164 2365.26895663\n",
      "165 2365.28474593\n",
      "166 2365.30072012\n",
      "167 2365.31738166\n",
      "168 2365.33389925\n",
      "169 2365.34988703\n",
      "170 2365.36664395\n",
      "171 2365.38058983\n",
      "172 2365.39425267\n",
      "173 2365.40614303\n",
      "174 2365.41809538\n",
      "175 2365.43077112\n",
      "176 2365.44345723\n",
      "177 2365.45497432\n",
      "178 2365.46633022\n",
      "179 2365.4765376\n",
      "180 2365.48757729\n",
      "181 2365.49876874\n",
      "182 2365.51035461\n",
      "183 2365.52214144\n",
      "184 2365.53367954\n",
      "185 2365.54688929\n",
      "186 2365.56113282\n",
      "187 2365.57473142\n",
      "188 2365.58339676\n",
      "189 2365.59180718\n",
      "190 2365.60068464\n",
      "191 2365.60941207\n",
      "192 2365.6175771\n",
      "193 2365.62218594\n",
      "194 2365.62944403\n",
      "195 2365.6371055\n",
      "196 2365.64469223\n",
      "197 2365.65242742\n",
      "198 2365.65963198\n",
      "199 2365.66700264\n",
      "Iters time 26.9701170921\n",
      "Iters time 15.8483481407\n"
     ]
    }
   ],
   "source": [
    "D, W = origin_n_dw_matrix.shape\n",
    "T = 10\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.random.uniform(size=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = create_reg_lda(-0.1, -0.1)\n",
    "\n",
    "calc_log_likelihood = create_calculate_likelihood_like_function(\n",
    "    loss_function=LogFunction(),\n",
    "    n_dw_matrix=origin_n_dw_matrix\n",
    ")\n",
    "\n",
    "total_words_number = origin_n_dw_matrix.sum()\n",
    "def callback(it, phi, theta):\n",
    "    print it,  np.exp(- calc_log_likelihood(phi, theta) / total_words_number)\n",
    "\n",
    "phi, theta = artm_thetaless_em_optimization(\n",
    "    n_dw_matrix=origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200,\n",
    "    iteration_callback=callback\n",
    ")\n",
    "\n",
    "phi, theta = artm_thetaless_em_optimization(\n",
    "    n_dw_matrix=origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 2365.66700264\n",
      "Topic correlation 5.99072996027e-05\n",
      "Avg top5 pmi 1.18756774579\n",
      "Avg top10 pmi 1.41952043811\n",
      "Avg top20 pmi 1.47183401218\n",
      "Sparsity 0.885761957731\n"
     ]
    }
   ],
   "source": [
    "print 'Perplexity', artm_calc_perplexity_factory(origin_n_dw_matrix)(phi, theta)\n",
    "print 'Topic correlation', artm_calc_topic_correlation(phi)\n",
    "print 'Avg top5 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 5)(phi)\n",
    "print 'Avg top10 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 10)(phi)\n",
    "print 'Avg top20 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 20)(phi)\n",
    "print 'Sparsity', 1. * np.sum(phi < 1e-20) / np.sum(phi >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(C=0.1, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.568  test-score: 0.649\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.731  test-score: 0.736\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.744  test-score: 0.747\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.572  test-score: 0.649\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.732  test-score: 0.736\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.749  test-score: 0.753\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.753  test-score: 0.744\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.572  test-score: 0.65\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.733  test-score: 0.735\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.747  test-score: 0.75\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.759  test-score: 0.753\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.746  test-score: 0.744\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.733  test-score: 0.734\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.747  test-score: 0.748\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.752  test-score: 0.753\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.754  test-score: 0.749\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.713  test-score: 0.718\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.747  test-score: 0.75\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.751  test-score: 0.752\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.756  test-score: 0.755\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.745  test-score: 0.747\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.679  test-score: 0.682\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.751  test-score: 0.751\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.754  test-score: 0.753\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.749  test-score: 0.751\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.731  test-score: 0.733\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.665  test-score: 0.662\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=10.0, gamma=1\n",
      "CV score: 0.759\n",
      "Test score:0.753\n"
     ]
    }
   ],
   "source": [
    "svm_score(theta, doc_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLSA: gradient optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10077.0201076\n",
      "1 9000.64995398\n",
      "2 8289.15908634\n",
      "3 7760.50577064\n",
      "4 7342.67075424\n",
      "5 6999.46812428\n",
      "6 6709.96890275\n",
      "7 6461.04857097\n",
      "8 6243.80641146\n",
      "9 6052.10960786\n",
      "10 5881.43857375\n",
      "11 5728.3868157\n",
      "12 5590.31127508\n",
      "13 5465.22617647\n",
      "14 5351.36649016\n",
      "15 5247.30909037\n",
      "16 5152.03017834\n",
      "17 5064.6228889\n",
      "18 4984.24890204\n",
      "19 4910.26260413\n",
      "20 4842.07727317\n",
      "21 4779.15688395\n",
      "22 4721.02700376\n",
      "23 4667.3711954\n",
      "24 4617.78821401\n",
      "25 4571.96402783\n",
      "26 4529.63693079\n",
      "27 4490.50908723\n",
      "28 4454.33282924\n",
      "29 4420.84640399\n",
      "30 4389.84806195\n",
      "31 4361.14183026\n",
      "32 4334.5713221\n",
      "33 4309.99315515\n",
      "34 4287.24255502\n",
      "35 4266.15147853\n",
      "36 4246.5892184\n",
      "37 4228.42562612\n",
      "38 4211.54444688\n",
      "39 4195.83746838\n",
      "40 4181.18689862\n",
      "41 4167.48956218\n",
      "42 4154.67090928\n",
      "43 4142.63700646\n",
      "44 4131.32195601\n",
      "45 4120.65886981\n",
      "46 4110.59334532\n",
      "47 4101.06769628\n",
      "48 4092.03921378\n",
      "49 4083.45569101\n",
      "50 4075.2837242\n",
      "51 4067.50378137\n",
      "52 4060.08402337\n",
      "53 4052.99081329\n",
      "54 4046.20296281\n",
      "55 4039.70004519\n",
      "56 4033.45519261\n",
      "57 4027.44872721\n",
      "58 4021.68185388\n",
      "59 4016.13942614\n",
      "60 4010.79385191\n",
      "61 4005.63573185\n",
      "62 4000.66036282\n",
      "63 3995.85654539\n",
      "64 3991.21380444\n",
      "65 3986.72450839\n",
      "66 3982.37608133\n",
      "67 3978.1639385\n",
      "68 3974.08541033\n",
      "69 3970.12973495\n",
      "70 3966.29161995\n",
      "71 3962.56841321\n",
      "72 3958.9502953\n",
      "73 3955.43569474\n",
      "74 3952.01848501\n",
      "75 3948.69554636\n",
      "76 3945.46353232\n",
      "77 3942.31817835\n",
      "78 3939.25676173\n",
      "79 3936.27381493\n",
      "80 3933.36768577\n",
      "81 3930.53403795\n",
      "82 3927.76682917\n",
      "83 3925.06562559\n",
      "84 3922.42854182\n",
      "85 3919.85205792\n",
      "86 3917.33404987\n",
      "87 3914.87569602\n",
      "88 3912.47144366\n",
      "89 3910.11857477\n",
      "90 3907.81542704\n",
      "91 3905.55994543\n",
      "92 3903.35069491\n",
      "93 3901.18638922\n",
      "94 3899.06608436\n",
      "95 3896.98980922\n",
      "96 3894.95421576\n",
      "97 3892.95851711\n",
      "98 3891.00296042\n",
      "99 3889.08723098\n",
      "100 3887.20886993\n",
      "101 3885.36582763\n",
      "102 3883.55746168\n",
      "103 3881.78217788\n",
      "104 3880.04048454\n",
      "105 3878.33071103\n",
      "106 3876.65152736\n",
      "107 3875.00220285\n",
      "108 3873.3825018\n",
      "109 3871.79234483\n",
      "110 3870.23074163\n",
      "111 3868.69574432\n",
      "112 3867.18682256\n",
      "113 3865.70296541\n",
      "114 3864.24489402\n",
      "115 3862.81098199\n",
      "116 3861.39993664\n",
      "117 3860.01055566\n",
      "118 3858.64398058\n",
      "119 3857.29966903\n",
      "120 3855.97626584\n",
      "121 3854.67394429\n",
      "122 3853.39090564\n",
      "123 3852.12754075\n",
      "124 3850.88199252\n",
      "125 3849.65473986\n",
      "126 3848.4463882\n",
      "127 3847.25534414\n",
      "128 3846.0814674\n",
      "129 3844.92503411\n",
      "130 3843.78444699\n",
      "131 3842.65957072\n",
      "132 3841.55051875\n",
      "133 3840.45678713\n",
      "134 3839.3792501\n",
      "135 3838.31747404\n",
      "136 3837.27060155\n",
      "137 3836.23781925\n",
      "138 3835.21893713\n",
      "139 3834.21375232\n",
      "140 3833.22185126\n",
      "141 3832.24302889\n",
      "142 3831.27615863\n",
      "143 3830.32080844\n",
      "144 3829.37750078\n",
      "145 3828.44604953\n",
      "146 3827.52666158\n",
      "147 3826.61880113\n",
      "148 3825.72184683\n",
      "149 3824.83534808\n",
      "150 3823.95977768\n",
      "151 3823.09474629\n",
      "152 3822.23954577\n",
      "153 3821.39424815\n",
      "154 3820.55897335\n",
      "155 3819.73306639\n",
      "156 3818.91709745\n",
      "157 3818.11077669\n",
      "158 3817.3145429\n",
      "159 3816.52767157\n",
      "160 3815.74999325\n",
      "161 3814.98073441\n",
      "162 3814.22026959\n",
      "163 3813.46815583\n",
      "164 3812.72399318\n",
      "165 3811.9876538\n",
      "166 3811.25939703\n",
      "167 3810.53919582\n",
      "168 3809.82630509\n",
      "169 3809.12050577\n",
      "170 3808.42252223\n",
      "171 3807.7325369\n",
      "172 3807.04939315\n",
      "173 3806.37365193\n",
      "174 3805.70448655\n",
      "175 3805.04165062\n",
      "176 3804.3855122\n",
      "177 3803.73590453\n",
      "178 3803.09283431\n",
      "179 3802.45597999\n",
      "180 3801.82525093\n",
      "181 3801.20041662\n",
      "182 3800.5819204\n",
      "183 3799.9698245\n",
      "184 3799.36348592\n",
      "185 3798.76283948\n",
      "186 3798.16758641\n",
      "187 3797.57778171\n",
      "188 3796.99337328\n",
      "189 3796.41435473\n",
      "190 3795.84105644\n",
      "191 3795.27295697\n",
      "192 3794.70972133\n",
      "193 3794.15187177\n",
      "194 3793.59922478\n",
      "195 3793.05166828\n",
      "196 3792.50894648\n",
      "197 3791.97130263\n",
      "198 3791.43866032\n",
      "199 3790.91101076\n",
      "Iters time 23.720220089\n"
     ]
    }
   ],
   "source": [
    "D, W = origin_n_dw_matrix.shape\n",
    "T = 10\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.random.uniform(size=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = trivial_regularization\n",
    "\n",
    "calc_log_likelihood = create_calculate_likelihood_like_function(\n",
    "    loss_function=LogFunction(),\n",
    "    n_dw_matrix=origin_n_dw_matrix\n",
    ")\n",
    "\n",
    "total_words_number = origin_n_dw_matrix.sum()\n",
    "def callback(it, phi, theta):\n",
    "    print it,  np.exp(- calc_log_likelihood(phi, theta) / total_words_number)\n",
    "\n",
    "phi, theta = gradient_optimization(\n",
    "    n_dw_matrix=origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    theta_matrix=theta_matrix,\n",
    "    regularization_gradient_list=regularization_list,\n",
    "    iters_count=200,\n",
    "    loss_function=LogFunction(),\n",
    "    iteration_callback=callback,\n",
    "    learning_rate=1e-10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 3790.91101076\n",
      "Topic correlation 0.000550302870934\n",
      "Avg top5 pmi 0.427162644713\n",
      "Avg top10 pmi 0.425537991526\n",
      "Avg top20 pmi 0.444101821971\n",
      "Sparsity 0.59623192436\n"
     ]
    }
   ],
   "source": [
    "print 'Perplexity', artm_calc_perplexity_factory(origin_n_dw_matrix)(phi, theta)\n",
    "print 'Topic correlation', artm_calc_topic_correlation(phi)\n",
    "print 'Avg top5 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 5)(phi)\n",
    "print 'Avg top10 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 10)(phi)\n",
    "print 'Avg top20 pmi', artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, origin_n_dw_matrix.shape[0], 20)(phi)\n",
    "print 'Sparsity', 1. * np.sum(phi < 1e-20) / np.sum(phi >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(C=0.1, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.174  test-score: 0.18\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.177  test-score: 0.185\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.168  test-score: 0.183\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.173  test-score: 0.187\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.173  test-score: 0.19\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.169  test-score: 0.172\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.169  test-score: 0.169\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.175  test-score: 0.188\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.175  test-score: 0.188\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.174  test-score: 0.19\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.174  test-score: 0.174\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.175  test-score: 0.188\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.175  test-score: 0.187\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.174  test-score: 0.19\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.176  test-score: 0.183\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.17  test-score: 0.174\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.175  test-score: 0.186\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.176  test-score: 0.19\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.172  test-score: 0.189\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.173  test-score: 0.18\n"
     ]
    }
   ],
   "source": [
    "svm_score(theta, doc_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "куда-то не туда сошёлся и очень плохие результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## На всей коллекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLSA, EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters time 176.533018112\n"
     ]
    }
   ],
   "source": [
    "D, W = big_origin_n_dw_matrix.shape\n",
    "T = 30\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.random.uniform(size=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = trivial_regularization\n",
    "\n",
    "calc_log_likelihood = create_calculate_likelihood_like_function(\n",
    "    loss_function=LogFunction(),\n",
    "    n_dw_matrix=big_origin_n_dw_matrix\n",
    ")\n",
    "\n",
    "total_words_number = big_origin_n_dw_matrix.sum()\n",
    "def callback(it, phi, theta):\n",
    "    print it,  np.exp(- calc_log_likelihood(phi, theta) / total_words_number)\n",
    "\n",
    "phi, theta = em_optimization(\n",
    "    n_dw_matrix=big_origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    theta_matrix=theta_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200,\n",
    "    loss_function=LogFunction(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_plsa_phi_em = phi\n",
    "big_plsa_theta_em = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 1414.64730681\n",
      "Topic correlation 0.000386962147332\n",
      "Avg top5 pmi 1.86098445613\n",
      "Avg top10 pmi 1.90166746563\n",
      "Avg top20 pmi 1.79886716599\n",
      "Sparsity 0.842324748236\n"
     ]
    }
   ],
   "source": [
    "print 'Perplexity', artm_calc_perplexity_factory(big_origin_n_dw_matrix)(phi, theta)\n",
    "print 'Topic correlation', artm_calc_topic_correlation(phi)\n",
    "print 'Avg top5 pmi', artm_calc_pmi_top_factory(big_doc_occurences, big_doc_cooccurences, big_origin_n_dw_matrix.shape[0], 5)(phi)\n",
    "print 'Avg top10 pmi', artm_calc_pmi_top_factory(big_doc_occurences, big_doc_cooccurences, big_origin_n_dw_matrix.shape[0], 10)(phi)\n",
    "print 'Avg top20 pmi', artm_calc_pmi_top_factory(big_doc_occurences, big_doc_cooccurences, big_origin_n_dw_matrix.shape[0], 20)(phi)\n",
    "print 'Sparsity', 1. * np.sum(phi < 1e-20) / np.sum(phi >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(C=0.1, gamma=0.001) cv-score: 0.054  test-score: 0.053\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.054  test-score: 0.057\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.509  test-score: 0.535\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.554  test-score: 0.566\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.553  test-score: 0.568\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.054  test-score: 0.057\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.51  test-score: 0.536\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.557  test-score: 0.569\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.578  test-score: 0.59\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.572  test-score: 0.592\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.509  test-score: 0.536\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.557  test-score: 0.569\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.577  test-score: 0.587\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.584  test-score: 0.595\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.54  test-score: 0.57\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.557  test-score: 0.568\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.574  test-score: 0.582\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.58  test-score: 0.595\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.574  test-score: 0.592\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.512  test-score: 0.534\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.573  test-score: 0.582\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.579  test-score: 0.588\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.582  test-score: 0.595\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.546  test-score: 0.568\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.501  test-score: 0.52\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.575  test-score: 0.589\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.579  test-score: 0.596\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.576  test-score: 0.588\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.519  test-score: 0.544\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.495  test-score: 0.514\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=10.0, gamma=1\n",
      "CV score: 0.584\n",
      "Test score:0.595\n"
     ]
    }
   ],
   "source": [
    "svm_score(theta, big_doc_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters time 0.925080060959\n"
     ]
    }
   ],
   "source": [
    "phi, theta = naive_thetaless_em_optimization(\n",
    "    n_dw_matrix=big_origin_n_dw_matrix, \n",
    "    phi_matrix=big_plsa_phi_em,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 1988.88347039\n",
      "Topic correlation 0.000407564113863\n",
      "Avg top5 pmi 1.80782976209\n",
      "Avg top10 pmi 1.79368364694\n",
      "Avg top20 pmi 1.75037595911\n",
      "Sparsity 0.842531183782\n"
     ]
    }
   ],
   "source": [
    "print 'Perplexity', artm_calc_perplexity_factory(big_origin_n_dw_matrix)(phi, theta)\n",
    "print 'Topic correlation', artm_calc_topic_correlation(phi)\n",
    "print 'Avg top5 pmi', artm_calc_pmi_top_factory(big_doc_occurences, big_doc_cooccurences, big_origin_n_dw_matrix.shape[0], 5)(phi)\n",
    "print 'Avg top10 pmi', artm_calc_pmi_top_factory(big_doc_occurences, big_doc_cooccurences, big_origin_n_dw_matrix.shape[0], 10)(phi)\n",
    "print 'Avg top20 pmi', artm_calc_pmi_top_factory(big_doc_occurences, big_doc_cooccurences, big_origin_n_dw_matrix.shape[0], 20)(phi)\n",
    "print 'Sparsity', 1. * np.sum(phi < 1e-20) / np.sum(phi >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(C=0.1, gamma=0.001) cv-score: 0.054  test-score: 0.053\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.054  test-score: 0.053\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.109  test-score: 0.214\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.546  test-score: 0.566\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.6  test-score: 0.61\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.054  test-score: 0.053\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.108  test-score: 0.217\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.548  test-score: 0.566\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.603  test-score: 0.614\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.621  test-score: 0.641\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.107  test-score: 0.218\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.549  test-score: 0.566\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.603  test-score: 0.615\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.617  test-score: 0.64\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.626  test-score: 0.652\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.549  test-score: 0.566\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.603  test-score: 0.616\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.614  test-score: 0.634\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.623  test-score: 0.644\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.608  test-score: 0.638\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.603  test-score: 0.616\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.615  test-score: 0.633\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.615  test-score: 0.64\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.622  test-score: 0.644\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.579  test-score: 0.613\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.614  test-score: 0.633\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.612  test-score: 0.634\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.62  test-score: 0.642\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.609  test-score: 0.628\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.572  test-score: 0.598\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=10.0, gamma=10.0\n",
      "CV score: 0.626\n",
      "Test score:0.652\n"
     ]
    }
   ],
   "source": [
    "svm_score(theta, big_doc_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PLSA, Naive Thetaless EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters time 178.381123781\n"
     ]
    }
   ],
   "source": [
    "D, W = big_origin_n_dw_matrix.shape\n",
    "T = 30\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.random.uniform(size=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = trivial_regularization\n",
    "\n",
    "calc_log_likelihood = create_calculate_likelihood_like_function(\n",
    "    loss_function=LogFunction(),\n",
    "    n_dw_matrix=big_origin_n_dw_matrix\n",
    ")\n",
    "\n",
    "total_words_number = big_origin_n_dw_matrix.sum()\n",
    "def callback(it, phi, theta):\n",
    "    print it,  np.exp(- calc_log_likelihood(phi, theta) / total_words_number)\n",
    "\n",
    "phi, theta = naive_thetaless_em_optimization(\n",
    "    n_dw_matrix=big_origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 1711.55099784\n",
      "Topic correlation 0.000175446626635\n",
      "Avg top5 pmi 1.52647084897\n",
      "Avg top10 pmi 1.65998347844\n",
      "Avg top20 pmi 1.77594116676\n",
      "Sparsity 0.873347969504\n"
     ]
    }
   ],
   "source": [
    "print 'Perplexity', artm_calc_perplexity_factory(big_origin_n_dw_matrix)(phi, theta)\n",
    "print 'Topic correlation', artm_calc_topic_correlation(phi)\n",
    "print 'Avg top5 pmi', artm_calc_pmi_top_factory(big_doc_occurences, big_doc_cooccurences, big_origin_n_dw_matrix.shape[0], 5)(phi)\n",
    "print 'Avg top10 pmi', artm_calc_pmi_top_factory(big_doc_occurences, big_doc_cooccurences, big_origin_n_dw_matrix.shape[0], 10)(phi)\n",
    "print 'Avg top20 pmi', artm_calc_pmi_top_factory(big_doc_occurences, big_doc_cooccurences, big_origin_n_dw_matrix.shape[0], 20)(phi)\n",
    "print 'Sparsity', 1. * np.sum(phi < 1e-20) / np.sum(phi >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(C=0.1, gamma=0.001) cv-score: 0.054  test-score: 0.053\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.054  test-score: 0.053\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.333  test-score: 0.422\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.548  test-score: 0.572\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.58  test-score: 0.598\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.054  test-score: 0.053\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.338  test-score: 0.427\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.551  test-score: 0.574\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.589  test-score: 0.602\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.599  test-score: 0.62\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.34  test-score: 0.427\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.552  test-score: 0.574\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.589  test-score: 0.603\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.594  test-score: 0.611\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.591  test-score: 0.613\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.552  test-score: 0.574\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.588  test-score: 0.602\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.593  test-score: 0.605\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.599  test-score: 0.613\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.562  test-score: 0.58\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.588  test-score: 0.602\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.592  test-score: 0.602\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.596  test-score: 0.613\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.58  test-score: 0.603\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.551  test-score: 0.567\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.592  test-score: 0.602\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.591  test-score: 0.604\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.595  test-score: 0.608\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.558  test-score: 0.576\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.549  test-score: 0.565\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=100.0, gamma=1\n",
      "CV score: 0.599\n",
      "Test score:0.613\n"
     ]
    }
   ],
   "source": [
    "svm_score(theta, big_doc_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PLSA, ARTM Thetaless EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters time 204.33359313\n"
     ]
    }
   ],
   "source": [
    "D, W = big_origin_n_dw_matrix.shape\n",
    "T = 30\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.random.uniform(size=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = trivial_regularization\n",
    "\n",
    "calc_log_likelihood = create_calculate_likelihood_like_function(\n",
    "    loss_function=LogFunction(),\n",
    "    n_dw_matrix=big_origin_n_dw_matrix\n",
    ")\n",
    "\n",
    "total_words_number = big_origin_n_dw_matrix.sum()\n",
    "def callback(it, phi, theta):\n",
    "    print it,  np.exp(- calc_log_likelihood(phi, theta) / total_words_number)\n",
    "\n",
    "phi, theta = artm_thetaless_em_optimization(\n",
    "    n_dw_matrix=big_origin_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 1793.06954679\n",
      "Topic correlation 2.65274620004e-05\n",
      "Avg top5 pmi 2.19231523539\n",
      "Avg top10 pmi 2.3588303139\n",
      "Avg top20 pmi 2.40053604237\n",
      "Sparsity 0.931713524259\n"
     ]
    }
   ],
   "source": [
    "print 'Perplexity', artm_calc_perplexity_factory(big_origin_n_dw_matrix)(phi, theta)\n",
    "print 'Topic correlation', artm_calc_topic_correlation(phi)\n",
    "print 'Avg top5 pmi', artm_calc_pmi_top_factory(big_doc_occurences, big_doc_cooccurences, big_origin_n_dw_matrix.shape[0], 5)(phi)\n",
    "print 'Avg top10 pmi', artm_calc_pmi_top_factory(big_doc_occurences, big_doc_cooccurences, big_origin_n_dw_matrix.shape[0], 10)(phi)\n",
    "print 'Avg top20 pmi', artm_calc_pmi_top_factory(big_doc_occurences, big_doc_cooccurences, big_origin_n_dw_matrix.shape[0], 20)(phi)\n",
    "print 'Sparsity', 1. * np.sum(phi < 1e-20) / np.sum(phi >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(C=0.1, gamma=0.001) cv-score: 0.054  test-score: 0.053\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.054  test-score: 0.053\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.298  test-score: 0.34\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.517  test-score: 0.528\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.542  test-score: 0.548\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.054  test-score: 0.053\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.302  test-score: 0.339\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.521  test-score: 0.529\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.55  test-score: 0.556\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.562  test-score: 0.572\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.303  test-score: 0.339\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.521  test-score: 0.529\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.55  test-score: 0.556\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.558  test-score: 0.568\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.552  test-score: 0.566\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.521  test-score: 0.529\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.55  test-score: 0.555\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.555  test-score: 0.563\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.558  test-score: 0.574\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.519  test-score: 0.535\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.55  test-score: 0.555\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.556  test-score: 0.561\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.558  test-score: 0.566\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.542  test-score: 0.56\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.505  test-score: 0.519\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.557  test-score: 0.56\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.553  test-score: 0.563\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.554  test-score: 0.566\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.517  test-score: 0.541\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.503  test-score: 0.517\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=1.0, gamma=10.0\n",
      "CV score: 0.562\n",
      "Test score:0.572\n"
     ]
    }
   ],
   "source": [
    "svm_score(theta, big_doc_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
