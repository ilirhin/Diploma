{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оптимальные питоновские реализация оптимизации ARTM\n",
    "\n",
    "# Оптимизация произвольной функции\n",
    "\n",
    "# Thetaless оптимизация\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.core.umath_tests import inner1d\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разные функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.log(x + 1e-20)\n",
    "    def calc_der(self, x):\n",
    "        return 1. / (x + 1e-20)\n",
    "    \n",
    "\n",
    "class IdFunction(object):\n",
    "    def calc(self, x):\n",
    "        return x + 1e-20\n",
    "    def calc_der(self, x):\n",
    "        return np.ones_like(x)\n",
    "    \n",
    "\n",
    "class SquareFunction(object):\n",
    "    def calc(self, x):\n",
    "        return (x + 1e-20) ** 2\n",
    "    def calc_der(self, x):\n",
    "        return 2. * (x + 1e-20) ** 2\n",
    "    \n",
    "\n",
    "class CubeLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.log(x + 1e-20) ** 3\n",
    "    def calc_der(self, x):\n",
    "        return 3. * np.log(x + 1e-20) ** 2 / (x + 1e-20)\n",
    "    \n",
    "\n",
    "class SquareLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.log(x + 1e-20) * np.abs(np.log(x + 1e-20))\n",
    "    def calc_der(self, x):\n",
    "        return 2. * np.abs(np.log(x + 1e-20)) / (x + 1e-20)\n",
    "\n",
    "    \n",
    "class FiveLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.log(x + 1e-20) ** 5\n",
    "    def calc_der(self, x):\n",
    "        return 5. * np.log(x + 1e-20) ** 4 / (x + 1e-20)\n",
    "    \n",
    "\n",
    "class CubeRootLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.cbrt(np.log(x + 1e-20))\n",
    "    def calc_der(self, x):\n",
    "        return 1. / 3 / (np.cbrt(np.log(x + 1e-20)) ** 2) / (x + 1e-20)\n",
    "    \n",
    "    \n",
    "class SquareRootLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.sqrt(- np.log(x + 1e-20))\n",
    "    def calc_der(self, x):\n",
    "        return 1. / 2. / np.sqrt(- np.log(x + 1e-20)) / (x + 1e-20)\n",
    "    \n",
    "\n",
    "class ExpFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.exp(x)\n",
    "    def calc_der(self, x):\n",
    "        return np.exp(x)\n",
    "\n",
    "    \n",
    "class EntropyFunction(object):\n",
    "    def calc(self, x):\n",
    "        return (np.log(x + 1e-20) + 50.) * (x + 1e-20)\n",
    "    def calc_der(self, x):\n",
    "        return np.log(x + 1e-20) + 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разные регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trivial_regularization(n_tw, n_dt):\n",
    "    return np.zeros_like(n_tw), np.zeros_like(n_dt)\n",
    "\n",
    "def create_reg_decorr(tau, theta_alpha=0.):\n",
    "    def fun(n_tw, n_dt):\n",
    "        phi_matrix = n_tw / np.sum(n_tw, axis=1)[:, np.newaxis]\n",
    "        theta_matrix = n_dt / np.sum(n_dt, axis=1)[:, np.newaxis]\n",
    "        aggr_phi = np.sum(phi_matrix, axis=1)\n",
    "        return - tau * np.transpose(phi_matrix * (aggr_phi[:, np.newaxis] - phi_matrix)), theta_alpha\n",
    "    return fun\n",
    "\n",
    "def create_reg_lda(phi_alpha, theta_alpha):\n",
    "    def fun (n_tw, n_dt):\n",
    "        return np.zeros_like(n_tw) + phi_alpha, np.zeros_like(n_dt) + theta_alpha\n",
    "    return fun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка Датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно скачать некоторые коллекции данных и установить библиотеки (nltk, gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tylorn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, calc_cooccurences=False, train_test_split=None, token_2_num=None):\n",
    "    is_token_2_num_provided = token_2_num is not None \n",
    "    # remove stopwords\n",
    "    if not is_token_2_num_provided:\n",
    "        token_2_num = {}\n",
    "        occurences = Counter()\n",
    "        for i, doc in enumerate(dataset.data):\n",
    "            tokens = gensim.utils.lemmatize(doc)\n",
    "            for token in set(tokens):\n",
    "                occurences[token] += 1\n",
    "            if i % 500 == 0:\n",
    "                print 'Processed: ', i, 'documents from', len(dataset.data)\n",
    "    \n",
    "    row, col, data = [], [], []\n",
    "    row_test, col_test, data_test = [], [], []\n",
    "    not_empty_docs_number = 0\n",
    "    doc_targets = []\n",
    "    doc_cooccurences = Counter()\n",
    "    doc_occurences = Counter()\n",
    "    random_gen = random.Random(42)\n",
    "    \n",
    "    for doc, target in zip(dataset.data, dataset.target):\n",
    "        tokens = gensim.utils.lemmatize(doc)\n",
    "        cnt = Counter()\n",
    "        cnt_test = Counter()\n",
    "        for token in tokens:\n",
    "            word = token.split('/')[0]\n",
    "            if not is_token_2_num_provided and word not in english_stopwords and 3 <= occurences[token] and token not in token_2_num:\n",
    "                token_2_num[token] = len(token_2_num)\n",
    "            if token in token_2_num:\n",
    "                if train_test_split is None or random_gen.random() < train_test_split:\n",
    "                    cnt[token_2_num[token]] += 1\n",
    "                else:\n",
    "                    cnt_test[token_2_num[token]] += 1\n",
    "        \n",
    "        if len(cnt) > 0 and (train_test_split is None or len(cnt_test) > 0):\n",
    "            for w, c in cnt.iteritems():\n",
    "                row.append(not_empty_docs_number)\n",
    "                col.append(w)\n",
    "                data.append(c)\n",
    "                \n",
    "            for w, c in cnt_test.iteritems():\n",
    "                row_test.append(not_empty_docs_number)\n",
    "                col_test.append(w)\n",
    "                data_test.append(c)\n",
    "                \n",
    "            not_empty_docs_number += 1\n",
    "            doc_targets.append(target)\n",
    "            \n",
    "            if calc_cooccurences:\n",
    "                words = set(cnt.keys() + cnt_test.keys())\n",
    "                doc_occurences.update(words)\n",
    "                doc_cooccurences.update({(w1, w2) for w1 in words for w2 in words if w1 != w2})\n",
    "        \n",
    "    num_2_token = {\n",
    "        v: k\n",
    "        for k, v in token_2_num.iteritems()\n",
    "    }\n",
    "    print 'Nonzero values:', len(data)\n",
    "    shape = (len(doc_targets), len(token_2_num))\n",
    "    \n",
    "    if train_test_split is None:\n",
    "        if calc_cooccurences:\n",
    "            return scipy.sparse.csr_matrix((data, (row, col)), shape=shape), token_2_num, num_2_token, doc_targets, doc_occurences, doc_cooccurences\n",
    "        else:\n",
    "            return scipy.sparse.csr_matrix((data, (row, col)), shape=shape), token_2_num, num_2_token, doc_targets\n",
    "    else:\n",
    "        if calc_cooccurences:\n",
    "            return (\n",
    "                scipy.sparse.csr_matrix((data, (row, col)), shape=shape),\n",
    "                scipy.sparse.csr_matrix((data_test, (row_test, col_test)), shape=shape),\n",
    "                token_2_num,\n",
    "                num_2_token,\n",
    "                doc_targets,\n",
    "                doc_occurences,\n",
    "                doc_cooccurences\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                scipy.sparse.csr_matrix((data, (row, col)), shape=shape),\n",
    "                scipy.sparse.csr_matrix((data_test, (row_test, col_test)), shape=shape),\n",
    "                token_2_num,\n",
    "                num_2_token,\n",
    "                doc_targets\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисление правдоподобных функций\n",
    "\n",
    "### имеется в виду вычисление функций вида $\\sum_{dw} n_{dw} f(\\sum_{t} \\phi_{wt} \\theta_{td})$\n",
    "\n",
    "##### Ключевой момент - использование функции inner1d. Она позволяет перемножить попарно строчки матриц, не сохраняя промежуточное состояние. А индексация в numpy не создаёт новый массив, делает view над ним. Таким образом, подсчёт $\\sum_{t} \\phi_{wt} \\theta_{td}$  делается максимально эффективным способом и по времени и по памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_calculate_likelihood_like_function(n_dw_matrix, loss_function=LogFunction()):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    docptr = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        docptr.extend([doc_num] * (indptr[doc_num + 1] - indptr[doc_num]))\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    \n",
    "    def fun(phi_matrix, theta_matrix):\n",
    "        s_data = loss_function.calc(inner1d(theta_matrix[docptr, :], np.transpose(phi_matrix)[wordptr, :]))\n",
    "        return np.sum(n_dw_matrix.data * s_data)\n",
    "\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM алгоритм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая схема:\n",
    "#### Неоходимо сначала вычислить $p_{tdw} = \\frac{\\phi_{wt} \\theta_{td}}{\\sum_s \\phi_{ws} \\theta_{sd}}$\n",
    "#### Считаем $n_{wt} = \\sum_d n_{dw} p_{tdw}$ и $n_{td} = \\sum_w n_{dw} p_{tdw}$\n",
    "#### Вычисляем $r_{wt}, r_{td}$ как функцию от $n_{wt}, n_{td}$\n",
    "#### Прибавляем, делаем положительную срезку и нормируем\n",
    "\n",
    "## Оптимизация вычисления:\n",
    "#### Обозначим за $s_{dw}$ следующее выражение $\\sum_t \\phi_{wt} \\theta_{td}$, фактически это наше предсказание для вероятности\n",
    "####  Тогда $p_{tdw} = \\frac{\\phi_{wt} \\theta_{td}}{s_{dw}}$\n",
    "#### Подставим это выражение например в $n_wt$\n",
    "#### И получим, что $n_{wt} = \\sum_d n_{dw} \\frac{\\phi_{wt} \\theta_{td}}{s_{dw}} = \\phi_{wt} \\sum_d \\theta_{td} \\cdot \\frac{n_{dw}}{s_{dw}}$, аналогично $n_{td} = \\theta_{td} \\sum_w \\phi_{wt} \\cdot \\frac{n_{dw}}{s_{dw}}$\n",
    "#### Таким образом, мы видим, что фактически нам нужно знать матрицу $\\frac{n_{dw}}{s_{dw}}$, а она очень разреженная, поэтому и $s_{dw}$ нужно не для всех пар вычислять, а только там, где $n_{dw} > 0$. \n",
    "#### То есть нам нужно эффективно закодить вычисление разженной матрицы $s_{dw}$ (матрица $n_{dw}$ уже есть в разреженном виде, так как подаётся на вход алгоритма), а затем просто поэлементно поделить\n",
    "#### Причём хочется, чтобы промежуточные значения $p_{tdw}$ не сохранялись (как мы увидели, они в конечном варианте не важны)\n",
    "#### Обозначим эту матрицу за $A$. Тогда $n_{wt} = \\phi_{wt} (\\Theta A)_{tw}$, а $n_{td} = \\theta_{td} (A \\Phi^T)_{dt}$.\n",
    "#### Перемножить разреженную матрицу на плотную можно быстро, если правильно её хранить (по строкам, или по столбцам)\n",
    "#### Если оптимизируется не правдоподобие, какая-то другая функция вида $\\sum_{dw} n_{dw} f(s_{dw})$ (правдоподобие будет, если $f(x) = \\ln x$ ) , то в этом случае нужно определить матрицу $A$ как $A_{dw} = n_{dw} f'(s_{dw})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def em_optimization(\n",
    "    n_dw_matrix, \n",
    "    phi_matrix,\n",
    "    theta_matrix,\n",
    "    regularization_list,\n",
    "    iters_count=100,\n",
    "    loss_function=LogFunction(),\n",
    "    iteration_callback=None,\n",
    "    const_phi=False\n",
    "):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    T = phi_matrix.shape[0]\n",
    "    phi_matrix = np.copy(phi_matrix)\n",
    "    theta_matrix = np.copy(theta_matrix)\n",
    "    docptr = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        docptr.extend([doc_num] * (indptr[doc_num + 1] - indptr[doc_num]))\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for it in xrange(iters_count):\n",
    "        phi_matrix_tr = np.transpose(phi_matrix)\n",
    "        # следующая строчка это 60% времени работы алгоритма\n",
    "        s_data = loss_function.calc_der(inner1d(theta_matrix[docptr, :], phi_matrix_tr[wordptr, :]))\n",
    "        # следующая часть это 25% времени работы алгоритма\n",
    "        A = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                n_dw_matrix.data * s_data, \n",
    "                n_dw_matrix.indices, \n",
    "                n_dw_matrix.indptr\n",
    "            ), \n",
    "            shape=n_dw_matrix.shape\n",
    "        )\n",
    "        A_tr = A.tocsc().transpose()\n",
    "        # Остальное это 15% времени\n",
    "        n_tw = np.transpose(A_tr.dot(theta_matrix)) * phi_matrix\n",
    "        n_dt = A.dot(phi_matrix_tr) * theta_matrix\n",
    "        \n",
    "        r_tw, r_dt = regularization_list[it](n_tw, n_dt)\n",
    "        n_tw += r_tw\n",
    "        n_dt += n_dt\n",
    "        n_tw[n_tw < 0] = 0\n",
    "        n_dt[n_dt < 0] = 0\n",
    "        \n",
    "        if not const_phi:\n",
    "            phi_matrix = n_tw / np.sum(n_tw, axis=1)[:, np.newaxis]\n",
    "        theta_matrix = n_dt / np.sum(n_dt, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        if iteration_callback is not None:\n",
    "            iteration_callback(it, phi_matrix, theta_matrix)\n",
    "    \n",
    "    print 'Iters time', time.time() - start_time\n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive thetaless EM\n",
    "\n",
    "\n",
    "### Основная идея: давайте вообще не хранить $\\Theta$, а вместо этого вычислять её на лету одной итерацией ЕМ алгоритма, которую можно легко выписать.\n",
    "\n",
    "##### Пусть тематический профиль документа инициализирован равномерно, то для этого документа $p_{tdw} = \\frac{\\phi_{wt}}{\\sum_s \\phi_s} \\equiv \\overline{\\phi}_{wt} \\equiv (\\overline{\\Phi})_{wt} \\equiv p(t~|~w)$ . Эту матрицу легко рассчитать.\n",
    "##### На первой итерации  будет подсчитано $n_{td} = \\sum_{d} n_{dw} p_{tdw} = \\sum_{d} n_{dw} (\\overline{\\Phi})_{wt} = (N\\overline{\\Phi})_{dt}$\n",
    "##### И, соответственно, $\\theta_{td} = \\frac{n_{td}}{\\sum_t n_{td}} =  \\frac{n_{td}}{n_d}$\n",
    "##### Введём матрицу $B_{dw} \\equiv \\frac{n_{dw}}{n_d}$, тогда $\\Theta = B \\overline{\\Phi}$ \n",
    "##### Идеологически, мы зафиксировали, что $\\Theta$ - детерминированная функция от $\\Phi$. И теперь оптимизируем не $L(\\Phi, \\Theta)$, а $\\overline{L}(\\Phi) = L(\\Phi, B \\overline{\\Phi})$\n",
    "##### Наивность решения состоит в том, что мы полностью игнорируем любые действия с $\\Theta$ на М шаге. То есть мы не считаем $n_{td}$ и не обновляем $\\theta_{td}$ (этой матрицы вообще нет). А с $n_{wt}$ мы поступаем также как на обычном М шаге. Регуляризаторы на $\\Phi$ обрабатываются точно также как и раньше ($r_{wt}$ прибавляется к $n_{wt}$), а регуляризаторы $\\Theta$ игнорируются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_thetaless_em_optimization(\n",
    "    n_dw_matrix, \n",
    "    phi_matrix,\n",
    "    regularization_list,\n",
    "    iters_count=100,\n",
    "    iteration_callback=None\n",
    "):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    T = phi_matrix.shape[0]\n",
    "    phi_matrix = np.copy(phi_matrix)\n",
    "    docptr = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        docptr.extend([doc_num] * (indptr[doc_num + 1] - indptr[doc_num]))\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for it in xrange(iters_count):\n",
    "        phi_rev_matrix = np.transpose(phi_matrix / np.sum(phi_matrix, axis=0))\n",
    "        theta_matrix = n_dw_matrix.dot(phi_rev_matrix)\n",
    "        theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "        phi_matrix_tr = np.transpose(phi_matrix)\n",
    "        \n",
    "        s_data = 1. / inner1d(theta_matrix[docptr, :], phi_matrix_tr[wordptr, :])\n",
    "        A = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                n_dw_matrix.data  * s_data , \n",
    "                n_dw_matrix.indices, \n",
    "                n_dw_matrix.indptr\n",
    "            ), \n",
    "            shape=n_dw_matrix.shape\n",
    "        ).tocsc()\n",
    "            \n",
    "        n_tw = (A.T.dot(theta_matrix)).T * phi_matrix\n",
    "        r_tw, _ = regularization_list[it](n_tw, theta_matrix)\n",
    "        n_tw += r_tw\n",
    "        n_tw[n_tw < 0] = 0\n",
    "        phi_matrix = n_tw / np.sum(n_tw, axis=1)[:, np.newaxis]\n",
    "\n",
    "        if iteration_callback is not None:\n",
    "            iteration_callback(it, phi_matrix, theta_matrix)\n",
    "    \n",
    "    print 'Iters time', time.time() - start_time    \n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARTM thetaless EM optimization\n",
    "\n",
    "##### Данное решение исправляет \"наивность\" предыдущего подхода, учитывая зависимость от $\\Theta$. Фактически, это можно написать в виде регуляризатора. Чтобы понять, как именно это сделать, вспомним как работает EM алгоритм.\n",
    "\n",
    "##### На каждой итерации сначала определяются $p_{tdw}$, фиксируются, а затем строится функционал нижней оценки: $Q(\\Phi, \\Theta) = \\sum_{dtw} n_{dw} p_{tdw} \\left( \\ln \\phi_{wt} + \\ln \\theta_{td}\\right) + R(\\Phi, \\Theta)$. Цель М-шага увеличить значение данного функционала по сравнению с $\\Phi$ и $\\Theta$ с предыдущей итерации.\n",
    "\n",
    "##### Несмотря на то, что теперь $\\Theta$ это функция от $\\Phi$, тот факт, что это всё ещё нижняя оценка, никуда не пропадает. Поэтому теперь наша цель подобрать $\\Phi$, чтобы увеличить значение по сравнению с $\\Phi$ с предыдущей итерации следующий функционал: \n",
    "\n",
    "$\\sum_{dtw} n_{dw} p_{tdw} \\left( \\ln \\phi_{wt} + \\ln (\\Theta(\\Phi))_{dt}\\right) + R(\\Phi, \\Theta(\\Phi))$.\n",
    "\n",
    "##### Возьмём производные как обычно\n",
    "\n",
    "##### $\\frac{\\partial{Q}}{\\partial{\\phi_{vr}}} = \\frac{1}{\\phi_{vr}} \\left( \\sum_{d} n_{dv} p_{rdv} + \\phi_{vr} \\frac{\\partial{R}}{\\partial{\\phi_{vr}}} + \\sum_{dtw} n_{dw} p_{tdw} \\frac{1}{\\theta_{td}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} +  \\sum_{dt} \\frac{\\partial{R}}{\\partial{\\theta_{td}}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} \\right)$\n",
    "\n",
    "##### Так как $p_{tdw} = \\frac{\\phi_{wt} \\theta_{td}}{\\sum_s \\phi_{ws} \\theta_{sd}}$, то третье слагаемое можно упростить\n",
    "\n",
    "##### $\\sum_{dtw} n_{dw} p_{tdw} \\frac{1}{\\theta_{td}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} = \\sum_{dtw} n_{dw}\\frac{\\phi_{wt}}{\\sum_s \\phi_{ws} \\theta_{sd}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} = \\sum_{dtw} A_{dw} \\phi_{wt} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}}$\n",
    "\n",
    "##### Итого\n",
    "\n",
    "##### $\\frac{\\partial{Q}}{\\partial{\\phi_{vr}}} = \\frac{1}{\\phi_{vr}} \\left( \\sum_{d} n_{dv} p_{rdv} + \\phi_{vr}\\left( \\frac{\\partial{R}}{\\partial{\\phi_{vr}}} + \\sum_{dtw} A_{dw} \\phi_{wt} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} +  \\sum_{dt} \\frac{\\partial{R}}{\\partial{\\theta_{td}}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} \\right) \\right)$\n",
    "\n",
    "##### Обозначим за $C = A \\Phi^T + \\frac{\\partial{R}}{\\partial{\\Theta}}$, тогда\n",
    "$\\frac{\\partial{Q}}{\\partial{\\phi_{vr}}} = \\frac{1}{\\phi_{vr}} \\left( \\sum_{d} n_{dv} p_{rdv} + \\phi_{vr}\\left( \\frac{\\partial{R}}{\\partial{\\phi_{vr}}} + \\sum_{dt} C_{dt} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} \\right) \\right)$\n",
    "\n",
    "##### Как видим, получившийся остаток фактически и есть требуемый регуяризатор на $\\Phi$. Осталось только найти $\\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}}$\n",
    "\n",
    "##### $\\theta_{td} = \\sum_{w} B_{dw} \\frac{\\phi_{wt}}{\\sum_s \\phi_{ws}}$. Обозначим $\\frac{1}{\\sum_s \\phi_{ws}}$ за $norm_w$, тогда\n",
    "\n",
    "$\\theta_{td} = \\sum_{w} B_{dw} \\phi_{wt} norm_w$\n",
    "\n",
    "$\\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} =  \\sum_{w} B_{dw}~norm_w \\delta_{vwrt} +  \\sum_{w} B_{dw} \\phi_{wt} \\frac{\\partial{norm_w}}{\\partial{\\phi_{vr}}} = \n",
    "\\sum_{w} B_{dw} norm_w \\delta_{vwrt} - \\sum_{w} B_{dw}~\\phi_{wt}~norm_w^2~\\delta_{vw} =\n",
    "B_{dv}~norm_v~\\delta_{rt} - B_{dv}~\\phi_{vt}~norm_w^2\n",
    "$\n",
    "\n",
    "##### Тут $\\delta$ это символ Кронекера\n",
    "\n",
    "##### Теперь\n",
    "\n",
    "$\\sum_{dt} C_{dt} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} = \\sum_{dt} C_{dt} \\left( B_{dv}~norm_v~\\delta_{rt} - B_{dv}~\\phi_{vt}~norm_v^2 \\right) =  norm_v~\\sum_d C_{dr} B_{dv} -  norm_v^2~\\sum_{dt} C_{dt} B_{dv} \\phi_{vt} = norm_v (C^T B)_{rv} - norm_v^2 (\\Phi^T C^T B)_{vv}$\n",
    "\n",
    "##### В numpy можно вычислить только диагональ при помощи einsum, поэтому эту регуляризационную добавку можно эффективно вычислить\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artm_thetaless_em_optimization(\n",
    "    n_dw_matrix, \n",
    "    phi_matrix,\n",
    "    regularization_list,\n",
    "    iters_count=100,\n",
    "    iteration_callback=None\n",
    "):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    T = phi_matrix.shape[0]\n",
    "    phi_matrix = np.copy(phi_matrix)\n",
    "    docptr = []\n",
    "    docsizes = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        size = indptr[doc_num + 1] - indptr[doc_num]\n",
    "        docptr.extend([doc_num] * size)\n",
    "        #docsizes.extend([size] * size)\n",
    "        docsizes.extend([np.sum(n_dw_matrix.data[indptr[doc_num]:indptr[doc_num + 1]])] * size)\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    docsizes = np.array(docsizes)\n",
    "    \n",
    "    B = scipy.sparse.csr_matrix(\n",
    "        (\n",
    "            1. * n_dw_matrix.data  / docsizes, \n",
    "            n_dw_matrix.indices, \n",
    "            n_dw_matrix.indptr\n",
    "        ), \n",
    "        shape=n_dw_matrix.shape\n",
    "    ).tocsc()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for it in xrange(iters_count):\n",
    "        word_norm = np.sum(phi_matrix, axis=0)\n",
    "        word_norm[word_norm == 0] = 1e-20\n",
    "        phi_rev_matrix = np.transpose(phi_matrix / word_norm)\n",
    "        \n",
    "        theta_matrix = n_dw_matrix.dot(phi_rev_matrix)\n",
    "        theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "        phi_matrix_tr = np.transpose(phi_matrix)\n",
    "        \n",
    "        s_data = 1. / inner1d(theta_matrix[docptr, :], phi_matrix_tr[wordptr, :])\n",
    "        A = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                n_dw_matrix.data  * s_data , \n",
    "                n_dw_matrix.indices, \n",
    "                n_dw_matrix.indptr\n",
    "            ), \n",
    "            shape=n_dw_matrix.shape\n",
    "        ).tocsc()\n",
    "            \n",
    "        n_tw = A.T.dot(theta_matrix).T * phi_matrix\n",
    "        \n",
    "        r_tw, r_dt = regularization_list[it](n_tw, theta_matrix)\n",
    "        theta_indices = theta_matrix > 0\n",
    "        r_dt[theta_indices] /= theta_matrix[theta_indices]\n",
    "        \n",
    "        g_dt = A.dot(phi_matrix_tr) + r_dt\n",
    "        tmp = g_dt.T * B / word_norm\n",
    "        r_tw += (tmp - np.einsum('ij,ji->i', phi_rev_matrix, tmp)) * phi_matrix\n",
    "        \n",
    "        n_tw += r_tw\n",
    "        n_tw[n_tw < 0] = 0\n",
    "        phi_matrix = n_tw / np.sum(n_tw, axis=1)[:, np.newaxis]\n",
    "        phi_matrix[np.isnan(phi_matrix)] = 0.\n",
    "\n",
    "        if iteration_callback is not None:\n",
    "            iteration_callback(it, phi_matrix, theta_matrix)\n",
    "    \n",
    "    print 'Iters time', time.time() - start_time    \n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient Descent\n",
    "\n",
    "##### Мы можем найти градиент оптимизируемой функции и сделать шаг вдоль него. Сделано для сравнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_optimization(\n",
    "    n_dw_matrix, \n",
    "    phi_matrix,\n",
    "    theta_matrix,\n",
    "    regularization_gradient_list,\n",
    "    iters_count=100,\n",
    "    loss_function=LogFunction(),\n",
    "    iteration_callback=None,\n",
    "    learning_rate=1.\n",
    "):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    T = phi_matrix.shape[0]\n",
    "    phi_matrix = np.copy(phi_matrix)\n",
    "    theta_matrix = np.copy(theta_matrix)\n",
    "    docptr = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        docptr.extend([doc_num] * (indptr[doc_num + 1] - indptr[doc_num]))\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for it in xrange(iters_count):\n",
    "        phi_matrix_tr = np.transpose(phi_matrix)\n",
    "        # следующая строчка это 60% времени работы алгоритма\n",
    "        s_data = loss_function.calc_der(inner1d(theta_matrix[docptr, :], phi_matrix_tr[wordptr, :]))\n",
    "        # следующая часть это 25% времени работы алгоритма\n",
    "        A = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                n_dw_matrix.data * s_data, \n",
    "                n_dw_matrix.indices, \n",
    "                n_dw_matrix.indptr\n",
    "            ), \n",
    "            shape=n_dw_matrix.shape\n",
    "        ).tocsc()\n",
    "        # Остальное это 15% времени\n",
    "        g_tw = theta_matrix.T * A\n",
    "        g_dt = A.dot(phi_matrix_tr)\n",
    "        \n",
    "        r_tw, r_dt = regularization_gradient_list[it](phi_matrix, theta_matrix)\n",
    "        g_tw += r_tw\n",
    "        g_dt += r_dt\n",
    "        \n",
    "        g_tw -= np.sum(g_tw * phi_matrix, axis=1)[:, np.newaxis]\n",
    "        g_dt -= np.sum(g_dt * theta_matrix, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        phi_matrix += g_tw * learning_rate\n",
    "        theta_matrix += g_dt * learning_rate\n",
    "        \n",
    "        phi_matrix[phi_matrix < 0] = 0\n",
    "        theta_matrix[theta_matrix < 0] = 0\n",
    "        \n",
    "        phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "        theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        if iteration_callback is not None:\n",
    "            iteration_callback(it, phi_matrix, theta_matrix)\n",
    "    \n",
    "    print 'Iters time', time.time() - start_time  \n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Оценка качества классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_score(theta, targets):\n",
    "    C_2d_range = [1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
    "    gamma_2d_range = [1e-3, 1e-2, 1e-1, 1, 1e1]\n",
    "    best_C, best_gamma, best_val = None, None, 0.\n",
    "    best_cv_algo_score_on_test = 0.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(theta, targets, test_size=0.30, stratify=targets, random_state=42)\n",
    "    for C in C_2d_range:\n",
    "        for gamma in gamma_2d_range:\n",
    "            val = np.mean(cross_val_score(SVC(C=C, gamma=gamma), X_train, y_train, scoring='accuracy', cv=4))\n",
    "            algo = SVC(C=C, gamma=gamma).fit(X_train, y_train)\n",
    "            test_score = accuracy_score(y_test, algo.predict(X_test))\n",
    "            print 'SVM(C={}, gamma={}) cv-score: {}  test-score: {}'.format(\n",
    "                C,\n",
    "                gamma,\n",
    "                round(val, 3),\n",
    "                round(test_score, 3)\n",
    "            )\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_C = C\n",
    "                best_gamma = gamma\n",
    "                best_cv_algo_score_on_test = test_score\n",
    "    print '\\n\\n\\nBest cv params: C={}, gamma={}\\nCV score: {}\\nTest score:{}'.format(\n",
    "        best_C,\n",
    "        best_gamma,\n",
    "        round(best_val, 3),\n",
    "        round(best_cv_algo_score_on_test, 3)\n",
    "    )\n",
    "    return best_C, best_gamma, best_val, best_cv_algo_score_on_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artm_calc_topic_correlation(phi):\n",
    "    T, W = phi.shape\n",
    "    return (np.sum(np.sum(phi, axis=0) ** 2) - np.sum(phi ** 2)) / (T * (T - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artm_calc_perplexity_factory(n_dw_matrix):\n",
    "    helper = create_calculate_likelihood_like_function(\n",
    "        loss_function=LogFunction(),\n",
    "        n_dw_matrix=n_dw_matrix\n",
    "    )\n",
    "    total_words_number = n_dw_matrix.sum()\n",
    "    return lambda phi, theta: np.exp(- helper(phi, theta) / total_words_number)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, documents_number, top_size):\n",
    "    def fun(phi):\n",
    "        T, W = phi.shape\n",
    "        pmi = 0.\n",
    "        for t in xrange(T):\n",
    "            top = heapq.nlargest(top_size, xrange(W), key=lambda w: phi[t, w])\n",
    "            for w1 in top:\n",
    "                for w2 in top:\n",
    "                    if w1 != w2:\n",
    "                        pmi += np.log(documents_number * (doc_cooccurences[(w1, w2)] + 1e-4) * 1. / doc_occurences[w1] / doc_occurences[w2])\n",
    "        return pmi / (T * top_size * (top_size - 1))\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры запусков с разделением на train и test по словам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    categories=['sci.electronics', 'sci.med', 'sci.space'],\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0 documents from 2961\n",
      "Processed:  500 documents from 2961\n",
      "Processed:  1000 documents from 2961\n",
      "Processed:  1500 documents from 2961\n",
      "Processed:  2000 documents from 2961\n",
      "Processed:  2500 documents from 2961\n",
      "Nonzero values: 130804\n",
      "CPU times: user 3min 19s, sys: 2.2 s, total: 3min 22s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_n_dw_matrix, test_n_dw_matrix, ttt_token_2_num, ttt_num_2_token, ttt_doc_targets, ttt_doc_occurences, ttt_doc_cooccurences = prepare_dataset(dataset, calc_cooccurences=True, train_test_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLSA: EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "\tTrain perplexity:   3189.46334242\n",
      "\tTest perplexity:     3531.73423591\n",
      "\tTopics correlation  0.000725998723954\n",
      "\tPhi sparsity        0.0014551152899\n",
      "\tTheta sparsity      0.000216684723727\n",
      "\tTop5 PMI            0.423826388526\n",
      "\tTop10 PMI           0.468634502156\n",
      "\tTop20 PMI           0.509231453518\n",
      "Iteration 10\n",
      "\tTrain perplexity:   2248.79526472\n",
      "\tTest perplexity:     3076.36649028\n",
      "\tTopics correlation  0.000662166455725\n",
      "\tPhi sparsity        0.00183568390419\n",
      "\tTheta sparsity      0.322246298303\n",
      "\tTop5 PMI            0.701193486577\n",
      "\tTop10 PMI           0.581648355023\n",
      "\tTop20 PMI           0.593460217964\n",
      "Iteration 20\n",
      "\tTrain perplexity:   1799.93196596\n",
      "\tTest perplexity:     2958.83947081\n",
      "\tTopics correlation  0.000620877413692\n",
      "\tPhi sparsity        0.155238415044\n",
      "\tTheta sparsity      0.584832069339\n",
      "\tTop5 PMI            0.934272740933\n",
      "\tTop10 PMI           0.920020291533\n",
      "\tTop20 PMI           0.805215334693\n",
      "Iteration 30\n",
      "\tTrain perplexity:   1691.70832655\n",
      "\tTest perplexity:     3242.32525268\n",
      "\tTopics correlation  0.000602092519829\n",
      "\tPhi sparsity        0.369588090441\n",
      "\tTheta sparsity      0.651932105453\n",
      "\tTop5 PMI            1.00482880142\n",
      "\tTop10 PMI           0.966133541135\n",
      "\tTop20 PMI           0.965540204588\n",
      "Iteration 40\n",
      "\tTrain perplexity:   1649.93268664\n",
      "\tTest perplexity:     3664.1425228\n",
      "\tTopics correlation  0.000595189412412\n",
      "\tPhi sparsity        0.476449518693\n",
      "\tTheta sparsity      0.678584326472\n",
      "\tTop5 PMI            0.937658433402\n",
      "\tTop10 PMI           1.09209050171\n",
      "\tTop20 PMI           0.992041201545\n",
      "Iteration 50\n",
      "\tTrain perplexity:   1623.10976836\n",
      "\tTest perplexity:     4138.48032795\n",
      "\tTopics correlation  0.000589758785033\n",
      "\tPhi sparsity        0.536232370719\n",
      "\tTheta sparsity      0.690393643915\n",
      "\tTop5 PMI            1.03946811644\n",
      "\tTop10 PMI           1.05509803735\n",
      "\tTop20 PMI           1.0378058479\n",
      "Iteration 60\n",
      "\tTrain perplexity:   1605.97200833\n",
      "\tTest perplexity:     4633.84558771\n",
      "\tTopics correlation  0.000583992501933\n",
      "\tPhi sparsity        0.572912469219\n",
      "\tTheta sparsity      0.698916576381\n",
      "\tTop5 PMI            1.12625513558\n",
      "\tTop10 PMI           1.08112738889\n",
      "\tTop20 PMI           1.09986693985\n",
      "Iteration 70\n",
      "\tTrain perplexity:   1594.26739283\n",
      "\tTest perplexity:     5139.01562856\n",
      "\tTopics correlation  0.000579430878598\n",
      "\tPhi sparsity        0.597022610253\n",
      "\tTheta sparsity      0.702744673167\n",
      "\tTop5 PMI            1.12145683789\n",
      "\tTop10 PMI           1.13646849782\n",
      "\tTop20 PMI           1.12634731172\n",
      "Iteration 80\n",
      "\tTrain perplexity:   1586.82015154\n",
      "\tTest perplexity:     5629.18977432\n",
      "\tTopics correlation  0.000575714386738\n",
      "\tPhi sparsity        0.614573539288\n",
      "\tTheta sparsity      0.706789454677\n",
      "\tTop5 PMI            1.12600792705\n",
      "\tTop10 PMI           1.13646849782\n",
      "\tTop20 PMI           1.1222699098\n",
      "Iteration 90\n",
      "\tTrain perplexity:   1581.03296661\n",
      "\tTest perplexity:     6107.7153217\n",
      "\tTopics correlation  0.000573050620265\n",
      "\tPhi sparsity        0.626863666891\n",
      "\tTheta sparsity      0.709498013723\n",
      "\tTop5 PMI            1.15144315402\n",
      "\tTop10 PMI           1.15011912387\n",
      "\tTop20 PMI           1.11659043269\n",
      "Iteration 100\n",
      "\tTrain perplexity:   1576.29426099\n",
      "\tTest perplexity:     6564.95043759\n",
      "\tTopics correlation  0.000570134790302\n",
      "\tPhi sparsity        0.635818222521\n",
      "\tTheta sparsity      0.711484290358\n",
      "\tTop5 PMI            1.12358117377\n",
      "\tTop10 PMI           1.15262987425\n",
      "\tTop20 PMI           1.12022202136\n",
      "Iters time 4.21734595299\n"
     ]
    }
   ],
   "source": [
    "D, W = train_n_dw_matrix.shape\n",
    "T = 10\n",
    "\n",
    "np.random.seed(64)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.ones(shape=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = create_reg_lda(0., 0.)\n",
    "\n",
    "_train_perplexity = artm_calc_perplexity_factory(train_n_dw_matrix) \n",
    "_test_perplexity = artm_calc_perplexity_factory(test_n_dw_matrix)\n",
    "_top5_pmi = artm_calc_pmi_top_factory(ttt_doc_occurences, ttt_doc_cooccurences, train_n_dw_matrix.shape[0], 5)\n",
    "_top10_pmi = artm_calc_pmi_top_factory(ttt_doc_occurences, ttt_doc_cooccurences, train_n_dw_matrix.shape[0], 10)\n",
    "_top20_pmi = artm_calc_pmi_top_factory(ttt_doc_occurences, ttt_doc_cooccurences, train_n_dw_matrix.shape[0], 20)\n",
    "\n",
    "def callback(it, phi, theta):\n",
    "    if it % 10 == 0:\n",
    "        print 'Iteration', it\n",
    "        print '\\tTrain perplexity:   {}'.format(_train_perplexity(phi, theta))\n",
    "        print '\\tTest perplexity:     {}'.format(_test_perplexity(phi, theta))\n",
    "        print '\\tTopics correlation  {}'.format(artm_calc_topic_correlation(phi))\n",
    "        print '\\tPhi sparsity        {}'.format(1. * np.sum(phi < 1e-20) / np.sum(phi >= 0))\n",
    "        print '\\tTheta sparsity      {}'.format(1. * np.sum(theta < 0.01) / np.sum(theta >= 0))\n",
    "        print '\\tTop5 PMI            {}'.format(_top5_pmi(phi))\n",
    "        print '\\tTop10 PMI           {}'.format(_top10_pmi(phi))\n",
    "        print '\\tTop20 PMI           {}'.format(_top20_pmi(phi))\n",
    "\n",
    "\n",
    "phi, theta = em_optimization(\n",
    "    n_dw_matrix=train_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    theta_matrix=theta_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=101,\n",
    "    loss_function=LogFunction(),\n",
    "    iteration_callback=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\tspace\n",
      "\tavailable\n",
      "\ttheory\n",
      "\tsystem\n",
      "\talso\n",
      "\tinformation\n",
      "\timage\n",
      "\tkeyboard\n",
      "\tedu\n",
      "\tbook\n",
      "\tnasa\n",
      "\tastronomy\n",
      "\ttime\n",
      "\tpub\n",
      "\tstar\n",
      "\tdatum\n",
      "\tarchive\n",
      "\tuniverse\n",
      "\tftp\n",
      "\tlist\n",
      "1\n",
      "\tused\n",
      "\tget\n",
      "\tknow\n",
      "\tbattery\n",
      "\tpower\n",
      "\tthank\n",
      "\tcircuit\n",
      "\tradio\n",
      "\tchip\n",
      "\tuse\n",
      "\tgood\n",
      "\tanyone\n",
      "\tedu\n",
      "\toutput\n",
      "\tinput\n",
      "\tmake\n",
      "\tdetector\n",
      "\tvoltage\n",
      "\ttime\n",
      "\tneed\n",
      "2\n",
      "\tpatient\n",
      "\ttreatment\n",
      "\tstudy\n",
      "\tvitamin\n",
      "\tfood\n",
      "\tdisease\n",
      "\talso\n",
      "\teffect\n",
      "\tdrug\n",
      "\tdiet\n",
      "\tcause\n",
      "\tget\n",
      "\tknow\n",
      "\tinfection\n",
      "\tmedical\n",
      "\tperson\n",
      "\tused\n",
      "\tcancer\n",
      "\tbody\n",
      "\tgood\n",
      "3\n",
      "\thealth\n",
      "\thiv\n",
      "\tmedical\n",
      "\tdisease\n",
      "\tnumber\n",
      "\tapril\n",
      "\tresearch\n",
      "\tpage\n",
      "\tcenter\n",
      "\tyear\n",
      "\tcancer\n",
      "\taids\n",
      "\tinformation\n",
      "\tnational\n",
      "\tuniversity\n",
      "\tpatient\n",
      "\tnewsletter\n",
      "\tchild\n",
      "\tvolume\n",
      "\tperson\n",
      "4\n",
      "\tget\n",
      "\tperson\n",
      "\tthink\n",
      "\tgo\n",
      "\tsay\n",
      "\tknow\n",
      "\tsee\n",
      "\tthing\n",
      "\ttake\n",
      "\ttime\n",
      "\tmake\n",
      "\tseem\n",
      "\tyear\n",
      "\tproblem\n",
      "\tday\n",
      "\tmuch\n",
      "\teven\n",
      "\tsomething\n",
      "\tdoctor\n",
      "\ttry\n",
      "5\n",
      "\tmission\n",
      "\tearth\n",
      "\torbit\n",
      "\tspace\n",
      "\tmoon\n",
      "\tsolar\n",
      "\tplanet\n",
      "\tlunar\n",
      "\tprobe\n",
      "\tsystem\n",
      "\tvenus\n",
      "\tmar\n",
      "\tsurface\n",
      "\tspacecraft\n",
      "\tfirst\n",
      "\tatmosphere\n",
      "\tyear\n",
      "\tshuttle\n",
      "\tmake\n",
      "\tsatellite\n",
      "6\n",
      "\tedu\n",
      "\tcom\n",
      "\tpost\n",
      "\tknow\n",
      "\tphone\n",
      "\tgroup\n",
      "\tanyone\n",
      "\tinternet\n",
      "\tthank\n",
      "\temail\n",
      "\tget\n",
      "\tnet\n",
      "\tboard\n",
      "\tmail\n",
      "\tnewsgroup\n",
      "\tcs\n",
      "\tname\n",
      "\tline\n",
      "\tlist\n",
      "\tcompany\n",
      "7\n",
      "\tspace\n",
      "\tlaunch\n",
      "\tsatellite\n",
      "\tyear\n",
      "\tprogram\n",
      "\tscience\n",
      "\tcost\n",
      "\ttechnology\n",
      "\tsystem\n",
      "\tshuttle\n",
      "\trocket\n",
      "\tstation\n",
      "\talso\n",
      "\tvehicle\n",
      "\tnew\n",
      "\tbuild\n",
      "\tcommercial\n",
      "\tget\n",
      "\tlaunch\n",
      "\tdevelopment\n",
      "8\n",
      "\twire\n",
      "\tground\n",
      "\tpower\n",
      "\twiring\n",
      "\tcircuit\n",
      "\tused\n",
      "\toutlet\n",
      "\tcable\n",
      "\tneutral\n",
      "\tbox\n",
      "\tconductor\n",
      "\trequire\n",
      "\tuse\n",
      "\tconnect\n",
      "\thouse\n",
      "\tcurrent\n",
      "\tpanel\n",
      "\telectrical\n",
      "\tspacecraft\n",
      "\tsystem\n",
      "9\n",
      "\tsoftware\n",
      "\twater\n",
      "\tused\n",
      "\tget\n",
      "\tmake\n",
      "\tcopy\n",
      "\tprogram\n",
      "\tknow\n",
      "\twork\n",
      "\twant\n",
      "\talso\n",
      "\tuser\n",
      "\tgo\n",
      "\tuse\n",
      "\tmachine\n",
      "\ttime\n",
      "\tnew\n",
      "\tbuy\n",
      "\teven\n",
      "\tcompany\n"
     ]
    }
   ],
   "source": [
    "for t in xrange(T):\n",
    "    print t\n",
    "    top = heapq.nlargest(20, xrange(W), key=lambda w: phi[t, w])\n",
    "    print '\\n'.join('\\t' + x[:-3] for x in map(ttt_num_2_token.get, top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLSA: Thetaless EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "\tTrain perplexity:   3185.57389908\n",
      "\tTest perplexity:     3529.86337686\n",
      "\tTopics correlation  0.00072332950865\n",
      "\tPhi sparsity        0.0014551152899\n",
      "\tTheta sparsity      0.000216684723727\n",
      "\tTop5 PMI            0.42418486873\n",
      "\tTop10 PMI           0.471481807837\n",
      "\tTop20 PMI           0.50969968015\n",
      "Iteration 10\n",
      "\tTrain perplexity:   2295.8825486\n",
      "\tTest perplexity:     2803.53781419\n",
      "\tTopics correlation  0.000555588587629\n",
      "\tPhi sparsity        0.282012536378\n",
      "\tTheta sparsity      0.0150957024196\n",
      "\tTop5 PMI            0.821836003313\n",
      "\tTop10 PMI           0.824893165098\n",
      "\tTop20 PMI           0.871302561325\n",
      "Iteration 20\n",
      "\tTrain perplexity:   1986.3987337\n",
      "\tTest perplexity:     2581.85168597\n",
      "\tTopics correlation  0.00031856497952\n",
      "\tPhi sparsity        0.400526080143\n",
      "\tTheta sparsity      0.0900325027086\n",
      "\tTop5 PMI            0.965319406083\n",
      "\tTop10 PMI           1.11859956138\n",
      "\tTop20 PMI           1.07747160688\n",
      "Iteration 30\n",
      "\tTrain perplexity:   1913.24748471\n",
      "\tTest perplexity:     2547.76185602\n",
      "\tTopics correlation  0.000214279090678\n",
      "\tPhi sparsity        0.489545556302\n",
      "\tTheta sparsity      0.139942217407\n",
      "\tTop5 PMI            1.01628771649\n",
      "\tTop10 PMI           1.15508677696\n",
      "\tTop20 PMI           1.15172362249\n",
      "Iteration 40\n",
      "\tTrain perplexity:   1885.26890709\n",
      "\tTest perplexity:     2548.30435349\n",
      "\tTopics correlation  0.000180291443475\n",
      "\tPhi sparsity        0.564696664428\n",
      "\tTheta sparsity      0.161719032142\n",
      "\tTop5 PMI            1.03637312509\n",
      "\tTop10 PMI           1.11412176615\n",
      "\tTop20 PMI           1.1893808721\n",
      "Iteration 50\n",
      "\tTrain perplexity:   1870.88347382\n",
      "\tTest perplexity:     2547.0781699\n",
      "\tTopics correlation  0.000164713039788\n",
      "\tPhi sparsity        0.619744795165\n",
      "\tTheta sparsity      0.1744312026\n",
      "\tTop5 PMI            1.02797264561\n",
      "\tTop10 PMI           1.15656983808\n",
      "\tTop20 PMI           1.19758470493\n",
      "Iteration 60\n",
      "\tTrain perplexity:   1860.63446298\n",
      "\tTest perplexity:     2551.47161686\n",
      "\tTopics correlation  0.000154709554013\n",
      "\tPhi sparsity        0.659402283412\n",
      "\tTheta sparsity      0.18374864572\n",
      "\tTop5 PMI            1.03046163628\n",
      "\tTop10 PMI           1.10251973327\n",
      "\tTop20 PMI           1.20813925006\n",
      "Iteration 70\n",
      "\tTrain perplexity:   1854.42595656\n",
      "\tTest perplexity:     2559.33274187\n",
      "\tTopics correlation  0.000146782351643\n",
      "\tPhi sparsity        0.687631520036\n",
      "\tTheta sparsity      0.190465872156\n",
      "\tTop5 PMI            1.04799682543\n",
      "\tTop10 PMI           1.10514204808\n",
      "\tTop20 PMI           1.21532834694\n",
      "Iteration 80\n",
      "\tTrain perplexity:   1849.57730249\n",
      "\tTest perplexity:     2567.2152707\n",
      "\tTopics correlation  0.000137757471052\n",
      "\tPhi sparsity        0.708529214238\n",
      "\tTheta sparsity      0.196027446732\n",
      "\tTop5 PMI            1.05761597147\n",
      "\tTop10 PMI           1.20566805533\n",
      "\tTop20 PMI           1.20303050822\n",
      "Iteration 90\n",
      "\tTrain perplexity:   1845.41033666\n",
      "\tTest perplexity:     2579.73545431\n",
      "\tTopics correlation  0.000130607187835\n",
      "\tPhi sparsity        0.725386165212\n",
      "\tTheta sparsity      0.20119176598\n",
      "\tTop5 PMI            1.06668474184\n",
      "\tTop10 PMI           1.17542619342\n",
      "\tTop20 PMI           1.21970485043\n",
      "Iteration 100\n",
      "\tTrain perplexity:   1842.04415982\n",
      "\tTest perplexity:     2592.37867044\n",
      "\tTopics correlation  0.000127007748044\n",
      "\tPhi sparsity        0.737631520036\n",
      "\tTheta sparsity      0.205453232214\n",
      "\tTop5 PMI            1.06020357246\n",
      "\tTop10 PMI           1.16718630736\n",
      "\tTop20 PMI           1.21306419078\n",
      "Iters time 4.98649501801\n"
     ]
    }
   ],
   "source": [
    "D, W = train_n_dw_matrix.shape\n",
    "T = 10\n",
    "\n",
    "np.random.seed(64)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.ones(shape=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = create_reg_lda(0., 0.)\n",
    "\n",
    "_train_perplexity = artm_calc_perplexity_factory(train_n_dw_matrix) \n",
    "_test_perplexity = artm_calc_perplexity_factory(test_n_dw_matrix)\n",
    "_top5_pmi = artm_calc_pmi_top_factory(ttt_doc_occurences, ttt_doc_cooccurences, train_n_dw_matrix.shape[0], 5)\n",
    "_top10_pmi = artm_calc_pmi_top_factory(ttt_doc_occurences, ttt_doc_cooccurences, train_n_dw_matrix.shape[0], 10)\n",
    "_top20_pmi = artm_calc_pmi_top_factory(ttt_doc_occurences, ttt_doc_cooccurences, train_n_dw_matrix.shape[0], 20)\n",
    "\n",
    "def callback(it, phi, theta):\n",
    "    if it % 10 == 0:\n",
    "        print 'Iteration', it\n",
    "        print '\\tTrain perplexity:   {}'.format(_train_perplexity(phi, theta))\n",
    "        print '\\tTest perplexity:     {}'.format(_test_perplexity(phi, theta))\n",
    "        print '\\tTopics correlation  {}'.format(artm_calc_topic_correlation(phi))\n",
    "        print '\\tPhi sparsity        {}'.format(1. * np.sum(phi < 1e-20) / np.sum(phi >= 0))\n",
    "        print '\\tTheta sparsity      {}'.format(1. * np.sum(theta < 0.01) / np.sum(theta >= 0))\n",
    "        print '\\tTop5 PMI            {}'.format(_top5_pmi(phi))\n",
    "        print '\\tTop10 PMI           {}'.format(_top10_pmi(phi))\n",
    "        print '\\tTop20 PMI           {}'.format(_top20_pmi(phi))\n",
    "\n",
    "\n",
    "phi, theta = artm_thetaless_em_optimization(\n",
    "    n_dw_matrix=train_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=101,\n",
    "    iteration_callback=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\tavailable\n",
      "\tbook\n",
      "\tdatum\n",
      "\timage\n",
      "\ttheory\n",
      "\treference\n",
      "\tgeneral\n",
      "\tfield\n",
      "\tsystem\n",
      "\tbase\n",
      "\tastronomy\n",
      "\tkeyboard\n",
      "\tstar\n",
      "\tray\n",
      "\texperiment\n",
      "\tword\n",
      "\tradar\n",
      "\telement\n",
      "\tpc\n",
      "\tposition\n",
      "1\n",
      "\tanyone\n",
      "\tthank\n",
      "\tlook\n",
      "\tline\n",
      "\tradio\n",
      "\tuse\n",
      "\tchip\n",
      "\tbuy\n",
      "\trun\n",
      "\tbattery\n",
      "\toutput\n",
      "\tlead\n",
      "\tsoon\n",
      "\tfrequency\n",
      "\tlike\n",
      "\tinput\n",
      "\tappreciate\n",
      "\tsignal\n",
      "\tdetector\n",
      "\tvoltage\n",
      "2\n",
      "\tpatient\n",
      "\tcause\n",
      "\teffect\n",
      "\tdisease\n",
      "\tdoctor\n",
      "\tfood\n",
      "\tstudy\n",
      "\ttreatment\n",
      "\tdrug\n",
      "\tinfection\n",
      "\tvitamin\n",
      "\tbody\n",
      "\tdiet\n",
      "\tmedical\n",
      "\tmonth\n",
      "\tmedicine\n",
      "\tclinical\n",
      "\tcell\n",
      "\tpain\n",
      "\teat\n",
      "3\n",
      "\tinformation\n",
      "\tnumber\n",
      "\thealth\n",
      "\tresearch\n",
      "\tcenter\n",
      "\tapril\n",
      "\tmedical\n",
      "\thiv\n",
      "\tuniversity\n",
      "\tnational\n",
      "\tinclude\n",
      "\tpage\n",
      "\tyear\n",
      "\tchild\n",
      "\tstate\n",
      "\taids\n",
      "\tvolume\n",
      "\tcancer\n",
      "\tissue\n",
      "\treport\n",
      "4\n",
      "\tget\n",
      "\tknow\n",
      "\ttime\n",
      "\tmake\n",
      "\tperson\n",
      "\tgo\n",
      "\tsee\n",
      "\tsay\n",
      "\tthink\n",
      "\ttake\n",
      "\tgood\n",
      "\tproblem\n",
      "\tthing\n",
      "\tfind\n",
      "\twant\n",
      "\twell\n",
      "\tmany\n",
      "\tway\n",
      "\teven\n",
      "\tgive\n",
      "5\n",
      "\tearth\n",
      "\tmission\n",
      "\torbit\n",
      "\tmoon\n",
      "\tsolar\n",
      "\tspacecraft\n",
      "\tplanet\n",
      "\tlunar\n",
      "\tprobe\n",
      "\tfirst\n",
      "\tsystem\n",
      "\tsurface\n",
      "\tmar\n",
      "\tdegree\n",
      "\tvenus\n",
      "\tenergy\n",
      "\tsun\n",
      "\tshuttle\n",
      "\tmass\n",
      "\tmile\n",
      "6\n",
      "\tedu\n",
      "\tcom\n",
      "\tmail\n",
      "\tpost\n",
      "\tname\n",
      "\tsend\n",
      "\tphone\n",
      "\tgroup\n",
      "\tcompany\n",
      "\tarticle\n",
      "\tplease\n",
      "\temail\n",
      "\tboard\n",
      "\tinternet\n",
      "\tlist\n",
      "\tnet\n",
      "\tsci\n",
      "\tmessage\n",
      "\tplease\n",
      "\tgov\n",
      "7\n",
      "\tspace\n",
      "\tprogram\n",
      "\tyear\n",
      "\tscience\n",
      "\tsatellite\n",
      "\tlaunch\n",
      "\tcost\n",
      "\tsystem\n",
      "\ttechnology\n",
      "\tproject\n",
      "\tworld\n",
      "\tbuild\n",
      "\tstation\n",
      "\tdevelopment\n",
      "\tscientific\n",
      "\tshuttle\n",
      "\tvehicle\n",
      "\tdesign\n",
      "\tflight\n",
      "\tnasa\n",
      "8\n",
      "\tpower\n",
      "\tcurrent\n",
      "\twire\n",
      "\tground\n",
      "\tcircuit\n",
      "\tbox\n",
      "\trequire\n",
      "\tlight\n",
      "\tusually\n",
      "\tplace\n",
      "\twiring\n",
      "\ttest\n",
      "\tsubject\n",
      "\tequipment\n",
      "\ttemperature\n",
      "\tunit\n",
      "\trange\n",
      "\tcable\n",
      "\toutlet\n",
      "\thot\n",
      "9\n",
      "\tused\n",
      "\talso\n",
      "\tnew\n",
      "\thigh\n",
      "\tneed\n",
      "\twork\n",
      "\tpart\n",
      "\tuse\n",
      "\twater\n",
      "\tpossible\n",
      "\tlow\n",
      "\tsoftware\n",
      "\ttype\n",
      "\tsmall\n",
      "\tuse\n",
      "\tarea\n",
      "\tbit\n",
      "\tcomputer\n",
      "\torder\n",
      "\tsystem\n"
     ]
    }
   ],
   "source": [
    "for t in xrange(T):\n",
    "    print t\n",
    "    top = heapq.nlargest(20, xrange(W), key=lambda w: phi[t, w])\n",
    "    print '\\n'.join('\\t' + x[:-3] for x in map(ttt_num_2_token.get, top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(phi, axis=0) < 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(theta, axis=1) < 0.9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Разделение на train test по документам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=['sci.electronics', 'sci.med', 'sci.space', 'sci.crypt', 'rec.sport.baseball', 'rec.sport.hockey'],\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0 documents from 3570\n",
      "Processed:  500 documents from 3570\n",
      "Processed:  1000 documents from 3570\n",
      "Processed:  1500 documents from 3570\n",
      "Processed:  2000 documents from 3570\n",
      "Processed:  2500 documents from 3570\n",
      "Processed:  3000 documents from 3570\n",
      "Processed:  3500 documents from 3570\n",
      "Nonzero values: 199636\n",
      "CPU times: user 4min 3s, sys: 96 ms, total: 4min 4s\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_dw_matrix_doc_train, token_2_num_doc_train, num_2_token_doc_train, doc_targets_doc_train = prepare_dataset(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories=['sci.electronics', 'sci.med', 'sci.space', 'sci.crypt', 'rec.sport.baseball', 'rec.sport.hockey'],\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero values: 109917\n",
      "CPU times: user 1min 8s, sys: 8 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_dw_matrix_doc_test, token_2_num_doc_test, num_2_token_doc_test, doc_targets_doc_test = prepare_dataset(dataset_test, token_2_num=token_2_num_doc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters time 4.03662395477\n",
      "SVM(C=0.1, gamma=0.001) cv-score: 0.208  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.208  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.643  test-score: 0.65\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.676  test-score: 0.685\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.69  test-score: 0.7\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.208  test-score: 0.169\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.643  test-score: 0.652\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.677  test-score: 0.683\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.688  test-score: 0.7\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.695  test-score: 0.704\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.643  test-score: 0.653\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.678  test-score: 0.682\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.687  test-score: 0.7\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.688  test-score: 0.709\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.685  test-score: 0.697\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.677  test-score: 0.683\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.689  test-score: 0.691\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.692  test-score: 0.706\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.693  test-score: 0.708\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.679  test-score: 0.687\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.69  test-score: 0.693\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.685  test-score: 0.704\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.691  test-score: 0.714\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.689  test-score: 0.703\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.67  test-score: 0.676\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.686  test-score: 0.691\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.692  test-score: 0.705\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.693  test-score: 0.711\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.684  test-score: 0.703\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.648  test-score: 0.661\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=1.0, gamma=10.0\n",
      "CV score: 0.695\n",
      "Test score:0.704\n"
     ]
    }
   ],
   "source": [
    "D, W = n_dw_matrix_doc_train.shape\n",
    "T = 5\n",
    "\n",
    "np.random.seed(47)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.ones(shape=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = trivial_regularization\n",
    "\n",
    "phi, theta = em_optimization(\n",
    "    n_dw_matrix=n_dw_matrix_doc_train, \n",
    "    phi_matrix=phi_matrix,\n",
    "    theta_matrix=theta_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=150,\n",
    "    loss_function=LogFunction()\n",
    ")\n",
    "\n",
    "best_C, best_gamma, _, _ = svm_score(theta, doc_targets_doc_train)\n",
    "algo = SVC(C=best_C, gamma=best_gamma).fit(theta, doc_targets_doc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSA not const phi\n",
      "Iters time 0.0184760093689\n",
      "1 0.498251748252\n",
      "Iters time 0.0313498973846\n",
      "2 0.613199300699\n",
      "Iters time 0.0481491088867\n",
      "3 0.644667832168\n",
      "Iters time 0.0741291046143\n",
      "4 0.666520979021\n",
      "Iters time 0.0801839828491\n",
      "5 0.683129370629\n",
      "\n",
      "PLSA const phi\n",
      "Iters time 0.014631986618\n",
      "1 0.498251748252\n",
      "Iters time 0.0475838184357\n",
      "2 0.626311188811\n",
      "Iters time 0.0461721420288\n",
      "3 0.666083916084\n",
      "Iters time 0.0600559711456\n",
      "4 0.690996503497\n",
      "Iters time 0.0745630264282\n",
      "5 0.692307692308\n",
      "\n",
      "ARTM thetaless\n",
      "Iters time 0.0183432102203\n",
      "1 0.498251748252\n",
      "Iters time 0.0338151454926\n",
      "2 0.54458041958\n",
      "Iters time 0.0561671257019\n",
      "3 0.564248251748\n",
      "Iters time 0.080549955368\n",
      "4 0.57736013986\n",
      "Iters time 0.104263067245\n",
      "5 0.583916083916\n"
     ]
    }
   ],
   "source": [
    "D, W = n_dw_matrix_doc_test.shape\n",
    "theta_matrix = np.ones(shape=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "print 'PLSA not const phi' \n",
    "for iters in xrange(1, 6):\n",
    "    _, theta_test = em_optimization(\n",
    "        n_dw_matrix=n_dw_matrix_doc_test, \n",
    "        phi_matrix=phi,\n",
    "        theta_matrix=theta_matrix,\n",
    "        regularization_list=regularization_list,\n",
    "        iters_count=iters,\n",
    "        loss_function=LogFunction(),\n",
    "    )\n",
    "    print iters, accuracy_score(algo.predict(theta_test), doc_targets_doc_test)\n",
    "\n",
    "print '\\nPLSA const phi' \n",
    "for iters in xrange(1, 6):\n",
    "    _, theta_test = em_optimization(\n",
    "        n_dw_matrix=n_dw_matrix_doc_test, \n",
    "        phi_matrix=phi,\n",
    "        theta_matrix=theta_matrix,\n",
    "        regularization_list=regularization_list,\n",
    "        iters_count=iters,\n",
    "        loss_function=LogFunction(),\n",
    "        const_phi=True\n",
    "    )\n",
    "    print iters, accuracy_score(algo.predict(theta_test), doc_targets_doc_test)\n",
    "    \n",
    "print '\\nARTM thetaless'    \n",
    "for iters in xrange(1, 6):\n",
    "    _, theta_test = artm_thetaless_em_optimization(\n",
    "        n_dw_matrix=n_dw_matrix_doc_test, \n",
    "        phi_matrix=phi,\n",
    "        regularization_list=regularization_list,\n",
    "        iters_count=iters,\n",
    "    )\n",
    "    print iters, accuracy_score(algo.predict(theta_test), doc_targets_doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters time 4.7924489975\n",
      "SVM(C=0.1, gamma=0.001) cv-score: 0.206  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.206  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.543  test-score: 0.588\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.63  test-score: 0.633\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.638  test-score: 0.643\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.206  test-score: 0.169\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.541  test-score: 0.592\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.634  test-score: 0.633\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.644  test-score: 0.646\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.639  test-score: 0.652\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.541  test-score: 0.595\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.634  test-score: 0.633\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.644  test-score: 0.646\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.635  test-score: 0.658\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.643  test-score: 0.653\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.635  test-score: 0.633\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.642  test-score: 0.645\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.635  test-score: 0.649\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.64  test-score: 0.658\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.63  test-score: 0.631\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.643  test-score: 0.645\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.636  test-score: 0.652\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.633  test-score: 0.658\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.642  test-score: 0.656\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.605  test-score: 0.606\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.639  test-score: 0.653\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.636  test-score: 0.648\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.634  test-score: 0.662\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.643  test-score: 0.654\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.581  test-score: 0.601\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=1.0, gamma=1\n",
      "CV score: 0.644\n",
      "Test score:0.646\n"
     ]
    }
   ],
   "source": [
    "D, W = n_dw_matrix_doc_train.shape\n",
    "T = 5\n",
    "\n",
    "np.random.seed(47)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.ones(shape=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = trivial_regularization\n",
    "\n",
    "phi, theta = artm_thetaless_em_optimization(\n",
    "    n_dw_matrix=n_dw_matrix_doc_train, \n",
    "    phi_matrix=phi_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=150\n",
    ")\n",
    "\n",
    "best_C, best_gamma, _, _ = svm_score(theta, doc_targets_doc_train)\n",
    "algo = SVC(C=best_C, gamma=best_gamma).fit(theta, doc_targets_doc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSA not const phi\n",
      "Iters time 0.0153000354767\n",
      "1 0.621940559441\n",
      "Iters time 0.034432888031\n",
      "2 0.622814685315\n",
      "Iters time 0.0490560531616\n",
      "3 0.615384615385\n",
      "Iters time 0.067636013031\n",
      "4 0.61756993007\n",
      "Iters time 0.0804588794708\n",
      "5 0.620192307692\n",
      "\n",
      "PLSA const phi\n",
      "Iters time 0.0172531604767\n",
      "1 0.621940559441\n",
      "Iters time 0.0339169502258\n",
      "2 0.622377622378\n",
      "Iters time 0.0523760318756\n",
      "3 0.620192307692\n",
      "Iters time 0.0636301040649\n",
      "4 0.616258741259\n",
      "Iters time 0.0816099643707\n",
      "5 0.615384615385\n",
      "\n",
      "ARTM thetaless\n",
      "Iters time 0.0220048427582\n",
      "1 0.621940559441\n",
      "Iters time 0.0384120941162\n",
      "2 0.623688811189\n",
      "Iters time 0.0598139762878\n",
      "3 0.621503496503\n",
      "Iters time 0.0781240463257\n",
      "4 0.618881118881\n",
      "Iters time 0.0938749313354\n",
      "5 0.620629370629\n"
     ]
    }
   ],
   "source": [
    "D, W = n_dw_matrix_doc_test.shape\n",
    "theta_matrix = np.ones(shape=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "print 'PLSA not const phi' \n",
    "for iters in xrange(1, 6):\n",
    "    _, theta_test = em_optimization(\n",
    "        n_dw_matrix=n_dw_matrix_doc_test, \n",
    "        phi_matrix=phi,\n",
    "        theta_matrix=theta_matrix,\n",
    "        regularization_list=regularization_list,\n",
    "        iters_count=iters,\n",
    "        loss_function=LogFunction(),\n",
    "    )\n",
    "    print iters, accuracy_score(algo.predict(theta_test), doc_targets_doc_test)\n",
    "\n",
    "print '\\nPLSA const phi' \n",
    "for iters in xrange(1, 6):\n",
    "    _, theta_test = em_optimization(\n",
    "        n_dw_matrix=n_dw_matrix_doc_test, \n",
    "        phi_matrix=phi,\n",
    "        theta_matrix=theta_matrix,\n",
    "        regularization_list=regularization_list,\n",
    "        iters_count=iters,\n",
    "        loss_function=LogFunction(),\n",
    "        const_phi=True\n",
    "    )\n",
    "    print iters, accuracy_score(algo.predict(theta_test), doc_targets_doc_test)\n",
    "    \n",
    "print '\\nARTM thetaless'    \n",
    "for iters in xrange(1, 6):\n",
    "    _, theta_test = artm_thetaless_em_optimization(\n",
    "        n_dw_matrix=n_dw_matrix_doc_test, \n",
    "        phi_matrix=phi,\n",
    "        regularization_list=regularization_list,\n",
    "        iters_count=iters,\n",
    "    )\n",
    "    print iters, accuracy_score(algo.predict(theta_test), doc_targets_doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
