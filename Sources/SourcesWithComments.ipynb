{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оптимальные питоновские реализация оптимизации ARTM\n",
    "\n",
    "# Оптимизация произвольной функции\n",
    "\n",
    "# Thetaless оптимизация\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.core.umath_tests import inner1d\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разные функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.log(x + 1e-20)\n",
    "    def calc_der(self, x):\n",
    "        return 1. / (x + 1e-20)\n",
    "    \n",
    "\n",
    "class IdFunction(object):\n",
    "    def calc(self, x):\n",
    "        return x + 1e-20\n",
    "    def calc_der(self, x):\n",
    "        return np.ones_like(x)\n",
    "    \n",
    "\n",
    "class SquareFunction(object):\n",
    "    def calc(self, x):\n",
    "        return (x + 1e-20) ** 2\n",
    "    def calc_der(self, x):\n",
    "        return 2. * (x + 1e-20) ** 2\n",
    "    \n",
    "\n",
    "class CubeLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.log(x + 1e-20) ** 3\n",
    "    def calc_der(self, x):\n",
    "        return 3. * np.log(x + 1e-20) ** 2 / (x + 1e-20)\n",
    "    \n",
    "\n",
    "class SquareLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.log(x + 1e-20) * np.abs(np.log(x + 1e-20))\n",
    "    def calc_der(self, x):\n",
    "        return 2. * np.abs(np.log(x + 1e-20)) / (x + 1e-20)\n",
    "\n",
    "    \n",
    "class FiveLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.log(x + 1e-20) ** 5\n",
    "    def calc_der(self, x):\n",
    "        return 5. * np.log(x + 1e-20) ** 4 / (x + 1e-20)\n",
    "    \n",
    "\n",
    "class CubeRootLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.cbrt(np.log(x + 1e-20))\n",
    "    def calc_der(self, x):\n",
    "        return 1. / 3 / (np.cbrt(np.log(x + 1e-20)) ** 2) / (x + 1e-20)\n",
    "    \n",
    "    \n",
    "class SquareRootLogFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.sqrt(- np.log(x + 1e-20))\n",
    "    def calc_der(self, x):\n",
    "        return 1. / 2. / np.sqrt(- np.log(x + 1e-20)) / (x + 1e-20)\n",
    "    \n",
    "\n",
    "class ExpFunction(object):\n",
    "    def calc(self, x):\n",
    "        return np.exp(x)\n",
    "    def calc_der(self, x):\n",
    "        return np.exp(x)\n",
    "\n",
    "    \n",
    "class EntropyFunction(object):\n",
    "    def calc(self, x):\n",
    "        return (np.log(x + 1e-20) + 50.) * (x + 1e-20)\n",
    "    def calc_der(self, x):\n",
    "        return np.log(x + 1e-20) + 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разные регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trivial_regularization(n_tw, n_dt):\n",
    "    return np.zeros_like(n_tw), np.zeros_like(n_dt)\n",
    "\n",
    "def create_reg_decorr(tau, theta_alpha=0.):\n",
    "    def fun(n_tw, n_dt):\n",
    "        phi_matrix = n_tw / np.sum(n_tw, axis=1)[:, np.newaxis]\n",
    "        theta_matrix = n_dt / np.sum(n_dt, axis=1)[:, np.newaxis]\n",
    "        aggr_phi = np.sum(phi_matrix, axis=1)\n",
    "        return - tau * np.transpose(phi_matrix * (aggr_phi[:, np.newaxis] - phi_matrix)), theta_alpha\n",
    "    return fun\n",
    "\n",
    "def create_reg_lda(phi_alpha, theta_alpha):\n",
    "    def fun (n_tw, n_dt):\n",
    "        return np.zeros_like(n_tw) + phi_alpha, np.zeros_like(n_dt) + theta_alpha\n",
    "    return fun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка Датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно скачать некоторые коллекции данных и установить библиотеки (nltk, gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tylorn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, calc_cooccurences=False, train_test_split=None, token_2_num=None):\n",
    "    is_token_2_num_provided = token_2_num is not None \n",
    "    # remove stopwords\n",
    "    if not is_token_2_num_provided:\n",
    "        token_2_num = {}\n",
    "        occurences = Counter()\n",
    "        for i, doc in enumerate(dataset.data):\n",
    "            tokens = gensim.utils.lemmatize(doc)\n",
    "            for token in set(tokens):\n",
    "                occurences[token] += 1\n",
    "            if i % 500 == 0:\n",
    "                print 'Processed: ', i, 'documents from', len(dataset.data)\n",
    "    \n",
    "    row, col, data = [], [], []\n",
    "    row_test, col_test, data_test = [], [], []\n",
    "    not_empty_docs_number = 0\n",
    "    doc_targets = []\n",
    "    doc_cooccurences = Counter()\n",
    "    doc_occurences = Counter()\n",
    "    random_gen = random.Random(42)\n",
    "    \n",
    "    for doc, target in zip(dataset.data, dataset.target):\n",
    "        tokens = gensim.utils.lemmatize(doc)\n",
    "        cnt = Counter()\n",
    "        cnt_test = Counter()\n",
    "        for token in tokens:\n",
    "            word = token.split('/')[0]\n",
    "            if not is_token_2_num_provided and word not in english_stopwords and 3 <= occurences[token] and token not in token_2_num:\n",
    "                token_2_num[token] = len(token_2_num)\n",
    "            if token in token_2_num:\n",
    "                if train_test_split is None or random_gen.random() < train_test_split:\n",
    "                    cnt[token_2_num[token]] += 1\n",
    "                else:\n",
    "                    cnt_test[token_2_num[token]] += 1\n",
    "        \n",
    "        if len(cnt) > 0 and (train_test_split is None or len(cnt_test) > 0):\n",
    "            for w, c in cnt.iteritems():\n",
    "                row.append(not_empty_docs_number)\n",
    "                col.append(w)\n",
    "                data.append(c)\n",
    "                \n",
    "            for w, c in cnt_test.iteritems():\n",
    "                row_test.append(not_empty_docs_number)\n",
    "                col_test.append(w)\n",
    "                data_test.append(c)\n",
    "                \n",
    "            not_empty_docs_number += 1\n",
    "            doc_targets.append(target)\n",
    "            \n",
    "            if calc_cooccurences:\n",
    "                words = set(cnt.keys() + cnt_test.keys())\n",
    "                doc_occurences.update(words)\n",
    "                doc_cooccurences.update({(w1, w2) for w1 in words for w2 in words if w1 != w2})\n",
    "        \n",
    "    num_2_token = {\n",
    "        v: k\n",
    "        for k, v in token_2_num.iteritems()\n",
    "    }\n",
    "    print 'Nonzero values:', len(data)\n",
    "    if train_test_split is None:\n",
    "        if calc_cooccurences:\n",
    "            return scipy.sparse.csr_matrix((data, (row, col))), token_2_num, num_2_token, doc_targets, doc_occurences, doc_cooccurences\n",
    "        else:\n",
    "            return scipy.sparse.csr_matrix((data, (row, col))), token_2_num, num_2_token, doc_targets\n",
    "    else:\n",
    "        if calc_cooccurences:\n",
    "            return (\n",
    "                scipy.sparse.csr_matrix((data, (row, col))),\n",
    "                scipy.sparse.csr_matrix((data_test, (row_test, col_test))),\n",
    "                token_2_num,\n",
    "                num_2_token,\n",
    "                doc_targets,\n",
    "                doc_occurences,\n",
    "                doc_cooccurences\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                scipy.sparse.csr_matrix((data, (row, col))),\n",
    "                scipy.sparse.csr_matrix((data_test, (row_test, col_test))),\n",
    "                token_2_num,\n",
    "                num_2_token,\n",
    "                doc_targets\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисление правдоподобных функций\n",
    "\n",
    "### имеется в виду вычисление функций вида $\\sum_{dw} n_{dw} f(\\sum_{t} \\phi_{wt} \\theta_{td})$\n",
    "\n",
    "##### Ключевой момент - использование функции inner1d. Она позволяет перемножить попарно строчки матриц, не сохраняя промежуточное состояние. А индексация в numpy не создаёт новый массив, делает view над ним. Таким образом, подсчёт $\\sum_{t} \\phi_{wt} \\theta_{td}$  делается максимально эффективным способом и по времени и по памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_calculate_likelihood_like_function(n_dw_matrix, loss_function=LogFunction()):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    docptr = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        docptr.extend([doc_num] * (indptr[doc_num + 1] - indptr[doc_num]))\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    \n",
    "    def fun(phi_matrix, theta_matrix):\n",
    "        s_data = loss_function.calc(inner1d(theta_matrix[docptr, :], np.transpose(phi_matrix)[wordptr, :]))\n",
    "        return np.sum(n_dw_matrix.data * s_data)\n",
    "\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM алгоритм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая схема:\n",
    "#### Неоходимо сначала вычислить $p_{tdw} = \\frac{\\phi_{wt} \\theta_{td}}{\\sum_s \\phi_{ws} \\theta_{sd}}$\n",
    "#### Считаем $n_{wt} = \\sum_d n_{dw} p_{tdw}$ и $n_{td} = \\sum_w n_{dw} p_{tdw}$\n",
    "#### Вычисляем $r_{wt}, r_{td}$ как функцию от $n_{wt}, n_{td}$\n",
    "#### Прибавляем, делаем положительную срезку и нормируем\n",
    "\n",
    "## Оптимизация вычисления:\n",
    "#### Обозначим за $s_{dw}$ следующее выражение $\\sum_t \\phi_{wt} \\theta_{td}$, фактически это наше предсказание для вероятности\n",
    "####  Тогда $p_{tdw} = \\frac{\\phi_{wt} \\theta_{td}}{s_{dw}}$\n",
    "#### Подставим это выражение например в $n_wt$\n",
    "#### И получим, что $n_{wt} = \\sum_d n_{dw} \\frac{\\phi_{wt} \\theta_{td}}{s_{dw}} = \\phi_{wt} \\sum_d \\theta_{td} \\cdot \\frac{n_{dw}}{s_{dw}}$, аналогично $n_{td} = \\theta_{td} \\sum_w \\phi_{wt} \\cdot \\frac{n_{dw}}{s_{dw}}$\n",
    "#### Таким образом, мы видим, что фактически нам нужно знать матрицу $\\frac{n_{dw}}{s_{dw}}$, а она очень разреженная, поэтому и $s_{dw}$ нужно не для всех пар вычислять, а только там, где $n_{dw} > 0$. \n",
    "#### То есть нам нужно эффективно закодить вычисление разженной матрицы $s_{dw}$ (матрица $n_{dw}$ уже есть в разреженном виде, так как подаётся на вход алгоритма), а затем просто поэлементно поделить\n",
    "#### Причём хочется, чтобы промежуточные значения $p_{tdw}$ не сохранялись (как мы увидели, они в конечном варианте не важны)\n",
    "#### Обозначим эту матрицу за $A$. Тогда $n_{wt} = \\phi_{wt} (\\Theta A)_{tw}$, а $n_{td} = \\theta_{td} (A \\Phi^T)_{dt}$.\n",
    "#### Перемножить разреженную матрицу на плотную можно быстро, если правильно её хранить (по строкам, или по столбцам)\n",
    "#### Если оптимизируется не правдоподобие, какая-то другая функция вида $\\sum_{dw} n_{dw} f(s_{dw})$ (правдоподобие будет, если $f(x) = \\ln x$ ) , то в этом случае нужно определить матрицу $A$ как $A_{dw} = n_{dw} f'(s_{dw})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def em_optimization(\n",
    "    n_dw_matrix, \n",
    "    phi_matrix,\n",
    "    theta_matrix,\n",
    "    regularization_list,\n",
    "    iters_count=100,\n",
    "    loss_function=LogFunction(),\n",
    "    iteration_callback=None,\n",
    "    const_phi=False\n",
    "):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    T = phi_matrix.shape[0]\n",
    "    phi_matrix = np.copy(phi_matrix)\n",
    "    theta_matrix = np.copy(theta_matrix)\n",
    "    docptr = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        docptr.extend([doc_num] * (indptr[doc_num + 1] - indptr[doc_num]))\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for it in xrange(iters_count):\n",
    "        phi_matrix_tr = np.transpose(phi_matrix)\n",
    "        # следующая строчка это 60% времени работы алгоритма\n",
    "        s_data = loss_function.calc_der(inner1d(theta_matrix[docptr, :], phi_matrix_tr[wordptr, :]))\n",
    "        # следующая часть это 25% времени работы алгоритма\n",
    "        A = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                n_dw_matrix.data * s_data, \n",
    "                n_dw_matrix.indices, \n",
    "                n_dw_matrix.indptr\n",
    "            ), \n",
    "            shape=n_dw_matrix.shape\n",
    "        )\n",
    "        A_tr = A.tocsc().transpose()\n",
    "        # Остальное это 15% времени\n",
    "        n_tw = np.transpose(A_tr.dot(theta_matrix)) * phi_matrix\n",
    "        n_dt = A.dot(phi_matrix_tr) * theta_matrix\n",
    "        \n",
    "        r_tw, r_dt = regularization_list[it](n_tw, n_dt)\n",
    "        n_tw += r_tw\n",
    "        n_dt += n_dt\n",
    "        n_tw[n_tw < 0] = 0\n",
    "        n_dt[n_dt < 0] = 0\n",
    "        \n",
    "        if not const_phi:\n",
    "            phi_matrix = n_tw / np.sum(n_tw, axis=1)[:, np.newaxis]\n",
    "        theta_matrix = n_dt / np.sum(n_dt, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        if iteration_callback is not None:\n",
    "            iteration_callback(it, phi_matrix, theta_matrix)\n",
    "    \n",
    "    print 'Iters time', time.time() - start_time\n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive thetaless EM\n",
    "\n",
    "\n",
    "### Основная идея: давайте вообще не хранить $\\Theta$, а вместо этого вычислять её на лету одной итерацией ЕМ алгоритма, которую можно легко выписать.\n",
    "\n",
    "##### Пусть тематический профиль документа инициализирован равномерно, то для этого документа $p_{tdw} = \\frac{\\phi_{wt}}{\\sum_s \\phi_s} \\equiv \\overline{\\phi}_{wt} \\equiv (\\overline{\\Phi})_{wt} \\equiv p(t~|~w)$ . Эту матрицу легко рассчитать.\n",
    "##### На первой итерации  будет подсчитано $n_{td} = \\sum_{d} n_{dw} p_{tdw} = \\sum_{d} n_{dw} (\\overline{\\Phi})_{wt} = (N\\overline{\\Phi})_{dt}$\n",
    "##### И, соответственно, $\\theta_{td} = \\frac{n_{td}}{\\sum_t n_{td}} =  \\frac{n_{td}}{n_d}$\n",
    "##### Введём матрицу $B_{dw} \\equiv \\frac{n_{dw}}{n_d}$, тогда $\\Theta = B \\overline{\\Phi}$ \n",
    "##### Идеологически, мы зафиксировали, что $\\Theta$ - детерминированная функция от $\\Phi$. И теперь оптимизируем не $L(\\Phi, \\Theta)$, а $\\overline{L}(\\Phi) = L(\\Phi, B \\overline{\\Phi})$\n",
    "##### Наивность решения состоит в том, что мы полностью игнорируем любые действия с $\\Theta$ на М шаге. То есть мы не считаем $n_{td}$ и не обновляем $\\theta_{td}$ (этой матрицы вообще нет). А с $n_{wt}$ мы поступаем также как на обычном М шаге. Регуляризаторы на $\\Phi$ обрабатываются точно также как и раньше ($r_{wt}$ прибавляется к $n_{wt}$), а регуляризаторы $\\Theta$ игнорируются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_thetaless_em_optimization(\n",
    "    n_dw_matrix, \n",
    "    phi_matrix,\n",
    "    regularization_list,\n",
    "    iters_count=100,\n",
    "    iteration_callback=None\n",
    "):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    T = phi_matrix.shape[0]\n",
    "    phi_matrix = np.copy(phi_matrix)\n",
    "    docptr = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        docptr.extend([doc_num] * (indptr[doc_num + 1] - indptr[doc_num]))\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for it in xrange(iters_count):\n",
    "        phi_rev_matrix = np.transpose(phi_matrix / np.sum(phi_matrix, axis=0))\n",
    "        theta_matrix = n_dw_matrix.dot(phi_rev_matrix)\n",
    "        theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "        phi_matrix_tr = np.transpose(phi_matrix)\n",
    "        \n",
    "        s_data = 1. / inner1d(theta_matrix[docptr, :], phi_matrix_tr[wordptr, :])\n",
    "        A = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                n_dw_matrix.data  * s_data , \n",
    "                n_dw_matrix.indices, \n",
    "                n_dw_matrix.indptr\n",
    "            ), \n",
    "            shape=n_dw_matrix.shape\n",
    "        ).tocsc()\n",
    "            \n",
    "        n_tw = (A.T.dot(theta_matrix)).T * phi_matrix\n",
    "        r_tw, _ = regularization_list[it](n_tw, theta_matrix)\n",
    "        n_tw += r_tw\n",
    "        n_tw[n_tw < 0] = 0\n",
    "        phi_matrix = n_tw / np.sum(n_tw, axis=1)[:, np.newaxis]\n",
    "\n",
    "        if iteration_callback is not None:\n",
    "            iteration_callback(it, phi_matrix, theta_matrix)\n",
    "    \n",
    "    print 'Iters time', time.time() - start_time    \n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARTM thetaless EM optimization\n",
    "\n",
    "##### Данное решение исправляет \"наивность\" предыдущего подхода, учитывая зависимость от $\\Theta$. Фактически, это можно написать в виде регуляризатора. Чтобы понять, как именно это сделать, вспомним как работает EM алгоритм.\n",
    "\n",
    "##### На каждой итерации сначала определяются $p_{tdw}$, фиксируются, а затем строится функционал нижней оценки: $Q(\\Phi, \\Theta) = \\sum_{dtw} n_{dw} p_{tdw} \\left( \\ln \\phi_{wt} + \\ln \\theta_{td}\\right) + R(\\Phi, \\Theta)$. Цель М-шага увеличить значение данного функционала по сравнению с $\\Phi$ и $\\Theta$ с предыдущей итерации.\n",
    "\n",
    "##### Несмотря на то, что теперь $\\Theta$ это функция от $\\Phi$, тот факт, что это всё ещё нижняя оценка, никуда не пропадает. Поэтому теперь наша цель подобрать $\\Phi$, чтобы увеличить значение по сравнению с $\\Phi$ с предыдущей итерации следующий функционал: \n",
    "\n",
    "$\\sum_{dtw} n_{dw} p_{tdw} \\left( \\ln \\phi_{wt} + \\ln (\\Theta(\\Phi))_{dt}\\right) + R(\\Phi, \\Theta(\\Phi))$.\n",
    "\n",
    "##### Возьмём производные как обычно\n",
    "\n",
    "##### $\\frac{\\partial{Q}}{\\partial{\\phi_{vr}}} = \\frac{1}{\\phi_{vr}} \\left( \\sum_{d} n_{dv} p_{rdv} + \\phi_{vr} \\frac{\\partial{R}}{\\partial{\\phi_{vr}}} + \\sum_{dtw} n_{dw} p_{tdw} \\frac{1}{\\theta_{td}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} +  \\sum_{dt} \\frac{\\partial{R}}{\\partial{\\theta_{td}}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} \\right)$\n",
    "\n",
    "##### Так как $p_{tdw} = \\frac{\\phi_{wt} \\theta_{td}}{\\sum_s \\phi_{ws} \\theta_{sd}}$, то третье слагаемое можно упростить\n",
    "\n",
    "##### $\\sum_{dtw} n_{dw} p_{tdw} \\frac{1}{\\theta_{td}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} = \\sum_{dtw} n_{dw}\\frac{\\phi_{wt}}{\\sum_s \\phi_{ws} \\theta_{sd}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} = \\sum_{dtw} A_{dw} \\phi_{wt} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}}$\n",
    "\n",
    "##### Итого\n",
    "\n",
    "##### $\\frac{\\partial{Q}}{\\partial{\\phi_{vr}}} = \\frac{1}{\\phi_{vr}} \\left( \\sum_{d} n_{dv} p_{rdv} + \\phi_{vr}\\left( \\frac{\\partial{R}}{\\partial{\\phi_{vr}}} + \\sum_{dtw} A_{dw} \\phi_{wt} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} +  \\sum_{dt} \\frac{\\partial{R}}{\\partial{\\theta_{td}}} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} \\right) \\right)$\n",
    "\n",
    "##### Обозначим за $C = A \\Phi^T + \\frac{\\partial{R}}{\\partial{\\Theta}}$, тогда\n",
    "$\\frac{\\partial{Q}}{\\partial{\\phi_{vr}}} = \\frac{1}{\\phi_{vr}} \\left( \\sum_{d} n_{dv} p_{rdv} + \\phi_{vr}\\left( \\frac{\\partial{R}}{\\partial{\\phi_{vr}}} + \\sum_{dt} C_{dt} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} \\right) \\right)$\n",
    "\n",
    "##### Как видим, получившийся остаток фактически и есть требуемый регуяризатор на $\\Phi$. Осталось только найти $\\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}}$\n",
    "\n",
    "##### $\\theta_{td} = \\sum_{w} B_{dw} \\frac{\\phi_{wt}}{\\sum_s \\phi_{ws}}$. Обозначим $\\frac{1}{\\sum_s \\phi_{ws}}$ за $norm_w$, тогда\n",
    "\n",
    "$\\theta_{td} = \\sum_{w} B_{dw} \\phi_{wt} norm_w$\n",
    "\n",
    "$\\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} =  \\sum_{w} B_{dw}~norm_w \\delta_{vwrt} +  \\sum_{w} B_{dw} \\phi_{wt} \\frac{\\partial{norm_w}}{\\partial{\\phi_{vr}}} = \n",
    "\\sum_{w} B_{dw} norm_w \\delta_{vwrt} - \\sum_{w} B_{dw}~\\phi_{wt}~norm_w^2~\\delta_{vw} =\n",
    "B_{dv}~norm_v~\\delta_{rt} - B_{dv}~\\phi_{vt}~norm_w^2\n",
    "$\n",
    "\n",
    "##### Тут $\\delta$ это символ Кронекера\n",
    "\n",
    "##### Теперь\n",
    "\n",
    "$\\sum_{dt} C_{dt} \\frac{\\partial{\\theta_{td}}}{\\partial{\\phi_{vr}}} = \\sum_{dt} C_{dt} \\left( B_{dv}~norm_v~\\delta_{rt} - B_{dv}~\\phi_{vt}~norm_v^2 \\right) =  norm_v~\\sum_d C_{dr} B_{dv} -  norm_v^2~\\sum_{dt} C_{dt} B_{dv} \\phi_{vt} = norm_v (C^T B)_{rv} - norm_v^2 (\\Phi^T C^T B)_{vv}$\n",
    "\n",
    "##### В numpy можно вычислить только диагональ при помощи einsum, поэтому эту регуляризационную добавку можно эффективно вычислить\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artm_thetaless_em_optimization(\n",
    "    n_dw_matrix, \n",
    "    phi_matrix,\n",
    "    regularization_list,\n",
    "    iters_count=100,\n",
    "    iteration_callback=None\n",
    "):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    T = phi_matrix.shape[0]\n",
    "    phi_matrix = np.copy(phi_matrix)\n",
    "    docptr = []\n",
    "    docsizes = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        size = indptr[doc_num + 1] - indptr[doc_num]\n",
    "        docptr.extend([doc_num] * size)\n",
    "        docsizes.extend([size] * size)\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    docsizes = np.array(docsizes)\n",
    "    \n",
    "    B = scipy.sparse.csr_matrix(\n",
    "        (\n",
    "            1. * n_dw_matrix.data  / docsizes, \n",
    "            n_dw_matrix.indices, \n",
    "            n_dw_matrix.indptr\n",
    "        ), \n",
    "        shape=n_dw_matrix.shape\n",
    "    ).tocsc()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for it in xrange(iters_count):\n",
    "        word_norm = np.sum(phi_matrix, axis=0)\n",
    "        word_norm[word_norm == 0] = 1e-20\n",
    "        phi_rev_matrix = np.transpose(phi_matrix / word_norm)\n",
    "        \n",
    "        theta_matrix = n_dw_matrix.dot(phi_rev_matrix)\n",
    "        theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "        phi_matrix_tr = np.transpose(phi_matrix)\n",
    "        \n",
    "        s_data = 1. / inner1d(theta_matrix[docptr, :], phi_matrix_tr[wordptr, :])\n",
    "        A = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                n_dw_matrix.data  * s_data , \n",
    "                n_dw_matrix.indices, \n",
    "                n_dw_matrix.indptr\n",
    "            ), \n",
    "            shape=n_dw_matrix.shape\n",
    "        ).tocsc()\n",
    "            \n",
    "        n_tw = A.T.dot(theta_matrix).T * phi_matrix\n",
    "        \n",
    "        r_tw, r_dt = regularization_list[it](n_tw, theta_matrix)\n",
    "        theta_indices = theta_matrix > 0\n",
    "        r_dt[theta_indices] /= theta_matrix[theta_indices]\n",
    "        \n",
    "        g_dt = A.dot(phi_matrix_tr) + r_dt\n",
    "        tmp = g_dt.T * B / word_norm\n",
    "        r_tw += (tmp - np.einsum('ij,ji->i', phi_rev_matrix, tmp)) * phi_matrix\n",
    "        \n",
    "        n_tw += r_tw\n",
    "        n_tw[n_tw < 0] = 0\n",
    "        phi_matrix = n_tw / np.sum(n_tw, axis=1)[:, np.newaxis]\n",
    "        phi_matrix[np.isnan(phi_matrix)] = 0.\n",
    "\n",
    "        if iteration_callback is not None:\n",
    "            iteration_callback(it, phi_matrix, theta_matrix)\n",
    "    \n",
    "    print 'Iters time', time.time() - start_time    \n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient Descent\n",
    "\n",
    "##### Мы можем найти градиент оптимизируемой функции и сделать шаг вдоль него. Сделано для сравнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_optimization(\n",
    "    n_dw_matrix, \n",
    "    phi_matrix,\n",
    "    theta_matrix,\n",
    "    regularization_gradient_list,\n",
    "    iters_count=100,\n",
    "    loss_function=LogFunction(),\n",
    "    iteration_callback=None,\n",
    "    learning_rate=1.\n",
    "):\n",
    "    D, W = n_dw_matrix.shape\n",
    "    T = phi_matrix.shape[0]\n",
    "    phi_matrix = np.copy(phi_matrix)\n",
    "    theta_matrix = np.copy(theta_matrix)\n",
    "    docptr = []\n",
    "    indptr = n_dw_matrix.indptr\n",
    "    for doc_num in xrange(D):\n",
    "        docptr.extend([doc_num] * (indptr[doc_num + 1] - indptr[doc_num]))\n",
    "    docptr = np.array(docptr)\n",
    "    wordptr = n_dw_matrix.indices\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for it in xrange(iters_count):\n",
    "        phi_matrix_tr = np.transpose(phi_matrix)\n",
    "        # следующая строчка это 60% времени работы алгоритма\n",
    "        s_data = loss_function.calc_der(inner1d(theta_matrix[docptr, :], phi_matrix_tr[wordptr, :]))\n",
    "        # следующая часть это 25% времени работы алгоритма\n",
    "        A = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                n_dw_matrix.data * s_data, \n",
    "                n_dw_matrix.indices, \n",
    "                n_dw_matrix.indptr\n",
    "            ), \n",
    "            shape=n_dw_matrix.shape\n",
    "        ).tocsc()\n",
    "        # Остальное это 15% времени\n",
    "        g_tw = theta_matrix.T * A\n",
    "        g_dt = A.dot(phi_matrix_tr)\n",
    "        \n",
    "        r_tw, r_dt = regularization_gradient_list[it](phi_matrix, theta_matrix)\n",
    "        g_tw += r_tw\n",
    "        g_dt += r_dt\n",
    "        \n",
    "        g_tw -= np.sum(g_tw * phi_matrix, axis=1)[:, np.newaxis]\n",
    "        g_dt -= np.sum(g_dt * theta_matrix, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        phi_matrix += g_tw * learning_rate\n",
    "        theta_matrix += g_dt * learning_rate\n",
    "        \n",
    "        phi_matrix[phi_matrix < 0] = 0\n",
    "        theta_matrix[theta_matrix < 0] = 0\n",
    "        \n",
    "        phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "        theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        if iteration_callback is not None:\n",
    "            iteration_callback(it, phi_matrix, theta_matrix)\n",
    "    \n",
    "    print 'Iters time', time.time() - start_time  \n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Оценка качества классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_score(theta, targets):\n",
    "    C_2d_range = [1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
    "    gamma_2d_range = [1e-3, 1e-2, 1e-1, 1, 1e1]\n",
    "    best_C, best_gamma, best_val = None, None, 0.\n",
    "    best_cv_algo_score_on_test = 0.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(theta, targets, test_size=0.30, stratify=targets, random_state=42)\n",
    "    for C in C_2d_range:\n",
    "        for gamma in gamma_2d_range:\n",
    "            val = np.mean(cross_val_score(SVC(C=C, gamma=gamma), X_train, y_train, scoring='accuracy', cv=4))\n",
    "            algo = SVC(C=C, gamma=gamma).fit(X_train, y_train)\n",
    "            test_score = accuracy_score(y_test, algo.predict(X_test))\n",
    "            print 'SVM(C={}, gamma={}) cv-score: {}  test-score: {}'.format(\n",
    "                C,\n",
    "                gamma,\n",
    "                round(val, 3),\n",
    "                round(test_score, 3)\n",
    "            )\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_C = C\n",
    "                best_gamma = gamma\n",
    "                best_cv_algo_score_on_test = test_score\n",
    "    print '\\n\\n\\nBest cv params: C={}, gamma={}\\nCV score: {}\\nTest score:{}'.format(\n",
    "        best_C,\n",
    "        best_gamma,\n",
    "        round(best_val, 3),\n",
    "        round(best_cv_algo_score_on_test, 3)\n",
    "    )\n",
    "    return best_C, best_gamma, best_val, best_cv_algo_score_on_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artm_calc_topic_correlation(phi):\n",
    "    T, W = phi.shape\n",
    "    return (np.sum(np.sum(phi, axis=0) ** 2) - np.sum(phi ** 2)) / (T * (T - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artm_calc_perplexity_factory(n_dw_matrix):\n",
    "    helper = create_calculate_likelihood_like_function(\n",
    "        loss_function=LogFunction(),\n",
    "        n_dw_matrix=n_dw_matrix\n",
    "    )\n",
    "    total_words_number = n_dw_matrix.sum()\n",
    "    return lambda phi, theta: np.exp(- helper(phi, theta) / total_words_number)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def artm_calc_pmi_top_factory(doc_occurences, doc_cooccurences, documents_number, top_size):\n",
    "    def fun(phi):\n",
    "        T, W = phi.shape\n",
    "        pmi = 0.\n",
    "        for t in xrange(T):\n",
    "            top = heapq.nlargest(top_size, xrange(W), key=lambda w: phi[t, w])\n",
    "            for w1 in top:\n",
    "                for w2 in top:\n",
    "                    if w1 != w2:\n",
    "                        pmi += np.log(documents_number * (doc_cooccurences[(w1, w2)] + 0.1) * 1. / doc_occurences[w1] / doc_occurences[w2])\n",
    "        return pmi / (T * top_size * (top_size - 1))\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры запусков с разделением на train и test по словам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    categories=['sci.electronics', 'sci.med', 'sci.space', 'sci.crypt', 'rec.sport.baseball', 'rec.sport.hockey'],\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0 documents from 5945\n",
      "Processed:  500 documents from 5945\n",
      "Processed:  1000 documents from 5945\n",
      "Processed:  1500 documents from 5945\n",
      "Processed:  2000 documents from 5945\n",
      "Processed:  2500 documents from 5945\n",
      "Processed:  3000 documents from 5945\n",
      "Processed:  3500 documents from 5945\n",
      "Processed:  4000 documents from 5945\n",
      "Processed:  4500 documents from 5945\n",
      "Processed:  5000 documents from 5945\n",
      "Processed:  5500 documents from 5945\n",
      "Nonzero values: 268395\n",
      "CPU times: user 6min 35s, sys: 3.86 s, total: 6min 39s\n",
      "Wall time: 6min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_n_dw_matrix, test_n_dw_matrix, ttt_token_2_num, ttt_num_2_token, ttt_doc_targets, ttt_doc_occurences, ttt_doc_cooccurences = prepare_dataset(dataset, calc_cooccurences=True, train_test_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLSA: EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\tTrain perplexity:   4011.17484299\n",
      "\tTest perplexity:     4329.28352139\n",
      "\tTopics correlation  0.000681707612939\n",
      "\tPhi sparsity        0.00187708565072\n",
      "\tTheta sparsity      0.000289435600579\n",
      "\tTop5 PMI            0.394723437323\n",
      "\tTop10 PMI           0.427239393998\n",
      "\tTop20 PMI           0.459551106816\n",
      "10\n",
      "\tTrain perplexity:   2340.05141012\n",
      "\tTest perplexity:     3079.73275105\n",
      "\tTopics correlation  0.000600784772598\n",
      "\tPhi sparsity        0.0111651835373\n",
      "\tTheta sparsity      0.440726000965\n",
      "\tTop5 PMI            0.957654624513\n",
      "\tTop10 PMI           0.854134998668\n",
      "\tTop20 PMI           0.863416920255\n",
      "20\n",
      "\tTrain perplexity:   1836.96293559\n",
      "\tTest perplexity:     2883.9455118\n",
      "\tTopics correlation  0.000541222836568\n",
      "\tPhi sparsity        0.277896737115\n",
      "\tTheta sparsity      0.669138929088\n",
      "\tTop5 PMI            1.03593318118\n",
      "\tTop10 PMI           1.14284664569\n",
      "\tTop20 PMI           1.07933703324\n",
      "30\n",
      "\tTrain perplexity:   1760.66489449\n",
      "\tTest perplexity:     3142.46101117\n",
      "\tTopics correlation  0.000531821989124\n",
      "\tPhi sparsity        0.509445680386\n",
      "\tTheta sparsity      0.718174143753\n",
      "\tTop5 PMI            1.05855764286\n",
      "\tTop10 PMI           1.23244852414\n",
      "\tTop20 PMI           1.0993439079\n",
      "40\n",
      "\tTrain perplexity:   1732.34985939\n",
      "\tTest perplexity:     3493.82875175\n",
      "\tTopics correlation  0.000527374535086\n",
      "\tPhi sparsity        0.601849276974\n",
      "\tTheta sparsity      0.739315002412\n",
      "\tTop5 PMI            1.11619847814\n",
      "\tTop10 PMI           1.23440971207\n",
      "\tTop20 PMI           1.14518742092\n",
      "50\n",
      "\tTrain perplexity:   1715.38841733\n",
      "\tTest perplexity:     3855.33686805\n",
      "\tTopics correlation  0.000524616794763\n",
      "\tPhi sparsity        0.648651279199\n",
      "\tTheta sparsity      0.749939700917\n",
      "\tTop5 PMI            1.12922811719\n",
      "\tTop10 PMI           1.24744760269\n",
      "\tTop20 PMI           1.14863822444\n",
      "60\n",
      "\tTrain perplexity:   1703.39350443\n",
      "\tTest perplexity:     4214.33709189\n",
      "\tTopics correlation  0.000522763380933\n",
      "\tPhi sparsity        0.676501668521\n",
      "\tTheta sparsity      0.756825856247\n",
      "\tTop5 PMI            1.14343675802\n",
      "\tTop10 PMI           1.25453566296\n",
      "\tTop20 PMI           1.17170748133\n",
      "70\n",
      "\tTrain perplexity:   1694.25605783\n",
      "\tTest perplexity:     4559.35859174\n",
      "\tTopics correlation  0.000520899651736\n",
      "\tPhi sparsity        0.695221542455\n",
      "\tTheta sparsity      0.761336227689\n",
      "\tTop5 PMI            1.13328048509\n",
      "\tTop10 PMI           1.25815692477\n",
      "\tTop20 PMI           1.19857928535\n",
      "80\n",
      "\tTrain perplexity:   1686.86289484\n",
      "\tTest perplexity:     4890.39060053\n",
      "\tTopics correlation  0.00051931436601\n",
      "\tPhi sparsity        0.707883759733\n",
      "\tTheta sparsity      0.76486975398\n",
      "\tTop5 PMI            1.12160711489\n",
      "\tTop10 PMI           1.25855587312\n",
      "\tTop20 PMI           1.20843856714\n",
      "90\n",
      "\tTrain perplexity:   1681.2031888\n",
      "\tTest perplexity:     5194.63178608\n",
      "\tTopics correlation  0.000517864108881\n",
      "\tPhi sparsity        0.717491657397\n",
      "\tTheta sparsity      0.767788229619\n",
      "\tTop5 PMI            1.096778269\n",
      "\tTop10 PMI           1.25777424766\n",
      "\tTop20 PMI           1.20935336685\n",
      "100\n",
      "\tTrain perplexity:   1676.41380155\n",
      "\tTest perplexity:     5456.24772783\n",
      "\tTopics correlation  0.000516587841536\n",
      "\tPhi sparsity        0.72485168706\n",
      "\tTheta sparsity      0.77018813314\n",
      "\tTop5 PMI            1.11267659799\n",
      "\tTop10 PMI           1.27421283858\n",
      "\tTop20 PMI           1.21475504581\n",
      "Iters time 10.1115128994\n"
     ]
    }
   ],
   "source": [
    "D, W = train_n_dw_matrix.shape\n",
    "T = 15\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.ones(shape=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = create_reg_lda(0., 0.)\n",
    "\n",
    "_train_perplexity = artm_calc_perplexity_factory(train_n_dw_matrix) \n",
    "_test_perplexity = artm_calc_perplexity_factory(test_n_dw_matrix)\n",
    "_top5_pmi = artm_calc_pmi_top_factory(ttt_doc_occurences, ttt_doc_cooccurences, train_n_dw_matrix.shape[0], 5)\n",
    "_top10_pmi = artm_calc_pmi_top_factory(ttt_doc_occurences, ttt_doc_cooccurences, train_n_dw_matrix.shape[0], 10)\n",
    "_top20_pmi = artm_calc_pmi_top_factory(ttt_doc_occurences, ttt_doc_cooccurences, train_n_dw_matrix.shape[0], 20)\n",
    "\n",
    "def callback(it, phi, theta):\n",
    "    if it % 10 == 0:\n",
    "        print 'Iteration', it\n",
    "        print '\\tTrain perplexity:   {}'.format(_train_perplexity(phi, theta))\n",
    "        print '\\tTest perplexity:     {}'.format(_test_perplexity(phi, theta))\n",
    "        print '\\tTopics correlation  {}'.format(artm_calc_topic_correlation(phi))\n",
    "        print '\\tPhi sparsity        {}'.format(1. * np.sum(phi < 1e-20) / np.sum(phi >= 0))\n",
    "        print '\\tTheta sparsity      {}'.format(1. * np.sum(theta < 0.01) / np.sum(theta >= 0))\n",
    "        print '\\tTop5 PMI            {}'.format(_top5_pmi(phi))\n",
    "        print '\\tTop10 PMI           {}'.format(_top10_pmi(phi))\n",
    "        print '\\tTop20 PMI           {}'.format(_top20_pmi(phi))\n",
    "\n",
    "\n",
    "phi, theta = em_optimization(\n",
    "    n_dw_matrix=train_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    theta_matrix=theta_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=101,\n",
    "    loss_function=LogFunction(),\n",
    "    iteration_callback=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLSA: Thetaless EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "\tTrain perplexity:   4003.33075262\n",
      "\tTest perplexity:     4323.39836247\n",
      "\tTopics correlation  0.000679240512961\n",
      "\tPhi sparsity        0.00232665925102\n",
      "\tTheta sparsity      0.000289435600579\n",
      "\tTop5 PMI            0.394723437323\n",
      "\tTop10 PMI           0.411234624191\n",
      "\tTop20 PMI           0.457036877687\n",
      "Iteration 10\n",
      "\tTrain perplexity:   2233.34167565\n",
      "\tTest perplexity:     2748.03255718\n",
      "\tTopics correlation  0.000403814425786\n",
      "\tPhi sparsity        0.572209862811\n",
      "\tTheta sparsity      0.155294259527\n",
      "\tTop5 PMI            1.13223344737\n",
      "\tTop10 PMI           1.22002365649\n",
      "\tTop20 PMI           1.2354214603\n",
      "Iteration 20\n",
      "\tTrain perplexity:   2058.42121552\n",
      "\tTest perplexity:     2720.73503461\n",
      "\tTopics correlation  0.000208348753981\n",
      "\tPhi sparsity        0.634719132369\n",
      "\tTheta sparsity      0.266847563917\n",
      "\tTop5 PMI            1.21482546843\n",
      "\tTop10 PMI           1.30021602832\n",
      "\tTop20 PMI           1.32177865922\n",
      "Iteration 30\n",
      "\tTrain perplexity:   2021.74559798\n",
      "\tTest perplexity:     2774.14889056\n",
      "\tTopics correlation  0.000146451543949\n",
      "\tPhi sparsity        0.684950871339\n",
      "\tTheta sparsity      0.308731307284\n",
      "\tTop5 PMI            1.21305733911\n",
      "\tTop10 PMI           1.33325900943\n",
      "\tTop20 PMI           1.34409499199\n",
      "Iteration 40\n",
      "\tTrain perplexity:   2006.2892997\n",
      "\tTest perplexity:     2832.80803688\n",
      "\tTopics correlation  0.000113223038088\n",
      "\tPhi sparsity        0.728295328142\n",
      "\tTheta sparsity      0.332187650748\n",
      "\tTop5 PMI            1.21842817735\n",
      "\tTop10 PMI           1.34285122367\n",
      "\tTop20 PMI           1.36630324375\n",
      "Iteration 50\n",
      "\tTrain perplexity:   1997.55170451\n",
      "\tTest perplexity:     2886.73089389\n",
      "\tTopics correlation  9.89016885381e-05\n",
      "\tPhi sparsity        0.759380793474\n",
      "\tTheta sparsity      0.344259527255\n",
      "\tTop5 PMI            1.24294407867\n",
      "\tTop10 PMI           1.36292159999\n",
      "\tTop20 PMI           1.37774442008\n",
      "Iteration 60\n",
      "\tTrain perplexity:   1992.8612336\n",
      "\tTest perplexity:     2932.80762023\n",
      "\tTopics correlation  9.07651561917e-05\n",
      "\tPhi sparsity        0.782610307749\n",
      "\tTheta sparsity      0.352013989387\n",
      "\tTop5 PMI            1.25951991555\n",
      "\tTop10 PMI           1.36590219363\n",
      "\tTop20 PMI           1.37906030218\n",
      "Iteration 70\n",
      "\tTrain perplexity:   1989.53413917\n",
      "\tTest perplexity:     2972.77669101\n",
      "\tTopics correlation  8.37414883865e-05\n",
      "\tPhi sparsity        0.799814608825\n",
      "\tTheta sparsity      0.359563434636\n",
      "\tTop5 PMI            1.27263468524\n",
      "\tTop10 PMI           1.3776389622\n",
      "\tTop20 PMI           1.40258768433\n",
      "Iteration 80\n",
      "\tTrain perplexity:   1986.27957688\n",
      "\tTest perplexity:     3012.62444965\n",
      "\tTopics correlation  8.04114100058e-05\n",
      "\tPhi sparsity        0.813269373378\n",
      "\tTheta sparsity      0.366172214182\n",
      "\tTop5 PMI            1.30033856183\n",
      "\tTop10 PMI           1.39882134574\n",
      "\tTop20 PMI           1.40326335465\n",
      "Iteration 90\n",
      "\tTrain perplexity:   1984.30495583\n",
      "\tTest perplexity:     3045.25425199\n",
      "\tTopics correlation  7.83827400931e-05\n",
      "\tPhi sparsity        0.823475157582\n",
      "\tTheta sparsity      0.369802219006\n",
      "\tTop5 PMI            1.29667044151\n",
      "\tTop10 PMI           1.39579983832\n",
      "\tTop20 PMI           1.43309507299\n",
      "Iteration 100\n",
      "\tTrain perplexity:   1982.60380165\n",
      "\tTest perplexity:     3075.84449235\n",
      "\tTopics correlation  7.62956651372e-05\n",
      "\tPhi sparsity        0.831743604004\n",
      "\tTheta sparsity      0.373311625663\n",
      "\tTop5 PMI            1.29062422113\n",
      "\tTop10 PMI           1.40449898063\n",
      "\tTop20 PMI           1.41447311502\n",
      "Iters time 11.1589660645\n"
     ]
    }
   ],
   "source": [
    "D, W = train_n_dw_matrix.shape\n",
    "T = 15\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.ones(shape=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = create_reg_lda(0., 0.)\n",
    "\n",
    "_train_perplexity = artm_calc_perplexity_factory(train_n_dw_matrix) \n",
    "_test_perplexity = artm_calc_perplexity_factory(test_n_dw_matrix)\n",
    "_top5_pmi = artm_calc_pmi_top_factory(ttt_doc_occurences, ttt_doc_cooccurences, train_n_dw_matrix.shape[0], 5)\n",
    "_top10_pmi = artm_calc_pmi_top_factory(ttt_doc_occurences, ttt_doc_cooccurences, train_n_dw_matrix.shape[0], 10)\n",
    "_top20_pmi = artm_calc_pmi_top_factory(ttt_doc_occurences, ttt_doc_cooccurences, train_n_dw_matrix.shape[0], 20)\n",
    "\n",
    "def callback(it, phi, theta):\n",
    "    if it % 10 == 0:\n",
    "        print 'Iteration', it\n",
    "        print '\\tTrain perplexity:   {}'.format(_train_perplexity(phi, theta))\n",
    "        print '\\tTest perplexity:     {}'.format(_test_perplexity(phi, theta))\n",
    "        print '\\tTopics correlation  {}'.format(artm_calc_topic_correlation(phi))\n",
    "        print '\\tPhi sparsity        {}'.format(1. * np.sum(phi < 1e-20) / np.sum(phi >= 0))\n",
    "        print '\\tTheta sparsity      {}'.format(1. * np.sum(theta < 0.01) / np.sum(theta >= 0))\n",
    "        print '\\tTop5 PMI            {}'.format(_top5_pmi(phi))\n",
    "        print '\\tTop10 PMI           {}'.format(_top10_pmi(phi))\n",
    "        print '\\tTop20 PMI           {}'.format(_top20_pmi(phi))\n",
    "\n",
    "\n",
    "phi, theta = artm_thetaless_em_optimization(\n",
    "    n_dw_matrix=train_n_dw_matrix, \n",
    "    phi_matrix=phi_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=101,\n",
    "    iteration_callback=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Разделение на train test по документам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=['sci.electronics', 'sci.med', 'sci.space', 'sci.crypt', 'rec.sport.baseball', 'rec.sport.hockey'],\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0 documents from 3570\n",
      "Processed:  500 documents from 3570\n",
      "Processed:  1000 documents from 3570\n",
      "Processed:  1500 documents from 3570\n",
      "Processed:  2000 documents from 3570\n",
      "Processed:  2500 documents from 3570\n",
      "Processed:  3000 documents from 3570\n",
      "Processed:  3500 documents from 3570\n",
      "Nonzero values: 199636\n",
      "CPU times: user 4min 30s, sys: 24 ms, total: 4min 30s\n",
      "Wall time: 4min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_dw_matrix_doc_train, token_2_num_doc_train, num_2_token_doc_train, doc_targets_doc_train = prepare_dataset(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories=['sci.electronics', 'sci.med', 'sci.space', 'sci.crypt', 'rec.sport.baseball', 'rec.sport.hockey'],\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero values: 109917\n",
      "CPU times: user 1min 3s, sys: 0 ns, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_dw_matrix_doc_test, token_2_num_doc_test, num_2_token_doc_test, doc_targets_doc_test = prepare_dataset(dataset_test, token_2_num=token_2_num_doc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters time 6.26754903793\n",
      "SVM(C=0.1, gamma=0.001) cv-score: 0.208  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.208  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.757  test-score: 0.765\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.795  test-score: 0.803\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.785  test-score: 0.806\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.208  test-score: 0.169\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.759  test-score: 0.766\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.797  test-score: 0.808\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.808  test-score: 0.826\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.807  test-score: 0.822\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.759  test-score: 0.764\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.796  test-score: 0.809\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.81  test-score: 0.823\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.814  test-score: 0.826\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.794  test-score: 0.809\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.796  test-score: 0.809\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.807  test-score: 0.82\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.82  test-score: 0.83\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.809  test-score: 0.822\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.774  test-score: 0.793\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.808  test-score: 0.819\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.816  test-score: 0.829\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.815  test-score: 0.828\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.786  test-score: 0.802\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.768  test-score: 0.775\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.812  test-score: 0.829\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.817  test-score: 0.83\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.807  test-score: 0.82\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.775  test-score: 0.794\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.767  test-score: 0.773\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=100.0, gamma=0.1\n",
      "CV score: 0.82\n",
      "Test score:0.83\n"
     ]
    }
   ],
   "source": [
    "D, W = n_dw_matrix_doc_train.shape\n",
    "T = 15\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.ones(shape=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = trivial_regularization\n",
    "\n",
    "phi, theta = em_optimization(\n",
    "    n_dw_matrix=n_dw_matrix_doc_train, \n",
    "    phi_matrix=phi_matrix,\n",
    "    theta_matrix=theta_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=150,\n",
    "    loss_function=LogFunction()\n",
    ")\n",
    "\n",
    "best_C, best_gamma, _, _ = svm_score(theta, doc_targets_doc_train)\n",
    "algo = SVC(C=best_C, gamma=best_gamma).fit(theta, doc_targets_doc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSA not const phi\n",
      "Iters time 0.0253489017487\n",
      "1 0.731206293706\n",
      "Iters time 0.0532741546631\n",
      "2 0.786713286713\n",
      "Iters time 0.135732889175\n",
      "3 0.799825174825\n",
      "Iters time 0.1014149189\n",
      "4 0.804632867133\n",
      "Iters time 0.132496118546\n",
      "5 0.803321678322\n",
      "\n",
      "PLSA const phi\n",
      "Iters time 0.0241158008575\n",
      "1 0.731206293706\n",
      "Iters time 0.0487589836121\n",
      "2 0.795891608392\n",
      "Iters time 0.0743708610535\n",
      "3 0.806381118881\n",
      "Iters time 0.0987200737\n",
      "4 0.811625874126\n",
      "Iters time 0.12330698967\n",
      "5 0.812062937063\n",
      "\n",
      "ARTM thetaless\n",
      "Iters time 0.0329439640045\n",
      "1 0.731206293706\n",
      "Iters time 0.0648798942566\n",
      "2 0.765297202797\n",
      "Iters time 0.0959179401398\n",
      "3 0.762237762238\n",
      "Iters time 0.128988027573\n",
      "4 0.761800699301\n",
      "Iters time 0.15828704834\n",
      "5 0.762674825175\n"
     ]
    }
   ],
   "source": [
    "D, W = n_dw_matrix_doc_test.shape\n",
    "theta_matrix = np.ones(shape=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "print 'PLSA not const phi' \n",
    "for iters in xrange(1, 6):\n",
    "    _, theta_test = em_optimization(\n",
    "        n_dw_matrix=n_dw_matrix_doc_test, \n",
    "        phi_matrix=phi,\n",
    "        theta_matrix=theta_matrix,\n",
    "        regularization_list=regularization_list,\n",
    "        iters_count=iters,\n",
    "        loss_function=LogFunction(),\n",
    "    )\n",
    "    print iters, accuracy_score(algo.predict(theta_test), doc_targets_doc_test)\n",
    "\n",
    "print '\\nPLSA const phi' \n",
    "for iters in xrange(1, 6):\n",
    "    _, theta_test = em_optimization(\n",
    "        n_dw_matrix=n_dw_matrix_doc_test, \n",
    "        phi_matrix=phi,\n",
    "        theta_matrix=theta_matrix,\n",
    "        regularization_list=regularization_list,\n",
    "        iters_count=iters,\n",
    "        loss_function=LogFunction(),\n",
    "        const_phi=True\n",
    "    )\n",
    "    print iters, accuracy_score(algo.predict(theta_test), doc_targets_doc_test)\n",
    "    \n",
    "print '\\nARTM thetaless'    \n",
    "for iters in xrange(1, 6):\n",
    "    _, theta_test = artm_thetaless_em_optimization(\n",
    "        n_dw_matrix=n_dw_matrix_doc_test, \n",
    "        phi_matrix=phi,\n",
    "        regularization_list=regularization_list,\n",
    "        iters_count=iters,\n",
    "    )\n",
    "    print iters, accuracy_score(algo.predict(theta_test), doc_targets_doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters time 7.60945916176\n",
      "SVM(C=0.1, gamma=0.001) cv-score: 0.203  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.01) cv-score: 0.203  test-score: 0.169\n",
      "SVM(C=0.1, gamma=0.1) cv-score: 0.343  test-score: 0.485\n",
      "SVM(C=0.1, gamma=1) cv-score: 0.787  test-score: 0.817\n",
      "SVM(C=0.1, gamma=10.0) cv-score: 0.792  test-score: 0.82\n",
      "SVM(C=1.0, gamma=0.001) cv-score: 0.203  test-score: 0.169\n",
      "SVM(C=1.0, gamma=0.01) cv-score: 0.351  test-score: 0.498\n",
      "SVM(C=1.0, gamma=0.1) cv-score: 0.785  test-score: 0.815\n",
      "SVM(C=1.0, gamma=1) cv-score: 0.802  test-score: 0.829\n",
      "SVM(C=1.0, gamma=10.0) cv-score: 0.804  test-score: 0.82\n",
      "SVM(C=10.0, gamma=0.001) cv-score: 0.352  test-score: 0.499\n",
      "SVM(C=10.0, gamma=0.01) cv-score: 0.786  test-score: 0.813\n",
      "SVM(C=10.0, gamma=0.1) cv-score: 0.805  test-score: 0.827\n",
      "SVM(C=10.0, gamma=1) cv-score: 0.807  test-score: 0.829\n",
      "SVM(C=10.0, gamma=10.0) cv-score: 0.788  test-score: 0.811\n",
      "SVM(C=100.0, gamma=0.001) cv-score: 0.786  test-score: 0.813\n",
      "SVM(C=100.0, gamma=0.01) cv-score: 0.803  test-score: 0.829\n",
      "SVM(C=100.0, gamma=0.1) cv-score: 0.808  test-score: 0.829\n",
      "SVM(C=100.0, gamma=1) cv-score: 0.806  test-score: 0.826\n",
      "SVM(C=100.0, gamma=10.0) cv-score: 0.767  test-score: 0.789\n",
      "SVM(C=1000.0, gamma=0.001) cv-score: 0.803  test-score: 0.829\n",
      "SVM(C=1000.0, gamma=0.01) cv-score: 0.809  test-score: 0.828\n",
      "SVM(C=1000.0, gamma=0.1) cv-score: 0.811  test-score: 0.828\n",
      "SVM(C=1000.0, gamma=1) cv-score: 0.782  test-score: 0.816\n",
      "SVM(C=1000.0, gamma=10.0) cv-score: 0.761  test-score: 0.783\n",
      "SVM(C=10000.0, gamma=0.001) cv-score: 0.811  test-score: 0.828\n",
      "SVM(C=10000.0, gamma=0.01) cv-score: 0.805  test-score: 0.824\n",
      "SVM(C=10000.0, gamma=0.1) cv-score: 0.805  test-score: 0.819\n",
      "SVM(C=10000.0, gamma=1) cv-score: 0.772  test-score: 0.783\n",
      "SVM(C=10000.0, gamma=10.0) cv-score: 0.76  test-score: 0.776\n",
      "\n",
      "\n",
      "\n",
      "Best cv params: C=10000.0, gamma=0.001\n",
      "CV score: 0.811\n",
      "Test score:0.828\n"
     ]
    }
   ],
   "source": [
    "D, W = n_dw_matrix_doc_train.shape\n",
    "T = 15\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "phi_matrix = np.random.uniform(size=(T, W)).astype(np.float64)\n",
    "phi_matrix /= np.sum(phi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "theta_matrix = np.ones(shape=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "regularization_list = np.zeros(200, dtype=object)\n",
    "regularization_list[:] = trivial_regularization\n",
    "\n",
    "phi, theta = artm_thetaless_em_optimization(\n",
    "    n_dw_matrix=n_dw_matrix_doc_train, \n",
    "    phi_matrix=phi_matrix,\n",
    "    regularization_list=regularization_list,\n",
    "    iters_count=150\n",
    ")\n",
    "\n",
    "best_C, best_gamma, _, _ = svm_score(theta, doc_targets_doc_train)\n",
    "algo = SVC(C=best_C, gamma=best_gamma).fit(theta, doc_targets_doc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSA not const phi\n",
      "Iters time 0.0273690223694\n",
      "1 0.807692307692\n",
      "Iters time 0.0589830875397\n",
      "2 0.804195804196\n",
      "Iters time 0.0850698947906\n",
      "3 0.801573426573\n",
      "Iters time 0.110976934433\n",
      "4 0.80201048951\n",
      "Iters time 0.117080211639\n",
      "5 0.803321678322\n",
      "\n",
      "PLSA const phi\n",
      "Iters time 0.022481918335\n",
      "1 0.807692307692\n",
      "Iters time 0.0538229942322\n",
      "2 0.80506993007\n",
      "Iters time 0.0831460952759\n",
      "3 0.805506993007\n",
      "Iters time 0.111595153809\n",
      "4 0.805506993007\n",
      "Iters time 0.134366035461\n",
      "5 0.80506993007\n",
      "\n",
      "ARTM thetaless\n",
      "Iters time 0.0378141403198\n",
      "1 0.807692307692\n",
      "Iters time 0.0722370147705\n",
      "2 0.807692307692\n",
      "Iters time 0.11547088623\n",
      "3 0.805506993007\n",
      "Iters time 0.188162088394\n",
      "4 0.803758741259\n",
      "Iters time 0.179550886154\n",
      "5 0.802447552448\n"
     ]
    }
   ],
   "source": [
    "D, W = n_dw_matrix_doc_test.shape\n",
    "theta_matrix = np.ones(shape=(D, T)).astype(np.float64)\n",
    "theta_matrix /= np.sum(theta_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "print 'PLSA not const phi' \n",
    "for iters in xrange(1, 6):\n",
    "    _, theta_test = em_optimization(\n",
    "        n_dw_matrix=n_dw_matrix_doc_test, \n",
    "        phi_matrix=phi,\n",
    "        theta_matrix=theta_matrix,\n",
    "        regularization_list=regularization_list,\n",
    "        iters_count=iters,\n",
    "        loss_function=LogFunction(),\n",
    "    )\n",
    "    print iters, accuracy_score(algo.predict(theta_test), doc_targets_doc_test)\n",
    "\n",
    "print '\\nPLSA const phi' \n",
    "for iters in xrange(1, 6):\n",
    "    _, theta_test = em_optimization(\n",
    "        n_dw_matrix=n_dw_matrix_doc_test, \n",
    "        phi_matrix=phi,\n",
    "        theta_matrix=theta_matrix,\n",
    "        regularization_list=regularization_list,\n",
    "        iters_count=iters,\n",
    "        loss_function=LogFunction(),\n",
    "        const_phi=True\n",
    "    )\n",
    "    print iters, accuracy_score(algo.predict(theta_test), doc_targets_doc_test)\n",
    "    \n",
    "print '\\nARTM thetaless'    \n",
    "for iters in xrange(1, 6):\n",
    "    _, theta_test = artm_thetaless_em_optimization(\n",
    "        n_dw_matrix=n_dw_matrix_doc_test, \n",
    "        phi_matrix=phi,\n",
    "        regularization_list=regularization_list,\n",
    "        iters_count=iters,\n",
    "    )\n",
    "    print iters, accuracy_score(algo.predict(theta_test), doc_targets_doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
