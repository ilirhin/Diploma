\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{MnSymbol}
\usepackage{wasysym}
\usepackage{mathtext}
\usepackage{mathenv}
\usepackage{listings}
\usepackage{color}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amssymb,amsfonts,amsmath,mathtext,cite,enumerate,float}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage[a4paper, left=25mm, right=20mm, top=20mm, bottom=20mm]{geometry}


\newtheorem{definition}{Определение}[section]
\newtheorem{remark}{Примечание}[subsection]
\newtheorem{suggest}[remark]{Соглашение}
\newtheorem{claim}[remark]{Утверждение}
\newtheorem{lemma}[remark]{Лемма}
\newtheorem{theorem}{Теорема}
\newtheorem{conseq}{Следствие}[theorem]
\newenvironment{Proof} 
	{\par\noindent{\bf Доказательство.}} 
	{\hfill$\blacksquare$}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}

\renewcommand{\baselinestretch}{1.4}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}

\DeclareMathOperator{\Supp}{Supp}
\DeclareMathOperator{\sparse}{sparse}

\begin{document}
\begin{titlepage}

\begin{center}

Министерство образования и науки Российской Федерации\\[1em]
Государственное образовательное учреждение\\
высшего профессионального образования \\
«МОСКОВСКИЙ ФИЗИКО-ТЕХНИЧЕСКИЙ ИНСТИТУТ \\
(ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ)»\\[1em]

\begin{minipage}{\textwidth}
\begin{flushleft}
\begin{tabular}{ l l }
Факультет & Инноваций и высоких технологий\\
Кафедра & Анализа данных
\end{tabular}
\end{flushleft}
\end{minipage}\\[1em]

\begin{minipage}{\textwidth}
\begin{flushright}
\textit{Тип работы:}\\
Выпускная квалификационная работа по направлению\\
010400 «Прикладные математика и информатика»
\end{flushright}
\end{minipage}\\[3em]


{Дипломная работа}\\
{на тему:}\\[1em]
\textbf{\large Сходимость численных методов вероятностного тематического моделирования}\\[6em]

\begin{minipage}{\textwidth}
\begin{flushright}
\textit{Научный руководитель:}\\
\underline{\hspace*{2.5cm}} д.ф.-м.н., профессор К.\,В.~Воронцов
\end{flushright}
\end{minipage}\\[3em]

\begin{minipage}{\textwidth}
\begin{flushright}
\textit{Работу выполнил:}\\
Студент 093 группы\\
\underline{\hspace*{2.5cm}} И.\,А.~Ирхин
\end{flushright}
\end{minipage}\\[3em]

\vfill
{\normalsize Москва 2016}
\end{center}
\end{titlepage}


	\tableofcontents
	\newpage
	\renewcommand{\baselinestretch}{1.5}
	\section{Введение}
Тематическое моделирование (topic modeling) — одно из современных приложений машинного обучения к анализу текстов, активно развивающееся с конца 90-х годов. Тематическая модель (topic model) коллекции текстовых документов определяет, к каким темам относится каждый документ и какие слова (термины) образуют каждую тему.\\
Вероятностная тематическая модель (ВТМ) описывает каждую тему дискретным распределением на множестве терминов, каждый документ — дискретным распределением на множестве тем. Предполагается, что коллекция документов -- это последовательность терминов, выбранных случайно и независимо из смеси таких распределений, и ставится задача восстановления компонент смеси по выборке.
	\subsection{О методах вероятностного тематического моделирования}
	\subsection{Цель работы}

	\section{Аддитивная регуляризация тематических моделей}

	\subsection{Классическая тематическая модель}
\subsubsection{Постановка задачи и обозначения}
Для  задачи вероятностоного тематического моделирования применяется  классическим решением является вероятностный латентный семантический анализ (Probabilistic Latent Semantic Analysis, PLSA). Модель предложена Томасом Хофманном в 1999 году \cite{plsadef2}. Введём обозначения, которые будут использоваться нами далее в работе.\\
Пусть $D$ -- множество (коллекция) текстовых документов, $W$ -- множество (словарь) всех употребляемых в них терминов (слов или словосочетаний). Каждый документ $d \in D$ представляет собой последовательность $n_d$ терминов $(w_1, . . . , w_{n_d})$ из словаря $W$. Термин может повторяться в документе много раз.
Пусть существует конечное множество тем $T$, и каждое употребление термина $w$ в каждом документе $d$ связано с некоторой темой $t \in T$, которая не известна. Формально тема определяется как дискретное (мультиномиальное) вероятностное распределение в пространстве слов заданного словаря $W$.\\
Введем дискретное вероятностное пространство $D \times W \times T$. Тогда коллекция документов может быть рассмотрена как множество троек $(d, w, t)$, выбранных случайно и независимо из дискретного распределения $p(d, w, t)$. При этом документы $d \in D$ и термины $w \in W$ являются наблюдаемыми переменными, тема $t \in T$ является латентной (скрытой) переменной.\\
Требуется найти распределения терминов в темах $p(w|t) \equiv \phi_{wt}$ для всех тем $t \in T$ и распределения тем в документах $p(t|d) \equiv \theta_{td}$ для всех документов $d \in D$. При этом делается ряд допущений.\\
Принимается гипотеза условной независимости $p(w|d,t) = p(w|t)$, и  по формуле полной вероятности получается вероятностная модель порождения документа $d$:
\[
p(w|d) = \sum_{t \in T} p(w|d,t)p(t|d) = \sum_{t \in T}p(w|t)p(t|d)=\sum_{t \in T}\phi_{wt}\theta_{td}
\]
Введем следующие обозначения:

$p_{tdw} \equiv p(t|d,w)$ -- вероятность того, что появление термина $w$ в документе $d$ связано с темой $t$;

$n_{dwt}$ -- число троек $(d,w,t)$ во всей коллекции. Другими словами, это число поялвений термина w в связи с темой t в документе d;

$n_{dw} = \sum_{t \in T} n_{dwt}$ -- число вхождений термина w в документ $d$, это наблюдаемая величина;

$n_{td} = \sum_{w \in d} n_{dwt}$ -- число вхождений всех терминов, связанных с темой t в документ $d$;

$n_{wt} = \sum_{d \in D} n_{dwt}$ -- число появлений термина w в связи с темой $t$ во всех документах коллеккции $D$;

$n_{w} = \sum_{d \in D} n_{dw}$ -- число вхожений термина w в коллекцию;

$n_{d} = \sum_{t \in T} n_{td}$ -- длина документа $d$;

$n_{t} = \sum_{w \in W} n_{wt}$ -- "длина темы" \ \ $t$, то есть число появления терминов в коллекции, связанных с темой $t$;

$n = \sum_{d \in D}\sum_{w \in d}\sum_{t \in T} n_{dwt}$ - длина коллекции.
\ \\
Правдоподобие --  это плотность распределения выборки $D$:
\[
p(D)=\prod^n_{i=1}p_i(d,w)=\prod_{d \in D}\prod_{w \in d}p(d,w)^{n_{dw}}
\]
Рассмотрим вероятностную тематическую модель $p(D,\Phi,\Theta)$, где 

$\Phi=(\phi_{wt})_{W \times T}$ -- искомая матрица терминов тем, $\phi_{wt} \equiv p(w|t)$.

$\Theta=(\theta_{td})_{T \times D}$ -- искомая матрица тем документов, $\theta_{td}\equiv p(t|d)$.
\ \\
Запишем задачу максимизации правдоподобия:
\[
p(D,\Phi,\Theta)=C\prod_{d \in D}\prod_{w \in d}p(d,w)^{n_{dw}}=\prod_{d \in D}\prod_{w \in d}p(d|w)^{n_{dw}}Cp(d)^{n_{dw}} \to \max_{\Phi,\Theta},
\]
где $C$ -- нормировочный множитель, зависящий только от чисел $n_{dw}$. Прологарифмируем правдоподобие, получив задачу максимизации:
\[
L(D,\Phi,\Theta)=\sum_{d \in D}\sum_{w \in d}n_{dw}\ln\sum_{t \in T}\phi_{wt}\theta_{td} \to \max_{\Phi,\Theta}
\]
при ограничениях неотрицательности и нормировки
\[
\left\{
	\begin{aligned}
		\phi_{wt} \geq 0,~~\theta_{td} \geq 0\\
		\sum_{w \in W} \phi_{wt} = 1,~~\sum_{t \in T} \theta_{td}  = 1.
	\end{aligned}
\right.
\]
\subsubsection{Итоговый алгоритм}
Данная оптимизационная задача решается при помощи ЕМ-алгоритма \cite{plsadef2}:\\
\textbf{E-шаг}\\
На E-шаге, используя текущие значения параметров $\phi_{wt}$ и $\theta_{td}$ по формуле Байеса вычисляется значение условных вероятностей 

$p_{tdw} \equiv p(t|d,w) = \frac {p(w|t)p(t|d)} {p(w|d)} = \frac {\varphi_{wt}\theta_{td}} {\sum_s\varphi_{ws}\theta_{sd}}$\\
\textbf{М-шаг}\\
На M-шаге решается обратная задача: по условным вероятностям тем $p_{tdw}$ вычисляются новые приближения $\phi_{wt}$ и $\theta_{td}$.
Можно заметить, что величина $n_{dwt}=n_{dw}p(t|d,w)=n_{dw}H_{dwt}$ оценивает число вхождений термина $w$ в документ $d$, связанных с темой $t$. При этом оценка не всегда является целым числом. Просуммировав $n_{dwt}$ по документам $d$ и по терминам $w$ получим оценки:

$n_{wt}=\sum_{d \in D} n_{dwt}$

$n_t = \sum_{w \in W}n_{wt}$

$n_{td}=\sum_{w \in d} n_{dwt}$

$n_d = \sum_{t \in T}n_{td}$

$\phi_{wt}=\frac{n_{wt}}{n_t}$

$\theta_{td} = \frac{n_{dt}}{n_{d}}$\\
В исходном варианте алгоритм PLSA имеется ряд недостатков:
\begin{enumerate}
\item Медленная сходимость на больших коллекциях, так как матрицы $\Phi$ и $\Theta$ в базовом варианте обновляются после прохода всей коллекции, а также необходимость хранить трехмерную матрицу $p_{tdw}$. Эти проблемы могут быть решены принудительным более частым обновлениям $\Phi$ и $\Theta$ и внесением E-шага внутрь M-шага алгоритма. Развитие этой идеи приводит к рациональному, стохастическому и онлайн алгоритмам PLSA.
\item Для алгоритма PLSA характерно переобучение, а также неединственность и неустойчивость решения. Это связано с большим числом параметров $\phi_{wt}$ и $\theta_{td}$ $(|W|\cdot|T|+|T|\cdot|D|)$ и на них не накладывается никаких ограничений регуляризации. Кроме того, алгоритм PLSA неверно оценивает вероятность новых слов. Так, если $n_w=0$, то $p(w|t)=0$ для всех  $t \in T$. Последний недостаток особо заметен в том случае, когда в контрольной выборке присутствует большое число новых терминов. Для устранения этой проблемы в алгоритм вводят регуляризации: сглаживание, разреживание,учёт дополнительной внешней информации.
\item Алгоритм не выделяет нетематические слова. В реальном тексте приличествуют термины, которые не относятся явно ни к одной из тем. Учет таких терминов возможен с помощью робастных тематических моделях, в которые добавляется шумовая и фоновая составляющие.
\item Алгоритм PLSA не позволяет управлять разреженностью. Действительно, если в начале работы алгоритма $\phi_{wt} = 0$ или $\theta_{td} = 0$, то и после завершения работы алгоритма значения этих параметров останется равным 0. Для борьбы с этим недостатком используют регуляризацию и постепенное разреживание.
\end{enumerate}
	\subsection{Добавление регуляризатора}
Для решения проблемы неединственности и неустойчивости используется регуляризация. На искомое решение накладываются дополнительные ограничения. Подход Аддитивной Регуляризации Тематических Моделей  (Additive Regularization of Topic Models, ARTM) \cite{artmdef1, artmdef2, artmdef3} основан на идее многокритериальной регуляризации. Он позволяет строить модели, удовлетворяющие многим ограничениям одновременно. Каждое ограничение формализуется в виде регуляризатора -- оптимизационного критерия $R_i(\Phi,\Theta)\to\max$, зависящего от параметров модели. Взвешенная сума всех таких критериев $R(\Phi,\Theta) = \sum_{i=1}^k \tau_i R_i(\Phi,\Theta)$ максимизируется совместно с основным критерием правдоподобия.
\[
\sum_{d\in D} \sum_{w\in d} n_{dw}\log \sum_{t\in T} \phi_{wt}\theta_{td} \;+\; R(\Phi,\Theta)\;\to\; \max_{\Phi,\Theta},
\]
при тех же ограничениях нормировки и неотрицательности.\\
Применение теоремы Каруша-Куна-Такера позволяет получить систему уравнений для стационарных точек данной оптимизационной задачи. Решение данной системы методом просых итераций даёт EM-алгоритм со следующими формулами M-шага:
\[
\left\{
	\begin{aligned}
\phi_{wt} \propto  \left(n_{wt} + \phi_{wt}\frac{\partial R}{\partial\phi_{wt}}\right)_{+},\\
\theta_{td} \propto  \left(n_{td} + \theta_{td}\frac{\partial R}{\partial\theta_{td}}\right)_{+}.
	\end{aligned}
\right.
\]
Для комбинирования регуляризаторов в АРТМ необходимо продумывать стратегию регуляризации:
\begin{enumerate}
\item Какие регуляризаторы необходимы в данной задаче.
\item Какие регуляризаторы должны работать одновременно, какие друг за другом или попеременно, делая необходимую подготовительную работу.
\item Как менять коэффициент регуляризации каждого регуляризатора в ходе итераций: по каким условиям включать, усиливать, ослаблять и отключать каждый регуляризатор.
\end{enumerate}
\subsection{Сравнение с другими подходами вероятностного тематического моделирования}
Вероятностное тематическое моделирование развивается, главным образом, в рамках байесовского обучения и графических моделей. В байесовском подходе коллекция текстов описывается единой вероятностной порождающей моделью, при этом учёт дополнительных данных и формализация дополнительных ограничений производится через априорные распределения.\\
У такого подхода есть свои недостатки:
\begin{enumerate}
\item Не всякого рода знания удобно формализовать через априорные распределения. Попытка учесть больше знаний, чтобы построить более адекватную модель, приводит к значительному усложнению математического аппарата. В литературе почти нет работ по комбинированию тематических моделей, несмотря на их очевидную практическую востребованность.
\item Не факт, что естественный язык можно рассматривать как чисто статистическое явление. Одна из основных тенденций вычислительной лингвистики -- создание гибридных моделей, объединяющих лучшие достижения статистических и лингвистических подходов. Лингвистические знания не всегда удобно описывать на вероятностном языке.
\item Большинство байесовских моделей вынужденно используют априорное распределение Дирихле\cite{ldadef1}. Оно математически удобно благодаря сопряжённости с мультиномиальным распределением. Однако оно не моделирует каких-либо явлений естественного языка и не имеет убедительных лингвистических обоснований. Более того, оно противоречит естественному требованию разреженности, не допуская чистых нулей в матрицах $\Phi$  и $\Theta$.
\item Априорное распределение Дирихле является слишком слабым регуляризатором. Проблему неустойчивости он не решает.
\end{enumerate}
Учитывая эти проблемы, выделим преимущества подхода АРТМ:
\begin{enumerate}
\item В АРТМ регуляризаторы не обязаны быть априорными распределениями и иметь какую-либо вероятностную интерпретацию.
\item Регуляризатор Дирихле утрачивает свою особую роль, его не обязательно использовать в каждой модели для всех тем.
\item Математический аппарат очень прост: чтобы добавить регуляризатор, достаточно добавить его производные в формулы М-шага.
\item Многие байесовские тематические модели (или заложенные в них идеи) удаётся переформулировать через регуляризаторы.
\item Суммируя регуляризаторы, взятые из разных моделей, можно легко строить многоцелевые комбинированные модели.
\item Тематические модели в АРТМ легче понимать, легче выводить и легче комбинировать.
\item Снижается порог вхождения в область тематического моделирования для исследователей из смежных областей.
\end{enumerate}
	\section{Сходимость алгоритма ARTM}
	\subsection{ARTM как GEM алгоритм}
	Напомним,  перед нами стоит задача максимизации следующего функционала:
\[
L + \tau R = \sum_{w,d} n_{dw} \ln\sum_t \phi_{wt} \theta_{td} + \tau R(\Phi, \Theta) \to \max
\]
По аналогии с GEM алгоритмом введём дополнительный функционал:
\[
	Q(\Phi, \Theta, \Phi', \Theta') = \sum\limits_{d, w, t} n_{dw} p'_{tdw} \ln{\phi_{wt}\theta_{td}} + \tau R(\Phi, \Theta),
\]
где за $p'_{tdw}$ обозначено $\frac{\phi'_{wt} \theta'_{td}}{\sum\limits_t \phi'_{wt} \theta'_{td}}$.\\
Наша цель -- увеличивать значение данного функционала по $\phi$ и $\theta$ в сравнении с $Q(\Phi', \Theta', \Phi', \Theta')$ на каждой итерации. Запишем задачу максимизации данного функционала:
\[
Q(\Phi, \Theta, \Phi', \Theta') \to \max_{\Phi, \Theta}.
\]
Если мы применим теорему Куна-Такера, мы получим, что стационарная точка $Q$ должна удовлетворять следующей системе:
\[
\left\{
	\begin{aligned}
		\phi_{wt} \propto \bigg( \sum\limits_d n_{dw} p'_{tdw} + \tau\phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}} \bigg)_{+},\\
		\theta_{td} \propto \bigg( \sum\limits_w n_{dw} p'_{tdw} + \tau\theta_{td} \frac{\partial{R}}{\partial{\theta_{td}}} \bigg)_{+}.
	\end{aligned}
\right.
\]
В итоге мы получили систему уравнений, похожих на итерации ARTM. Это означает, что каждую итерацию ARTM можно интерпретировать как попытку приблизить решение максимизационной задачи функционала $Q$, итерируя систему уравнений для стационарной точки $Q$. В зависимости от того какую точку мы будем брать начальной при итерировании системы уравнений, мы получим разные варианты итераций ARTM.\\
Если начальное приближение это $(\phi_{wt}, \theta_{td})$, то мы получим итерации
\[
\left\{
	\begin{aligned}
		\phi_{wt} \propto \bigg( n_{wt} + \tau\phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}} \big( \phi_{wt}, \theta_{td}\big) \bigg)_{+},\\
		\theta_{td} \propto \bigg( n_{td} + \tau\theta_{td} \frac{\partial{R}}{\partial{\theta_{td}}} \big( \phi_{wt}, \theta_{td}\big) \bigg)_{+}.
	\end{aligned}
\right.
\]
Эту формулу будем в дальнейшем называть стандартной формулой М-шага. Если же считать, что начальное приближение это $(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d})$, то
\[
\left\{
	\begin{aligned}
		\phi_{wt} \propto \bigg( n_{wt} + \tau \frac{n_{wt}}{n_t} \frac{\partial{R}}{\partial{\phi_{wt}}} \big(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d}\big) \bigg)_{+},\\
		\theta_{td} \propto \bigg(n_{td} + \tau \frac{n_{td}}{n_d} \frac{\partial{R}}{\partial{\theta_{td}}} \big(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d}\big) \bigg)_{+}.
	\end{aligned}
\right.
\]
Эту формулу будем называть несмещённой модификацией М-шага. Таким образом, интерпретируя ARTM как итерации GEM алгоритма, мы можем использовать результаты о сходимостях GEM алгоритмов.
	\subsection{Теоремы о сходимости ARTM }
	\subsubsection{Итерации ARTM в обобщённом виде}	  
Итерации ARTM можно записать в следующем виде: \\
\textbf{E-step:}   
 
$
\begin{aligned}
& p_{tdw} \propto \phi_{wt} \theta_{td}
\end{aligned}
$\medskip\\
\textbf{M-step:}

$
\begin{aligned}    
& n_{wt} = \sum\limits_{d} n_{dw} p_{tdw},~~n_{td} = \sum\limits_{w} n_{dw} p_{tdw}\\    
& r_{wt} =  \phi_{wt}\frac{\partial R}{\partial\phi_{wt}},~~ r_{td} =  \theta_{td}\frac{\partial R}{\partial\theta_{td}}\\
& \phi_{wt}  \propto \bigl(n_{wt} + r_{wt} \bigr)_{+},~~\theta_{td} \propto  \bigl(n_{td} + r_{td}\bigr)_{+}\\
\end{aligned}
$\medskip\\
Величины $r_{wt}$ и $r_{td}$ удобно называть регуляризационными добавками. Они являются какими-то функциями от $\Phi$ и $\Theta$. Фактичеки именно от их свойств зависит всё поведение алгоритма. Именно их мы и будем исследовать.


	\subsubsection{Ограниченные регуляризаторы}
	 В работе Wu \cite{wuem} были сформулированы достаточные условия для сходимости GEM алгоритма. Чтобы их сформулировать, нужно сначала ввести одно определение.
	\begin{definition}
	Будем говорить, что $A\colon X \to 2^X$ -- замкнутое point-to-set отображение, если из $x_k \to x$, $x \in X$, $y_k \to y$ и $y_k \in A(x_k)$ следует, что $y \in A(x)$.
	\end{definition}
	Итак, сформулируем теорему, немного изменив обозначения под ARTM:
	\begin{theorem} \label{theorem_wu} \ \\
	Пусть $\{\psi_p\}$ - GEM последовательность, сгенерированная правилом $\psi_{p+1} \in M(\psi_p)$, где $M$ -- закмнутое point-to-set отображение. Пусть также значение $L + \tau R$ конечно и не уменьшается на итерациях, но приэтом ограниченно сверху, $|| \psi_p - \psi_{p+1}|| \to 0$, а множество стационарных точек $L + \tau R$ дискретно. Тогда $\psi_p$ сходится к некоторой стационарной точке $L + \tau R$.
	\end{theorem}
	Мы сведём нашу задачу к данной теореме, но сначала нам потребуется ввести новое определение.
	\begin{definition}
	Будем говорить, что регуляризатор $\tau R$ обладает свойством $\delta$-регулярности, если на итерациях ARTM $\forall t~\exists w \colon~n_{wt} + r_{wt} > \delta$ и аналогичное условие для $\theta$. Если регуляризатор  обладает свойством $\delta$-регулярности при каком-то $\delta > 0$, то будем говорить, что данный регуляризатор сильно регулярен.
	\end{definition}
	Это понятие обобщает понятие регулярности (легко видеть, что обычная регулярность это 0-регулярность по этому определению). Регулярность позволяла нам утверждать, что итерации ARTM корректно определены. Сильная же регулярность позволяет утверждать, что преобразования, которые мы совершаем на итерациях, не только определены, но и непрерывны. Что можно сказать о выполнении этого свойства на практике? Мы можем гарантировать его следующим образом: если значение выражения становится меньше $\delta$, то мы зануляем всю тему и выкидываем её, таким образом происходит селекция тем (например, мы могли изначально задать слишком большое число для количества тем).
	\begin{theorem} \label{theorem_neighbour_zero1} \ \\
	Пусть $R$ -- ограниченная сверху и дифференцируемая функция, причем, как регуляризатор, обладающая свойством регулярности. Также будем допустим,  что значение $Q(\Phi, \Theta, \Phi', \Theta')$ конечно и не уменьшается в сравнении $Q(\Phi', \Theta', \Phi', \Theta')$ на каждой итерации. Тогда при $d$ и $w$ т.ч. $n_{dw} > 0$ выполнено
\[
KL(p_{tdw}^{(n)}||p_{tdw}^{(n + 1)}) \to 0 \text{ при } n \to \infty.
\]
	\end{theorem}
	\begin{Proof}\ \\
Заметим, что $Q$ можно переписать следующим образом:
\[
Q(\Phi, \Theta, \Phi', \Theta') = L(\Phi, \Theta) + \tau R(\Phi, \Theta) + \sum\limits_{d, w, t} n_{dw} p'_{tdw} \ln{p_{tdw}}
\]
Пусть на итерации мы перешли в точку $\Phi'', \Theta''$. $Q$ не уменьшается на итерациях, значит
\[
	Q(\Phi'', \Theta'', \Phi', \Theta') \geq Q(\Phi', \Theta', \Phi', \Theta')
\]
Подставим вместо $Q$ его выражение:
\[
	L(\Phi'', \Theta'') + \tau R(\Phi'', \Theta'') + \sum\limits_{d, w, t} n_{dw} p'_{tdw} \ln{p''_{tdw}}  \geq L(\Phi', \Theta') + \tau R(\Phi', \Theta') + \sum\limits_{d, w, t} n_{dw} p'_{tdw} \ln{p'_{tdw}}
\]
\[
	\Delta(L + \tau R) \geq  \sum\limits_{d, w, t} n_{dw} p'_{tdw} \ln{\frac{p'_{tdw}}{p''_{tdw}}} = \sum\limits_{d, w} n_{dw} KL(p'_{dw} || p''_{dw}) \geq 0
\]
Таким образом $L + \tau R$  тоже не уменьшается. Но это ограниченная сверху функция, значит $(L + \tau R)^{(n)}$ сходится при $n \to \infty$. Более того при $n_{dw} > 0$:
\[
	KL(p_{tdw}^{(n)}||p_{tdw}^{(n + 1)}) \leq \Delta (L + \tau R)^{(n)} \to 0.
\]
\end{Proof}\ \\
\begin{conseq} \ \\
Если в дополнение к условиям Теоремы \ref{theorem_neighbour_zero1} $\tau R$ сильно регулярен, а $r_{wt}$ и $r_{td}$ непрерывны, то:
\[
|\phi_{wt}^{(n)} - \phi_{wt}^{(n+1)}| \to 0 \text{ и } |\theta_{td}^{(n)} - \theta_{td}^{(n+1)}| \to 0
\]
\end{conseq}
\begin{Proof}\ \\
По неравеству Пинскера \cite{pinsker} $||P - Q||_1 \leq 2\sqrt{KL(P||Q)}$. Поэтому сходимость по $KL$ влечёт за собой сходимость по $l_1$ норме. Осталось заметить, что в потребованных условиях  $\phi_{wt}$ и $\theta_{td}$ являются непрерывными функциями от $p_{tdw}$. А значит, сходимость вторых влечёт за собой сходимость первых.
\end{Proof}\ \\\
\begin{conseq} \ \\
В условия Следствия 1 все предельные точки $\phi$ и $\theta$ являются стационарными точками $L + \tau R$.
\end{conseq}
\begin{Proof}\ \\
Опишем коротко идею доказательства,  более подробно и формально оно описано в \cite{wuem}. Пусть  $\phi^0, \theta^0$ -- предельная точка. Мы знаем, что выполнено:
\[
Q(\Phi, \Theta, \Phi^0, \Theta^0) =  L(\Phi, \Theta) + \tau R(\Phi, \Theta) + \sum\limits_{d, w, t} n_{dw} p^0_{tdw} \ln{p_{tdw}}.
\]
Поскольку $\phi^0, \theta^0$  -- предельная точка, то значение $Q$ уже нельзя увеличить, а значит производная по $\phi$ и по $\theta$ левой части равна нулю. При $\phi = \phi^0$ и $\theta = \theta^0$ KL достигает минимума, а значит и его производные равны нулю. Таким образом получается, что и производные $L + \tau R$ равны нулю, что и требовалось доказать.
\end{Proof}\ \\
\begin{conseq} \ \\
Если в дополнение к условиям Следствия 1, множество стационарных точек $L + \tau R$ дискретно, то $\phi_{wt}^{(n)}$ и $\theta_{td}^{(n)}$ сходятся к стационарной точке $L + \tau R$.
\end{conseq}
\begin{Proof}\ \\
Положим $M(\phi, \theta) = \{artm(\phi, \theta)\}$, гле под $artm(\phi, \theta)$ понимается применение формул ARTM. В условиях Следствия 1 $artm$ -- непрерывное преобразование. Поэтому $M$ -- замкнутое point-to-set отображение. Остаётся заметить, что остальные условия Теоремы \ref{theorem_wu} тоже выполнены.
\end{Proof}\ \\
\subsubsection{Неограниченные регуляризаторы}
В предыдущем разделе нам была важна ограниченность $R$. Однако в ARTM частно используется регуляризатор разреживания $- \alpha \ln \phi_{wt}$, который не является ограниченным. Однако на практике данный регуляризатор прекрасно работает. В данном разделе мы постараемся понять почему. Основная идея состоит в том, что на практике у нас есть машинная точность $\varepsilon$, и все значения меньшие $\varepsilon$ считаются равными нулю. Это позволяет нам ограничить область значений снизу, и тем самым ограничить регуляризатор сверху. Также есть проблема с занулениями значений на итерациях, которую в предыдущем параграфе мы обошли за счёт сильного ограничения на $Q$. Тем не менее, при определённых  ограничениях на регуляризатор эти зануления будут структурированными, что позволит нам провести анализ. Теперь более подробно и более формально.
\begin{definition}
Будем говорить, что регуляризатор $\tau R$ сохраняет 0, если на итерациях $n_{wt} = 0 \implies \phi_{wt} = 0$ и $n_{td} = 0 \implies \theta_{td} = 0$
\end{definition}
Легко понять, что это определение формализует следующие свойство итераций: если на какой-то итерации значение $\phi_{wt}$ стало равным нулю, то оно будет оставаться нулевым всегда, и аналогично для $\theta_{td}$. Легко видеть, что это свойство легко проверяется аналитически. Остаётся отметить, что на практике все регуляризаторы обладают подобным свойством.
\begin{definition}
Будем говорить, что регуляризатор $\tau R$ $\varepsilon$-разреживающий, если на итерациях $\phi_{wt}, \theta_{td} \notin (0, \varepsilon)$.
\end{definition}
Данное свойство позволит нам формально учесть машинную точность (с этой точки зрения все регуляризаторы будут $\varepsilon$-разреживающими). Однако с точки зрения практики есть одна интересная особенность. Мы используем регуляризатор разреживания, чтобы каждой теме принадлежало лишь небольшое число слов. Фактически мы зануляем $n_{wt}$ , если его значение меньше $\alpha$, таким образом, после нормировки $\phi_{wt} \geq \frac{n_{wt} - \alpha}{n_t}$,  на реальных коллекциях очень часто происходит следующее: характерные слова темы $t$ имеют существенное значение $n_{wt}$ (например больше 1), а не характерные постепенно зануляются. В итоге мы получаем, что, начиная с некоторой итерации, $\phi_{wt} \notin (0, \frac{1-\alpha}{n_t})$ . Подробнее об этом вопросе мы поговорим в экспериментальной части.\\
Есть альтернативный способ добиться данного ограничения, достаточно заменить исходный регуляризатор на $-\alpha \ln \min(\phi_{wt}, \alpha)$. В этом случае мы получим, что на М-шаге мы зануляем выражения меньше $\alpha$ и не изменяем остальные значения. В этом случае $\phi_{wt}\notin (0, \frac{\alpha}{n_t})$. Мы опробуем данный способ в наших экспериментах.
\begin{definition}
Будем говорить, что регуляризатор $\tau R$ справедливый, если на итерациях $n_{dw} > 0 \implies \exists t\colon p_{tdw} > 0$.
\end{definition}
Это свойство -- чистая формальность. Поскольку мы будем производить разреживания, то мы не должны случайно занулить элемент матрицы $\Phi \Theta$ для которого $n_{dw} > 0$. Это привело бы к падению $L$ до -$\infty$.  Данное свойство в точности требует, чтобы такого не происходило. На практике оно обычно будет выполнено за счёт фоновых тем\cite{artmdef2}, поскольку они как правило дают небольшие вероятности для всех тем.\\
Итак, мы ввели три новых свойства регуляризатора, теперь мы можем доказать следующую теорему:

\begin{theorem} \label{theorem_neighbour_zero2} \ \\
	Пусть $R$ -- дифференцируемая функция при $\phi_{wt}, \theta_{td} \in (0, 1]$, причем, как регуляризатор, сохраняющая 0, справедливая, $\varepsilon$-разреживающая и обладающая свойством регулярности. Также будем допустим,  что значение $Q(\Phi, \Theta, \Phi', \Theta')$ конечно и не уменьшается в сравнении $Q(\Phi', \Theta', \Phi', \Theta')$, начиная с некоторой итерации. Тогда выполнено
\[
KL(p_{tdw}^{(n)}||p_{tdw}^{(n + 1)}) \to 0 \text{ при } n \to \infty.
\]
\end{theorem}
\begin{Proof}\ \\
Поскольку регуляризатор сохраняет 0, то с некоторой итерации множество позиций с нулевыми значениями в матрице $\Phi$ и $\Theta$ стабилизируется и не будет больше изменяться. Это очевидно следует из того факта, что  множество всех позиций конечно. Обозначим это множество за $\Omega$. В силу того, что регуляризатор $\varepsilon$-разреживающий, значения $\Phi$ и $\Theta$ в позициях из $\Omega$ будут $\geq \varepsilon$. Но $R$ -- дифференцируемая функция при $\phi_{wt}, \theta_{td} \in [\varepsilon, 1]$, а значит непрерывная и ограниченная. Далее мы можем повторить рассуждения Теоремы \ref{theorem_neighbour_zero1}, ограничивших значениями $\phi_{wt}$ и $\theta_{td}$ только в этих позициях. 
\end{Proof}\ \\
Также как и в случае Теоремы \ref{theorem_neighbour_zero1} данная теорема будет иметь аналогичные три следствия. Мы не будем приводить их ешё раз, так как они будут совпадать почти в каждом слове. Приведём только итоговую теорему, объединяющую все утверждения.
\begin{theorem} \label{theorem_convergence1} \ \\
	Пусть $R$ -- дифференцируемая функция при $\phi_{wt}, \theta_{td} \in (0, 1]$, причем, как регуляризатор, сохраняющая 0, справедливая, $\varepsilon$-разреживающая и обладающая свойством сильной регулярности, а  $r_{wt}$ и $r_{td}$ непрерывны. Также будем допустим,  что значение $Q(\Phi, \Theta, \Phi', \Theta')$ конечно и не уменьшается в сравнении с $Q(\Phi', \Theta', \Phi', \Theta')$, начиная с некоторой итерации. Тогда, если множество стационарных точек $L + \tau R$ дискретно при любой фиксации множества ненулевых позиций, то $\phi_{wt}^{(n)}$ и $\theta_{td}^{(n)}$ сходятся к стационарной точке $L + \tau R$ при ограничении на какое-то множество нулевых позиций.
\end{theorem}
Давайте проанализируем регуляризатор разреживания с точки зрения данной теоремы в случае стандартных формул М-шага ($r_{wt} = \phi_{wt}\frac{\partial{R}}{\partial{\phi_{wt}}}$) . Свойство $\varepsilon$-разреживания мы уже обсудили ранее. Свойство справедивости обычно выполняется за счёт фоновых тем \cite{artmdef2} . Сохранение нуля данным регуляризатором и непрерывность $r_{wt}$ очевидны. Единственный важный момент -- это конечность $Q$ на итерациях. Если мы зануляем какое-то значение, то значение регуляризатора уходит в бесконечность. Чтобы избежать этого эффекта, надо считать $- [\phi_{wt} > 0] \ln\phi_{wt}$.\\
Таким образом, мы теперь можем ответить на вопрос, что происходит на итерациях ARTM (в предположении увеличения $Q$). По сути итерации можно разбить на два этапа: селекция ненулевых позиций и оптимизация. На первом этапе при помощи регуляризатора выбирается множество ненулевых позиций итогового решения. Понятно, что параллельно ведётся и оптимизация $L + \tau R$, но из наличия положительной срезки этот этап очень сложно анализировать. Его стоит воспринимать как подготовка начального приближения. На втором этапе оптимизация выходит на первый план. В силу того, что множество нулевых позиций не изменяется, положительную срезку в формулах можно убрать. Это облегчает анализ, более подробно об измении функционалов на итерациях мы поговорим в соответсвующей главе.

	\subsection{Cвойства траектории итерационного процесса ARTM}
Важным условием в теоремах сходимости является дискретность множества стационарных точек. В силу неединственности стохастического разложения матрицы (ссылка) это условие может не выполняться. Это подводит нас к поиску альтернативных достаточных условий сходимости. Сходимость итерационного процесса неразрывно связано со свойстами его траектории. Нам удалось связать свойства траектории процесса с изменениями $L + \tau R$.
\begin{theorem} \label{theorem_series}\ \\
	Пусть выполнены условия теоремы \ref{theorem_neighbour_zero1}. Тогда сходимость ряда
	\[
		\sum\limits_{n=1}^{\infty} (\Delta L^{(n)} + \tau \Delta R^{(n)})^{\alpha}
	\]
	влечёт за собой сходимость ряда
	\[
		\sum\limits_{n=1}^{\infty} (\Delta p_{tdw}^{(n)})^{2 \alpha}
	\]
\end{theorem}
\begin{Proof}\ \\
Было доказано, что $KL(p_{tdw}^{(n)}||p_{tdw}^{(n + 1)}) \leq \Delta (L + \tau R)^{(n)}$. По неравенству Пинскера $|| p_{dw}^{(n)} - p_{dw}^{(n+1)}||_1 \leq C \cdot \sqrt{KL(p_{tdw}^{(n)}||p_{tdw}^{(n+1)})} \leq C \sqrt{\Delta (L + \tau R)^{(n)}}$. А значит, $ (\Delta p_{tdw}^{(n)})^{2} \leq C^2 \Delta (L + \tau R)^{(n)} $, откуда очевидно следует требуемое утверждение.
\end{Proof}\ \\
\begin{conseq}
В условиях теоремы  \ref{theorem_neighbour_zero1} ряд  $\sum\limits_{n=1}^{\infty} (\Delta p_{tdw}^{(n)})^{2 \alpha}$ сходится при $\alpha \geq 1$.
\end{conseq}
\begin{Proof}\ \\
Монотонность по $\alpha$ свойства сходимости очевидна. При $\alpha=1$ мы имеем
\[
\sum\limits_{n=1}^{m} (\Delta L^{(n)} + \tau \Delta R^{(n)}) = ( L^{(m)} + \tau R^{(m)}) - ( L^{(0)} + \tau R^{(0)})
\]
А сходимость данной последовательности мы уже доказывали.
\end{Proof}\ \\
\begin{conseq}
В условиях теоремы  \ref{theorem_convergence1} условие дискретности множества стационарных точек можно заменить условием сходимости ряда
\[
\sum\limits_{n=1}^{\infty} \sqrt{\Delta L^{(n)} + \tau \Delta R^{(n)}}.
\]
\end{conseq}
К сожалению, это абсолютно неконструктивное условие. Однако, стоит взять во внимание, что при вычислениях, начиная с некоторого момента, изменения функционалов меньше машинной точности, и к этому моменту на практике частичная сумма ряда не уходит в бесконечность. Поэтому вычислительно на реальных коллекциях этот ряд сходится. Также стоит отметить, что с такой точки зрения полученная точка  сходимости будет вычислительно стационарной.
	\subsection{Изменение регуляризированного правдоподобия на итерациях ARTM}
	\subsubsection{Общий анализ}
В данной главе под $Q^{\prime}$ будем понимать $\sum\limits_{d, w, t} n_{dw} p'_{tdw} \ln{\phi_{wt}\theta_{td}}$. То есть $Q = Q^{\prime} + \tau R$.\\
Напомним, у нас есть два набора формул для регуляризационных добавок на М-шаге:
\[
\left\{
	\begin{aligned}
		r_{wt}= \tau\phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}},\\
		r_{td} = \tau\theta_{td} \frac{\partial{R}}{\partial{\theta_{td}}}.
	\end{aligned}
\right.
\text{~~и~~}
\left\{
	\begin{aligned}
		r_{wt} = \tau \frac{n_{wt}}{n_t} \frac{\partial{R}}{\partial{\phi_{wt}}} \biggl(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d}\biggr),\\
		r_{td}= \tau \frac{n_{td}}{n_d} \frac{\partial{R}}{\partial{\theta_{td}}} \biggl(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d}\biggr).
	\end{aligned}
\right.
\]
Само обновление параметров происходит по формуле 
\[
\left\{
\begin{aligned}
 \phi_{wt}  \propto \bigl(n_{wt} + r_{wt} \bigr)_{+},\\
\theta_{td} \propto  \bigl(n_{td} + r_{td}\bigr)_{+}
\end{aligned}
\right.
\]
Проводить анализ суммарного измеения функционала $Q$ по такой формуле слишком обременительно. Поэтому мы разложим это преобразование на два этапа. Первый этап -- максимизация $Q^{\prime}$:
\[
\left\{
	\begin{aligned}
		\phi_{wt} \propto n_{wt},\\
		\theta_{td} \propto n_{td} .
	\end{aligned}
\right.
\]
и второй этап (назовём его регуляризационным преобразованием) -- максимизация $R$:
\[
\left\{
\begin{aligned}
 \phi_{wt}  \propto \bigl(n_{wt} + r_{wt} \bigr)_{+},\\
\theta_{td} \propto  \bigl(n_{td} + r_{td}\bigr)_{+}
\end{aligned}
\right.
\]
Будем оценивать изменения функционалов отдельно по данным этапам. Для начала проанализируем эти этапы. Мы переходим в точку $(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d})$ на первом шаге, а затем стараемся максимизировать $R$. Как мы говорили, есть два варианта для формул регуляризационных добавок. Напомним их на примере $r_{wt}$. В первом случае $r_{wt} = \tau\phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}} \big( \phi_{wt}, \theta_{td}\big)$, а во втором $\phi_{wt} = \frac{n_{wt}}{n_t} \frac{\partial{R}}{\partial{\phi_{wt}}} \big(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d}\big)$. Первый способ предлагает определить добавку в начальной точке и потом её использовать, второй способ определяет добавку уже в новой точке. Понятно, что второй способ формально должен лучше максимизировать $R$, так как он выбирает добавку по более актуальной информации. Для первого варианта нам не удалось провести анализ, так как неясно как связаны градиент в точке $(\phi_{wt}, \theta_{td})$ и $(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d})$. Для второго варианта анализ оказался успешнее.\\
Введём новую функцию $R^{\prime}$ зависящую от $n_{wt}$ следующим образом: $R^{\prime}(n_{wt}, n_{td}) = R( \frac{n_{wt}}{\sum\limits_w n_{wt}},  \frac{n_{td}}{\sum\limits_t n_{td}})$. Также введём обозначение $g_{wt}$ для $\frac{\partial{R^{\prime}}}{\partial{n_{wt}}}$ и $g_{td}$ для $\frac{\partial{R^{\prime}}}{\partial{n_{td}}}$. Обратим внимание, что это функции. Для этих величин справедлива следующее утверждение:
\begin{claim}           
Для $g_{wt}$ и $g_{td}$ выполнено
\[
g_{wt} = \frac{1}{n_t} \sum_{u} \bigg(\frac{\partial{R}}{\partial{\phi_{wt}}}  -  \frac{\partial{R}}{\partial{\phi_{ut}}} \bigg)  \phi_{ut} ,
\]
\[
g_{td} = \frac{1}{n_d} \sum_{s} \bigg(\frac{\partial{R}}{\partial{\theta_{td}}}  -  \frac{\partial{R}}{\partial{\theta_{sd}}} \bigg)  \theta_{sd} .
\]
\end{claim}
\begin{Proof}\\
$\phi_{wt} = \frac{n_{wt}}{\sum_w n_{wt}}$, поэтому
\[
\frac{\partial{\phi_{ut}}}{\partial{n_{wt}}} = \frac{\partial{ \frac{n_{ut}}{\sum\limits_v n_{vt}}}}{\partial{n_{wt}}} = \frac{ \frac{\partial{n_{ut}}}{\partial{n_{wt}}}}{\sum\limits_v n_{vt}} - \frac{n_{ut}}{(\sum\limits_v n_{vt})^2} = I\{u = w\} \frac{1}{n_t} - \frac{\phi_{ut}}{n_t} = \frac{1}{n_t}\bigg( 
 I\{u = w\} - \phi_{ut} \bigg).
\]
А значит
\[
\frac{\partial{R^{\prime}}}{\partial{n_{wt}}} = \sum_{u} \frac{\partial{R}}{\partial{\phi_{ut}}} \frac{\partial{\phi_{ut}}}{\partial{n_{wt}}} = \frac{1}{n_t} \bigg( \frac{\partial{R}}{\partial{\phi_{wt}}} - \sum_{u}  \frac{\partial{R}}{\partial{\phi_{ut}}} \phi_{wt} \bigg) = \frac{1}{n_t} \bigg( \frac{\partial{R}}{\partial{\phi_{wt}}} - \sum_{u}  \frac{\partial{R}}{\partial{\phi_{ut}}} \phi_{ut} \bigg) = \frac{1}{n_t} \sum_{u} \bigg(\frac{\partial{R}}{\partial{\phi_{wt}}}  -  \frac{\partial{R}}{\partial{\phi_{ut}}} \bigg)  \phi_{ut}.
\]
Для $\frac{\partial{R^{\prime}}}{\partial{n_{td}}}$ формула доказывается аналогично.
\end{Proof}\\
Теперь мы можем доказать следующую лемму:
\begin{lemma}           
 Для несмещённой модификации М-шага в ходе регуляризационного преобразования  без занулений, угол  между вектором изменений и градиентом $R$ острый, если градиент ненулевой.
\end{lemma}
\begin{Proof}\\
Мы знаем, что при регуляризационном преобразовании $\Delta n_{wt} = \tau \phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}}$, а для несмещённой модификации $\phi_{wt} = \frac{n_{wt}}{\sum\limits_w n_{wt}}$. Отсюда
\[
\langle \Delta n, \nabla R^{\prime}(n_{wt}, n_{td})\rangle = \sum\limits_{w, t, u}  \frac{1}{n_{t}}  \bigg(  \frac{\partial{R}}{\partial{\phi_{wt}}}  -  \frac{\partial{R}}{\partial{\phi_{ut}}}  \bigg)  \tau \frac{\partial{R}}{\partial{\phi_{wt}}} \phi_{wt} \phi_{ut}
\]
Если переобозначить $u$ за $w$ и наоборот, то 
\[
\sum\limits_{w, t, u}  \frac{1}{n_{t}}  \bigg(  \frac{\partial{R}}{\partial{\phi_{wt}}}  -  \frac{\partial{R}}{\partial{\phi_{ut}}}  \bigg)  \tau \frac{\partial{R}}{\partial{\phi_{wt}}} \phi_{wt} \phi_{ut}  = \sum\limits_{w, t, u}  \frac{1}{n_{t}}  \bigg(  \frac{\partial{R}}{\partial{\phi_{ut}}}  -  \frac{\partial{R}}{\partial{\phi_{wt}}}  \bigg)  \tau \frac{\partial{R}}{\partial{\phi_{ut}}} \phi_{wt} \phi_{ut} = 
\]
\[
= \sum\limits_{w, t, u}  \frac{1}{n_{t}}  \bigg(  \frac{\partial{R}}{\partial{\phi_{wt}}}  -  \frac{\partial{R}}{\partial{\phi_{ut}}}  \bigg)  \tau \big(-\frac{\partial{R}}{\partial{\phi_{ut}}}\big) \phi_{wt} \phi_{ut} = 
\]
\[
= \frac12 \bigg(\sum\limits_{w, t, u}  \frac{1}{n_{t}}  \bigg(  \frac{\partial{R}}{\partial{\phi_{wt}}}  -  \frac{\partial{R}}{\partial{\phi_{ut}}}  \bigg)  \tau \frac{\partial{R}}{\partial{\phi_{wt}}} \phi_{wt} \phi_{ut} +  \sum\limits_{w, t, u}  \frac{1}{n_{t}}  \bigg(  \frac{\partial{R}}{\partial{\phi_{wt}}}  -  \frac{\partial{R}}{\partial{\phi_{ut}}}  \bigg)  \tau \big(-\frac{\partial{R}}{\partial{\phi_{ut}}}\big) \phi_{wt} \phi_{ut} \bigg)= 
\]
\[
= \frac12 \tau \sum\limits_{t, w, u}  \frac{1}{n_{t}} \bigg(  \frac{\partial{R}}{\partial{\phi_{wt}}}  -  \frac{\partial{R}}{\partial{\phi_{ut}}}  \bigg)^2 \phi_{wt} \phi_{ut} = \tau \sum\limits_{t, w < u}  \frac{1}{n_{t}} \bigg(  \frac{\partial{R}}{\partial{\phi_{wt}}}  -  \frac{\partial{R}}{\partial{\phi_{ut}}}  \bigg)^2 \phi_{wt} \phi_{ut} \geq 0
\]
Пусть достигается равенство, тогда $\frac{\partial{R}}{\partial{\phi_{wt}}}  =  \frac{\partial{R}}{\partial{\phi_{ut}}}$ для всех $u$ и $w$. Тогда
\[
\frac{\partial{R^{\prime}}}{\partial{n_{wt}}} = \frac{1}{n_t} \bigg( \frac{\partial{R}}{\partial{\phi_{wt}}} - \sum_{u}  \frac{\partial{R}}{\partial{\phi_{ut}}} \phi_{ut} \bigg) = \frac{1}{n_t} \bigg( \frac{\partial{R}}{\partial{\phi_{wt}}} - \sum_{u}  \frac{\partial{R}}{\partial{\phi_{wt}}} \phi_{ut} \bigg) =
\]
\[
=\frac{1}{n_t} \bigg( \frac{\partial{R}}{\partial{\phi_{wt}}} - \frac{\partial{R}}{\partial{\phi_{wt}}} \sum_{u} \phi_{ut} \bigg)  = \frac{1}{n_t} \bigg( \frac{\partial{R}}{\partial{\phi_{wt}}} - \frac{\partial{R}}{\partial{\phi_{wt}}} \bigg) = 0
\]
Значит, градиент нулевой -- противоречие. Значит, равенство строгое и угол острый.
\end{Proof}\ \\
\ \\
В прошлой главе мы доказывали, что при определённых условиях на регуляризатор занулений не будет, начиная с некоторой итерации. Таким образом, если коэффициент $\tau$ не слишком большой, то на регуляризационном преобразовании будет происходить увеличение $R$ в силу малого изменения $n_{wt}$.\\
\ \\
Теперь нужно объединить результаты двух итераций. В ходе первого шага мы переходим в точку максимума $Q^{\prime}$, значит градиент $Q^{\prime}$ в данной точке нулевой. Это означает, что в данной точке градиент $Q^{\prime} + \tau R$ сонаправлен с градиентом $R$, что означает, что на шаге регуляризационного преобразования происходит неуменьшение $Q^{\prime} + \tau R$. Осталось понять, как изменяется данный функционал на первом шаге. Начиная с некоторого момента изменения $Q^{\prime}$ становятся незначительны, а это означает, что поскольку мы максимизируем $Q^{\prime} + \tau R$ в локальной окрестности, в которой находится и исходная точка, а значит она была потенциальным кандидатом при выборе улучшения, но если мы выбрали другое направление, то мы увеличиваем значение $Q^{\prime} + \tau R$ по сравнению с исходным.\\
\subsubsection{Использование градиента регуляризатора}
У вышеописанного рассуждения есть два допущения. Первое, мы считаем, что изменения $\phi$ и $\theta$ невелики, обычно так и есть после нескольких первых итераций, когда основные частоты посчитаются и мы  перестанем делать большие скачки в пространстве матриц. Второе, мы считаем, что мы локально максимизируем $Q^{\prime} + \tau R$, однако мы доказали, что происходит увеличение, а не максимизация. Тем не менее, существует несколько способов обойти это условие. Во-первых, на практике угол очень острый, что позволяет производить требуемое увеличение. Во-вторых, мы можем выбирать направление между предлагаемым направлением и направлением на старую точку. В третьих, можно использовать значения $g_{wt}$ и $g_{td}$ в качестве регуляризационных добавок. Тогда направление изменения при регуляризационном преобразовании будет совпадать с направлением градиента, а значит будет локальная максимизация. То есть предлагается использовать следующую формулу для регуляризационных добавок на М-шаге:
\[
\left\{
	\begin{aligned}
		r_{wt} = \tau A_t g_{wt}\bigg(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d}\bigg) = \tau A_t \frac{1}{n_t} \bigg[{\frac{\partial{R}}{\partial{\phi_{wt}}} - \sum\limits_u \phi_{ut} \frac{\partial{R}}{\partial{\phi_{ut}}} }\bigg] \bigg(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d}\bigg),\\
		r_{td} = \tau B_d g_{td} \bigg(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d}\bigg) = \tau B_d \frac{1}{n_d} \ \bigg[ {\frac{\partial{R}}{\partial{\theta_{td}}} - \sum\limits_s \theta_{sd} \frac{\partial{R}}{\partial{\theta_{sd}}} }\bigg] \bigg(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d}\bigg) ,
	\end{aligned}
\right.
\]
где $A_t$ и $B_d$ это какие-то константы, характеризующие величину шага вдоль градиента по данной теме(документу), для экспериментов использовались два простых варианта: $A_t = B_d = 1$ и $A_t=B_d=50$.\\
Такие добавки можно эффективно вычислить, поскольку второк слагаемое общее для всех слов(тем). Поэтому его можно считать кумулятивно при первом пробега, на котором считается первое слагаемое. А затем на втором пробеге вычесть его из всех добавок. Таким образом, асимптотика работы не увеличится. 

Градиент регуляризатора можно использовать ещё одним способом. Мы можем сделать подмену регуляризатора. Пусть $S$ т.ч. $\phi_{wt}\frac{\partial{R}}{\partial{\phi_{wt}}} = C_t g_{wt}(\phi_{wt}, \theta_{td})$, где $C_t$ -- какая-то константа, зависящая только от темы. Тогда мы можем применить все наши рассуждения к данному регуляризатору, но для анализа использовать функционал $Q^{\prime} + \tau S$. Тогда направление изменения при регуляризационном преобразовании для несмещённой модификации будет совпадать с направлением градиента, а значит будет локальная максимизация. К сожалению, решение данной системы уравнений в частных производных затруднено тем фактом, что матрица коэффициентов вырождена, что означает, что решение существует не всегда (на текущий момент есть гипотеза, что необходимым и достаточным условием является $\sum \phi_{wt}^2 \frac{\partial{R}}{\partial{\phi_{wt}}} = 0$). Анализ данного уравнения является перпективой дальнейшей работы.
           \subsubsection{Классификация регуляризаторов}
Точки зрения изменения функционала $Q$ стоит выделить несколько типов регуляризаторов.\\
	 \textbf{Аналитические регуляризаторы}. В эту группу попадают регуляризаторы, для которых мы можем явно найти решение максимизационной задачи $Q$. В этом случае нам не требуется анализировать углы между градиентами, мы получим увеличение функционала просто по построению. Таковыми регуляризаторами являются, например, регуляризаторы сглаживания и разреживания. Аналитические регуляризаторы также обладают ешё одним важным свойством: их воздействие можно считать отдельно. Поясним, что именно мы имеем в виду. Пусть $R = R_1 + R_2$, где $R_1$ -- аналитический регуляризатор. На М-шаге нам необходимо построить увеличение функционала $Q + \tau R = Q + \tau R_1 + \tau R_2$. По формулам М-шага мы определяем $n_{wt}$ и $n_{td}$ как точку максимума $Q$, а затем увеличиваем $R$.  Однако, мы можем определеить $n_{wt}$ и $n_{td}$ как точку максимума $Q + \tau R_1 $ (это можно сделать в силу аналитичности $R_1$), а затем стараться увеличить $R_2$. Таким образом, мы будем пользоваться численным методами оптимизации только для той части регуляризатора, где не можем явно найти максимум.\\
	\textbf{Вогнутые регуляризаторы}. Функционал $Q^{\prime}$ -- вогнут, если $R$ тоже вогнутая функция, то $Q^{\prime} + \tau R$ тоже вогнутая функция, поэтому имеет единственный максимум. Мы доказали, что будет происходить увеличение $Q^{\prime} + \tau R$ при некоторых допущениях. Однако в случае случае вогнутого регулязиратора мы можем сказать, что на шаге регуляризационного преобразования мы приближаемся к глобальному максимуму, а не просто увеличиваем значение. Таковыми регуляризаторами являются регуляризаторы когерентности и лапласианы графов связей документов.\\
	\textbf{Неограниченные регуляризаторы}. В случае, если регуляризатор неограничен, мы получаем некорректую постановку оптимизационной задачи. Более подробно мы рассматривали эту проблему в предыдущем разделе.\\
	\textbf{Произвольные регуляризаторы}. Для произвольных регуляризаторов мы доказали увеличение $R$ при регуляризационном преобразовании, при дополнительных условиях оно преобразуется в увеличение $Q^{\prime} + \tau R$ на итерациях. Здесь наиболее интересно научиться делать подмену регуляризатора по системе уравнений, о которой мы говорили в конце предыдущей главы.

 \subsubsection{Различия предложенных М-шагов}
Пусть $R = -\tau \sum\limits_{w, t} \phi_{wt}$. Формально он не должен влиять на оптимизацию, поскольку просто равен константе при ограничениях задачи. Однако, стандартные формулы дадут следующий М-шаг:
\[
\left\{
	\begin{aligned}
		\phi_{wt} \propto \bigl( n_{wt} - \tau \phi_{wt}\bigr)_{+},\\
		\theta_{td} \propto \bigl( n_{td} - \tau \theta_{td}\bigr)_{+}.
	\end{aligned}
\right.
\]
Этот процесс сойдётся, возможно (если не будет занулений) к тому же самому, что и PLSA, но траектория будет другой. Используя несмещённые оценки, можно получить.
\[
\left\{
	\begin{aligned}
		\phi_{wt} \propto  \sparse_{\tau}(n_{wt}, n_{t}),\\
		\theta_{td} \propto \sparse_{\tau}(n_{td}, n_{d}),
	\end{aligned}
\right.
\]
где $\sparse_{\tau}(x, y) = x$, если $\tau < y$ и $0$ иначе. Это уже практически PLSA, но с условием, на селекцию тем (тема должна содержать какое-то минимальнок число слов, иначе мы просто занулим все  значения).
Использование градиентной добавки даёт 
\[
r_{wt} \propto \frac{\partial{R}}{\partial{\phi_{wt}}} - \sum\limits_u \phi_{ut} \frac{\partial{R}}{\partial{\phi_{ut}}} = -\tau + \tau =0
\]
То есть в точности PLSA.\\

Этот пример показывает, что градиентная добавка менее склонна к занулению параметров. Это наводит на мысль, что вначале можно использовать обычные формулы М-шага, или несмещённые оценки для селекции тем. А затем использовать градиентную поправку для более точной настройки параметров.\\
           \subsection{Стремление коэффициентов к нулю}
Важным нашим интрументом для анализа регуляризационного преобразования был подсчёт градиента не по $\phi_{wt}$, а по $n_{wt}$. Используя данный подход, мы можем доказать следующее утверждения для коэффициентов регуляризации, стремящихся к нулю.\\
\begin{claim}
Существует такая константа $\gamma$, что если $\tau_n \leq \gamma \Delta Q^{\prime}_n$, а также $\frac{1}{n_t} \frac{\partial{R}}{\partial{\phi_{wt}}}(n_{wt}, n_{td})$ -- ограниченная функция (константой $C$), то при $r_{wt} = \tau \frac{n_{wt}}{n_t} \frac{\partial{R}}{\partial{\phi_{wt}}} \biggl(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d}\biggr)$ и $r_{td}= \tau \frac{n_{td}}{n_d} \frac{\partial{R}}{\partial{\theta_{td}}} \biggl(\frac{n_{wt}}{n_t}, \frac{n_{td}}{n_d}\biggr)$ будет выполнено $\Delta Q^{\prime}_n \geq 0$.
\end{claim}
\begin{Proof}
\ \\
Для простоты рассмотрим случай $R(\Phi, \Theta) = R(\Phi)$. \\
При регуляризационном сглаживании  $\Delta n_{wt} = \bigg( n_{wt} + \tau_n \phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}}\bigg)_{+} - n_{wt} = \alpha_{wt} \phi_{wt}$.\\
\[
\bigg( n_{wt} + \tau_n \phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}}\bigg)_{+} - n_{wt} \leq  n_{wt} +\bigg| \tau_n \phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}}\bigg| - n_{wt} \leq \bigg| \tau_n \phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}}\bigg|
\]
\[
\bigg( n_{wt} + \tau_n \phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}}\bigg)_{+} - n_{wt} \geq  n_{wt} - \bigg| \tau_n \phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}}\bigg| - n_{wt} \geq - \bigg| \tau_n \phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}}\bigg|
\]
\[
|\Delta n_{wt} | \leq \tau_n\bigg|  \phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}}\bigg| \leq \tau_n n_{wt} C \leq \tau_n n_{w} C
\]
Мы уже считали градиент $R$ в предыдущей главе, поэтому просто подставим:
\[
 \bigg|\langle\Delta n, \nabla R(n_{wt}, n_{td}) \rangle\bigg| = \bigg| \sum\limits_{w, t, u}  \frac{1}{n_{t}}  \bigg(  \frac{\partial{R}}{\partial{\phi_{wt}}}  -  \frac{\partial{R}}{\partial{\phi_{ut}}}  \bigg)  \Delta n_{wt}  \phi_{ut} \bigg| \leq 2C^2\sum_w n_w  \tau_n \leq (2 \gamma C^2\sum_w n_w) \Delta Q^{\prime}_n
\]
Если $ 2 \gamma C^2 \sum_w n_w < 1$, то изменение $Q^{\prime}_n$ при регуляризационном преобразовании меньше чем при максимизации $Q^{\prime}_n$, а значит суммарный эффект будет положительным.
\end{Proof}\ \\
\ \\
Приведём простой пример того, как можно управлять коэффициентами регуляризации. Давайте, следить за тем, чтобы $Q^{\prime}_n$ всегда увеличивалось, а в случае, если оно не увеличивается, будем уменьшать $\tau_n$ (например домножением на 0.95).

	\section{Практические исследования}
\subsection{Используемая коллекция данных}
Нам была нужна достаточно большая коллекция документов, с заранее известным  и небольшим числом тем. 20Newsgroup не подошла, так как там слишком мало документов, Википедия не подошла поскольку содержит слишком много тем. Поэтому была собрана собственная коллекция документов. Мы скачали статьи с спортивного сайта sports.ru по разным видам спорта, темой документа мы считали вид спорта. Получившаяся коллекция документов состоит из 7 спортивных направлений примерно по 3000 статей в каждом. Также была проведена лемматизация текста и удаление стоп слов. Итоговые параметры коллекции: $|W| = 18831,~|D| = 21001,~|T| = 7$.
	\subsection{Сравниваемые алгоритмы}
Было проведено сравнение трёх алгоритмов: стандартного, несмещённой модификации М-шага и градиентной модификации М-шага. Приведём описание данных алгоритмов.

\begin{algorithm}
\caption{ARTM. Стандартный М-шаг}\label{malgo1}
\begin{algorithmic}[]
\Procedure{MStep}{$n_{wt}$, $n_{td}$, $\phi_{wt}$, $\theta_{td}$}
\State 1. Для всех пар $w$, $t$ вычислить $r_{wt} = \tau \phi_{wt} \frac{\partial{R}}{\partial{\phi_{wt}}}(\phi_{wt}, \theta_{td})$
\State 2. Для всех пар $t$, $d$ вычислить $r_{td} =\tau  \theta_{td} \frac{\partial{R}}{\partial{\theta_{td}}}(\phi_{wt}, \theta_{td})$
\State 3. Для всех пар $w$, $t$ обновить счётчики $n_{wt} = \max(n_{wt} + r_{wt}, 0)$
\State 4. Для всех пар $t$, $d$ обновить счётчики $n_{td} = \max(n_{td} + r_{td}, 0)$
\State 5. Вычислить нормировочные множители $n_t = \sum_w n_{wt}$ и $n_d = \sum_t n_{td}$
\State 6. Для всех пар $w$, $t$ вычислить $\phi_{wt} = \frac{n_{wt}}{n_t}$
\State 7. Для всех пар $t$, $d$ вычислить $\theta_{td} = \frac{n_{td}}{n_d}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{ARTM. Несмещённый М-шаг}\label{malgo2}
\begin{algorithmic}[]
\Procedure{MStep}{$n_{wt}$, $n_{td}$, $\phi_{wt}$, $\theta_{td}$}
\State 1. Вычислить нормировочные множители $n^{\prime}_t = \sum_w n_{wt}$ и $n^{\prime}_d = \sum_t n_{td}$
\State 2. Для всех пар $w$, $t$ вычислить $r_{wt}= \tau \frac{n_{wt}}{n^{\prime}_t}\frac{\partial{R}}{\partial{\phi_{wt}}}(\frac{n_{wt}}{n^{\prime}_t}, \frac{n_{td}}{n^{\prime}_d})$
\State 3. Для всех пар $t$, $d$ вычислить $r_{td}= \tau \frac{n_{td}}{n^{\prime}_d}\frac{\partial{R}}{\partial{\theta_{td}}}(\frac{n_{wt}}{n^{\prime}_t}, \frac{n_{td}}{n^{\prime}_d})$
\State 4. Для всех пар $w$, $t$ обновить счётчики $n_{wt} = \max(n_{wt} + r_{wt}, 0)$
\State 5. Для всех пар $t$, $d$ обновить счётчики $n_{td} = \max(n_{td} + r_{td}, 0)$
\State 6. Вычислить нормировочные множители $n_t = \sum_w n_{wt}$ и $n_d = \sum_t n_{td}$
\State 7. Для всех пар $w$, $t$ вычислить $\phi_{wt} = \frac{n_{wt}}{n_t}$
\State 8. Для всех пар $t$, $d$ вычислить $\theta_{td} = \frac{n_{td}}{n_d}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{ARTM. Градиентный М-шаг}\label{malgo3}
\begin{algorithmic}[]
\Procedure{MStep}{$n_{wt}$, $n_{td}$, $\phi_{wt}$, $\theta_{td}$}
\State 1. Вычислить нормировочные множители $n^{\prime}_t = \sum_w n_{wt}$ и $n^{\prime}_d = \sum_t n_{td}$
\State 2. Для всех пар $w$, $t$ вычислить $r_{wt}= \tau \frac{1}{n^{\prime}_t} \frac{\partial{R}}{\partial{\phi_{wt}}}(\frac{n_{wt}}{n^{\prime}_t}, \frac{n_{td}}{n^{\prime}_d})$
\State 3. Для всех пар $t$, $d$ вычислить $r_{td}= \tau \frac{1}{n^{\prime}_d} \frac{\partial{R}}{\partial{\theta_{td}}}(\frac{n_{wt}}{n^{\prime}_t}, \frac{n_{td}}{n^{\prime}_d})$
\State 6. Вычислить $r_t = \sum_w \frac{n_{wt}}{n^{\prime}_t} r_{wt}$ и $r_d = \sum_t \frac{n_{td}}{n^{\prime}_d} r_{td}$
\State 4. Для всех пар $w$, $t$ обновить счётчики $n_{wt} = \max(n_{wt} + r_{wt} - r_t, 0)$
\State 5. Для всех пар $t$, $d$ обновить счётчики $n_{td} = \max(n_{td} + r_{td} - r_t, 0)$
\State 6. Вычислить нормировочные множители $n_t = \sum_w n_{wt}$ и $n_d = \sum_t n_{td}$
\State 7. Для всех пар $w$, $t$ вычислить $\phi_{wt} = \frac{n_{wt}}{n_t}$
\State 8. Для всех пар $t$, $d$ вычислить $\theta_{td} = \frac{n_{td}}{n_d}$
\EndProcedure
\end{algorithmic}
\end{algorithm}\ \\

	
\subsection{Исследуемые величины}
В качестве объекта экспериментов был выбран регуляризатор декоррелирования: $R = \sum_w \sum_{s \neq t} \phi_{wt} \phi_{ws}$. Мы использовали четрые разных значения $\tau$: $-10^5$, $-10^6$, $-10^7$ и $-10^8$. Значения отрицательные поскольку мы хотим уменьшить корреляции тем. Также проверерялись разные количества тем: 3, 10, 30, чтобы проверить поведение алгоритма в случаях недооценки, достоточно точной оценки и переоценки  количества тем.  Нас интересовали следующие величины:
\begin{enumerate}
\item Минимальное ненулевое значение в матрицах $\Phi$ и $\Theta$. Таким образом мы проверим гипотезу  отделимости от нуля.
\item Значение $L$, $R$ и $L + \tau R$ на итерациях. Это показатель качества оптимизации качества оптимизации.
\item Изменение $R$ при выполнении М-шага. Тут есть  небольшая особенность. Дело в том, что значения $r_{wt}$ по модулю  при градиентной модификации на порядок меньше значений при стандартном М-шаге или несмещённой модификации. Поэтому, чтобы более точно сравнить качество оптимизации,  было решено нормировать изменение $R$ на норму ($l_1$ и $l_2$) вектора $r_{wt}$. Также мы попробовали помимо градиента использовать градиент умноженный на 50. Эта константа была выбрана из соображений, чтобы изменения $R$ были по порядку такие же как и у других алгоритмов.
\item Минимальные ненулевые значения $n_t$  на М-шаге.
\end{enumerate}
Алгоритм  запускался из 10 начальных приближений, одинаковых для всех запусков, и сравнивались средние значения целевых метрик. При фиксированных параметрах 10 начальных приближений отрабатывают за полчаса.

\subsection{Особенности реализации}
Во-первых, требовалось обеспечить условие, что $\phi$ и $\theta$ отделимы от нуля. Однако было замечено, что в матрицах $\Phi$ и $\Theta$ некоторые элементы стремятся к нулю, но обнуляются посредством машинной точности только спустя очень много итераций. Поэтому при выполнении М-шага мы производили очень слабое разреживание. То есть, $n_{wt}$ и $n_{td}$ меньшие $10^{-6}$ занулялись. Формально, такое воздействие можно задать регуляризатором, мы говорили об этом в главе про неограниченные регуляризаторы. Значения данного регуляризатора очень малы и не влияют на значение замеряемых функционалов.
Во-вторых, из-за ошибок округления возникали граничные эффекты при вычислении логарифма правдоподобия, поэтому при вычислении логарифмов в формуле мы использовали $\log(x + \varepsilon)$, где $\varepsilon$ --  маленькая константна равная $10^{-20}$. Благодаря этому логарифм правдоподобия не устремлялся к минус бесконечности в плохих точках, но становился достаточно малым, чтобы заметить такой эффект. Фактически размером этой константы определяется, в окрестности какого числа будут получаться  значения логарифма правдоподобия.
\subsection{Результаты и выводы}
\subsubsection{Значения $L + \tau R$}
$|T| = 3$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_3_LR_values}
$|T| = 10$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_10_LR_values}
$|T| = 30$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_30_LR_values}
При небольших значениях $|\tau|$ градиентная поправка показывает более слабые результаты. Это можно объяснить тем, что константы, на которые домножается регуляризационная поправка,  не совсем верно подобрана. Однако с ростом $|\tau|$ мы видим, что стандартные формулы и несмещённый вариант зануляют слишком много параметров, в то время как градиентная поправка, которая более аккуратно зануляет параметры, не создает провал логарифма правдоподобия. Зависимость от числа тем мы обсудиим в следующем параграфе.
\subsubsection{Значения $R$}
$|T| = 3$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_3_R_values}
$|T| = 10$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_10_R_values}
$|T| = 30$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_30_R_values}
При достаточно больших значения $|\tau|$ мы видим, что в первоначальной формуле М-шага есть скачки в изменении $R$ на первых итерациях. Это вызвано тем, что, как мы уже говорили, регуляризационные поправки считаются в точке с предыдущей итерации, а большое значение $\tau$ делает различие существенным. Также стоит отметить, что gradient\_x50 и unbiased оптимизируют $R$ заметно лучше остальных.\\
С ростом числа тем порядок значений $R$ изменился, и поэтому чувствительность к $|\tau|$ возросла. Как следствие мы наблюдаем, что колебания значений функционала начинаются раньше, размер этих колебаний существеннее, а градиентные поправки раньше начиют показывать лучшее качество, чем остальные алгоритмы.\\
Также стоит отметить интересный эффект, когда $|T| = 3$, значения $R$ очень малы и в итоге оптимизация $L$ выходит на первый план, что приводит к немонотонной траектории $R$. С ростом $\tau$ эта зависимость постепенно выпрямляется.
\subsubsection{Минимальное значения в $\Phi$ и $\Theta$}
$|T| = 3$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_3_minPhi_values}
\includegraphics[width=1.0\linewidth]{E:/topics_3_minTheta_values}
$|T| = 10$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_10_minPhi_values}
\includegraphics[width=1.0\linewidth]{E:/topics_10_minTheta_values}
$|T| = 30$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_30_minPhi_values}
\includegraphics[width=1.0\linewidth]{E:/topics_30_minTheta_values}
Несмотря на то, что градиентные методы более аккуратно  зануляют элементы $\Phi$, они делают это заметно лучше, значения существенно сильнее отделимы от нуля. Однако в элементах $\Theta$ разницы нет, если не считать случая $\tau = -10^{8}$, когда существенные зануления привели к существенной отделимости параметров от нуля.
\subsubsection{Изменения $R$ на втором этапе М-шага}
$|T| = 3$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_3_RMstepDiff}
\includegraphics[width=1.0\linewidth]{E:/topics_3_RMstepDiffPerL1}
\includegraphics[width=1.0\linewidth]{E:/topics_3_RMstepDiffPerL2}
$|T| = 10$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_10_RMstepDiff}
\includegraphics[width=1.0\linewidth]{E:/topics_10_RMstepDiffPerL1}
\includegraphics[width=1.0\linewidth]{E:/topics_10_RMstepDiffPerL2}
$|T| = 30$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_30_RMstepDiff}
\includegraphics[width=1.0\linewidth]{E:/topics_30_RMstepDiffPerL1}
\includegraphics[width=1.0\linewidth]{E:/topics_30_RMstepDiffPerL2}
Уменьшение значения $R$ на втором этапе М-шага заметно больше, чем в стандартном алгоритме, что ожидаемо, поскольку они были выведены с такой целью. Поскольку градиент это оптимальное направление изменения в $l_2$ норме, то при нормировке изменения $R$ на $l_2$ норму мы получаем, что градиентные поправки существенно эффективнее двух других методов. На этих графиках, также как и на траекториях $R$ на итерациях, можно увидеть колебание функционала для стандартной формулы М-шага.
\subsubsection{Минимальный размер темы}
$|T| = 3$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_3_MinTopicSize}
$|T| = 10$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_10_MinTopicSize}
$|T| = 30$:\\
\includegraphics[width=1.0\linewidth]{E:/topics_30_MinTopicSize}
Значения опускаются до 1, но отделимость от нуля сохраняется.  Видно, что градиентные методы более склонны к селекции тем. Отсюда можно сделать вывод, что градиентное направление обладает большей селективностью, чем стандартная формула. Также стоит отметить, что несмещённая модификация активнее отбирает темы чем стандартная модификация.
\section{Заключение}
	Подведём краткое резюме данной работы. В вероятностном тематическом моделированиии существует подход ARTM, он предоставлет быстрый и очень гибкий функционал для оптимизации, легко адаптируемый под конкретную задачу. Основная его идея состоит в максимизации регуляризованного правдоподобия при помощи ЕМ-алгоритма. На практике предложенный алгоритм успешно сходился, однако, не было теоретического обоснования данной сходимости. Определение ограничений, при выполнении которых, можно гарантировать сходимость, было целью данной работы. Мы проинтерпретировали итерации алгоритма ARTM как итерации GEM алгоритма, для которых условия сходимости хорошо изучены. Используя данные результаты, были найдены естественные ограничения на регуляризатор, выполнение которых достаточно просто проверить на практике. Пусть $k$ -- это номер итерации, переменная с  верхним индексом это обзначение для значения данной переменной на соответствующей итерации, тогда полученные условия можно сформулировать следующим образом:
\begin{enumerate}
\item Сохранение нуля регуляризатором.
\smallskip

$ n^k_{wt} = 0 \Rightarrow \phi^k_{wt} = 0$ , $n^k_{td} = 0 \Rightarrow \theta^k_{td} = 0$.
\item $\varepsilon$-отделимость от нуля элементов матриц $\Phi$ and $\Theta$.
\smallskip

$\exists \varepsilon>0\ \exists N\ \forall k > N\ \phi^k_{wt}, \theta^k_{td} \notin (0, \varepsilon)$. 
\item  Конечность логарифма правдоподобия на итерациях.
\smallskip

$ n_{dw}>0 \Rightarrow \forall k\ \exists t\colon p^k_{tdw} > 0$.
\item Отделимость от нуля знаменателя на М-шаге.
\smallskip

$\exists \delta >0\ \exists N\ \forall k > N \ \forall t\ \exists w\  n^k_{wt} + r^k_{wt} > \delta$ и аналогичное условие для $\theta$. 
\item Неуменьшение нижней оценки регуляризованного правдоподобия на итерациях.
\smallskip

$\exists N\ \forall k > N\colon\ \ Q^k (\phi^k, \theta^k)+ R(\Phi^k, \Theta^k) \geq Q^k(\phi^{k-1}, \theta^{k-1}) + R(\Phi^{k-1}, \Theta^{k-1})$, где $Q^k(\phi, \Theta) = \sum\limits_{t,d,w} p^k_{tdw} (\ln \phi_{wt} + \ln \theta_{td})$.
\end{enumerate}
Первое условие легко проверяется аналитически. Второе и третье условия можно обеспечить при реализации алгоритма. Четвёртое условие для матрицы $\Phi$ можно проинтерпретировать как критерий селекции тем. То есть, если значение становится меньше $\delta$, то мы просто зануляем всю строчку матрицы $\Phi$. С точки зрения реализации это эквивалентно просто удалению строчки в матрице и уменьшению числа тем. Для матрицы $\Theta$ выполнение данного условия можно достичь за счёт выбора $\tau$. Пятое условие должно быть обеспеченно за счёт правильного выбора регуляризационных добавок. Стандартный алгоритм предлагает использовать $r_{wt}^{k} = \tau \phi_{wt}^{k-1} \frac{\partial{R}}{\partial{\phi_{wt}}}(\phi_{wt^{k-1}}, \theta_{td}^{k-1})$ и $r_{td}^{k}=\tau  \theta_{td}^{k-1} \frac{\partial{R}}{\partial{\theta_{td}}}(\phi_{wt}^{k-1}, \theta_{td}^{k-1})$. Для данных формул нам не удалось получить хороших оценок, поэтому была рассмотрена следующая модификация: замена всех вхождений $\phi_{wt}$ и $\theta_{td}$ на их несмещённые оценки. То есть,  $r_{wt}^k= \tau \frac{n^k_{wt}}{n^k_t}\frac{\partial{R}}{\partial{\phi_{wt}}}\bigl(\frac{n^k_{wt}}{n^k_t}, \frac{n^k_{td}}{n^k_d}\bigr)$ и $r_{td}^k= \tau \frac{n^k_{td}}{n^k_d}\frac{\partial{R}}{\partial{\theta_{td}}}\bigl(\frac{n^k_{wt}}{n^k_t}, \frac{n^k_{td}}{n^k_d}\bigr)$. Используя идею, что можно рассматривать функцию $R$ не как функцию от $\phi_{wt}$ и $\theta_{td}$, а как функцию от $n_{wt}$ и $n_{td}$, только с нормировкой аргументов, мы получили, что на М-шаге происходит увеличение $R$, если взято не слишком большое $\tau$. Также была предложена идея использовать вычисленный градиент $R$ в качестве регуляризационных добавок на М-шаге: $r^k_{wt} = \tau A_t \bigl[{\frac{\partial{R}}{\partial{\phi_{wt}}} - \sum\limits_u \phi_{ut} \frac{\partial{R}}{\partial{\phi_{ut}}} }\bigr] \bigl(\frac{n^k_{wt}}{n^k_t}, \frac{n^k_{td}}{n^k_d}\bigr)$ и $r^k_{td} = \tau B_d \bigl[ {\frac{\partial{R}}{\partial{\theta_{td}}} - \sum\limits_s \theta_{sd} \frac{\partial{R}}{\partial{\theta_{sd}}} }\bigr] \bigl(\frac{n^k_{wt}}{n^k_t}, \frac{n^k_{td}}{n^k_d}\bigr)$. Определение формул для $A_t$ и $B_d$ в данной работе не производилось, для экспериментов использовались самые наивные варианты.\\
Был проведён эксперимент, в котором сравнивались три возможных формулы М-шага. Предложенные модификации показали небольшое улучшение по сравнению со стандартными формулами. Также был замечен следующий эффект: при достаточно больших $\tau$ наблюдаются скачки в функционалах для стандартной формулы М-шага, это сливетельствует в пользу того, что не получится теоретически доказать неуменьшение $R$ на М-шаге для стандартной формулы.\\
\subsection{Результаты, выносимые на защиту}
\begin{enumerate}
\item Условия для сходимости ЕМ-алгоритма ARTM, легко проверяемые и обеспечиваемые при реализациии.
\item Две модификации формул М-шага ЕМ-алгоритма, улучшающие сходимость без увеличения вычислительной сложности.
\item Оценки для изменения оптимизируемых функционалов для предложенных модификаций.
\end{enumerate}
\newpage
	\begin{thebibliography}{@@@@}
	\bibitem{plsadef1}
		Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Richard Harshman. Indexing by Latent Semantic Analysis,  JASIS (41), 1990.
	\bibitem{plsadef2}
		Thomas Hofmann. Probilistic latent semantic analysis, Proceedings of the Twenty-Second Annual International SIGIR Conference on Research and Development in Information Retrieval, 1999.
	\bibitem{ldadef1}
		David M. Blei, Andrew Ng, Michael Jordan. Latent Dirichlet allocation, Journal of Machine Learning Research,  2003
	\bibitem{artmdef1}
		Vorontsov K. V. Additive Regularization for Topic Models of Text Collections, Doklady Mathematics, 2014.
	\bibitem{artmdef2}
		Vorontsov K. V., Potapenko A. A. Tutorial on Probabilistic Topic Modeling: Additive Regularization for Stochastic Matrix Factorization,  AIST’2014, Analysis of Images, Social networks and Texts. Springer International Publishing Switzerland, 2014.
	\bibitem{artmdef3}
		Vorontsov K. V., Potapenko A. A. Additive Regularization of Topic Models, Machine Learning Journal, 2014.
	\bibitem{ldaonline1}
		Hoffman M. D., Blei D. M., Bach F. R. Online learning for latent dirichlet allocation, NIPS, Curran Associates, Inc., 2010.
	\bibitem{wuem}
		C. F. Jeff Wu. On the Convergence Properties of the EM Algorithm, The Annals of Statistics, 1983
	\bibitem{pinsker}
		F. Topsøe. Some inequalities for information divergence and related measures of discrimination. IEEE Transactions on Information Theory, 46(9):1602–1609, 2000
	\end{thebibliography}
\end{document}